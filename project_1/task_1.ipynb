{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[__Natural Language Understanding, Spring 2018, ETHZ__](http://www.da.inf.ethz.ch/teaching/2018/NLU/)\n",
    "\n",
    "[__Project 1__ (ETHZ network)](http://www.da.inf.ethz.ch/teaching/2018/NLU/material/project.pdf)\n",
    "\n",
    "\n",
    "# Task 1: RNN Language Modelling (30 +10 Points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a) Language Modelling (30 Points)\n",
    "Your task is to build a simple LSTM language model. To be precise, we assume that words are independent given the recurrent hidden state; we compute a new hidden state given the last hidden state and last word, and predict the next word given the hidden state:\n",
    "$$ P(w_1,\\dots,w_n) = 􏰀\\Pi_{t=1}^{n}(w_t|\\mathbf{h_t})$$\n",
    "$$ P(w_t|h_t) = \\text{softmax}(\\mathbf{Wh}_t)$$\n",
    "$$ \\mathbf{h_t} = f(\\mathbf{h_{t−1}}, w_{t-1}^{*})$$\n",
    "\n",
    "where $f$ is the LSTM recurrent function, $\\mathbf{W} \\in \\mathbb{R}^{|V|×d}$ are softmax weights and $\\mathbf{h_0}$ is either an all-zero constant or a trainable parameter.\n",
    "You can use the tensorflow cell implementation __[1](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell)__ to carry out the recurrent computation in $f$. However, you must construct the actual RNN yourself (e.g. don’t use tensorflow’s `static_rnn` or `dynamic_rnn` or any other RNN library). That means, you will need to use a python loop that sets up the unrolled graph. To make your life simpler, please follow these design choices:\n",
    "\n",
    "__Model and Data specification__\n",
    "\n",
    "- Use a special sentence-beginning symbol `<bos>` and a sentence-end symbol `<eos>` (please use exactly these, including brackets). The `<bos>` symbol is the input, when predicting the first word and the `<eos>` symbol you require your model to predict at the end of every sentence.\n",
    "- Use a maximum sentence length of 30 (including the `<bos>` and `<eos>` symbol). Ignore longer sentences during training and testing.\n",
    "- Use a special padding symbol `<pad>` (please use exactly this, including brackets) to fill up sentences of length shorter than 30. This way, all your input will have the same size.\n",
    "- Use a vocabulary consisting of the 20K most frequent words in the training set, including the symbols `<bos>`, `<eos>`, `<pad>` and `<unk>`. Replace out-of-vocabulary words with the `<unk>` symbol before feeding them into your network (don’t change the file content).\n",
    "- Provide the ground truth last word as input to the RNN, not the last word you predicted. This is common practice.\n",
    "- Language models are usually trained to minimize the cross-entropy. Use tensorflow’s `tf.nn.sparse_softmax_cross_entropy_with_logits` to compute the loss (*This operation fuses the computation of the soft-max and the cross entropy loss given the logits. For numerical stability, it’s very important to use this function.*). Use the AdamOptimizer with default parameters to minimize the loss. Use `tf.clip_by_global_norm` to clip the norm of the gradients to 5.\n",
    "- Use a batch size of 64.\n",
    "- Use the data at __[6](https://polybox.ethz.ch/index.php/s/qUc2NvUh2eONfEB)__. Don’t pre-process the input further. All the data is already white-space tokenized and\n",
    "lower-cased. One sentence per line.\n",
    "- To initialize your weight matrices, use the `tf.contrib.layers.xavier_initializer()` initializer introduced in __[5](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)__.\n",
    "\n",
    "__Experiments__\n",
    "All experiments should not run for longer than, say, four hours on the GPU. For this task, your\n",
    "grade won’t improve with performance.\n",
    "\n",
    "- __Experiment A__: Train your model with word-embedding dimensionality of 100 and a hidden state size of 512 and compute sentence perplexity on the evaluation set (see submission format below).\n",
    "- __Experiment B__: It is common practice, to pretrain word embeddings using e.g. `word2vec`. This should make your model train faster as words will come already with some useful representation. Use the code at __[3](http://da.inf.ethz.ch/teaching/2018/NLU/material/load_embeddings.py)__ to load these word embeddings __[4](https://polybox.ethz.ch/index.php/s/cpicEJeC2G4tq9U)__ trained on the same corpus. Train your model again and compute evaluation perplexity.\n",
    "- __Experiment C__: It is often desirable to make the LSTM more powerful, by increasing the hidden dimensionality. However, this will naturally increase the parameters $\\mathbf{W}$ of the softmax. As a compromise, one can use a larger hidden state, but down-project it before the softmax. Increase the hidden state dimensionality from 512 to 1024, but down-project $h_t$ to dimensionality 512 before predicting $w_t$ as in\n",
    "$$ \\mathbf{\\tilde{h}}_t = \\mathbf{W}_P\\mathbf{h}_t$$\n",
    "where WP are parameters. Train your model again and compute evaluation perplexity.\n",
    "\n",
    "__Submission and grading__\n",
    "- Grading scheme: 100% correctness.\n",
    "- Deadline April 20th, 23:59:59.\n",
    "- You are not allowed to copy-paste any larger code blocks from existing implementations.\n",
    "- Hand in\n",
    "    - Your python code\n",
    "    - __Three__ result files containing sentence-level perplexity numbers on the __test__ set (to be distributed) for\n",
    "all three experiments. Recall that perplexity of a sentence $S = ⟨w_1, \\dots , w_n⟩$ with respect to your model $p(w_t|w_1, \\dots, w_{t−1})$ is defined as\n",
    "$$ \\text{Perp} = 2^{-\\frac{1}{n} \\sum_{t=1}^{n}\\log_2 p(w_t|w_1,\\dots,w_{t−1})}$$\n",
    "The `<eos>` symbol is part of the sequence, while the `<pad>` symbols (if any) are not. Be sure to have the basis of the exponential and the logarithm match.<br>\n",
    "__Input format sentences.test__<br>\n",
    "One sentence (none of them is longer than 28 tokens) per line:<br>\n",
    "         ```beside her , jamie bounced on his seat .\n",
    "         i looked and saw claire montgomery looking up at me .\n",
    "         people might not know who alex was , but they knew to listen to him .```<br>\n",
    "__Required output format groupXX.perplexityY__<br>\n",
    "(where XX is your group __number__ and Y ∈ {A,B,C} is the experiment). One perplexity number per line:<br>\n",
    "         $10.232$<br>\n",
    "         $2.434$<br>\n",
    "         $5.232$<br>\n",
    "Make sure to have equally many lines in the output as there are in the input – otherwise your submission will be rejected automatically.\n",
    "    - You have to submit at https://cmt3.research.microsoft.com/NLUETHZ2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Generation (10 Points)\n",
    "Let’s use your trained language model from above to generate sentences. Given an initial sequence of words, your are asked to __greedily__ generate words until either your model decides to finish the sentence (it generated `<eos>`) or a given maximum length has been reached. Note, that this task does not involve any training. Please see the tensorflow documentation on how to save and restore your model from above.\n",
    "There are several ways how to implement the generation. For example, you can define a graph that computes just one step of the RNN given the last input and the last state (both from a new placeholder).\n",
    "$$ \\text{state}_t, p_t = f(\\text{state}_{t−1},w_{t−1}) $$\n",
    "That means, for a prefix of size $m$ and a desired length of $n$, you run this graph $n$ times. The first $m + 1$ times you take the input form the prefix. For the rest of the sequence, you take the most likely2 word $w^{t−1} = \\text{argmax}_w p_{t−1}(w)$ from the last step.\n",
    "\n",
    "- Grading scheme: 100% correctness.\n",
    "- Deadline April 20th, 23:59:59.\n",
    "- You are not allowed to copy-paste any larger code blocks from existing implementations.\n",
    "- Hand in\n",
    "    - Your python code\n",
    "    - Your continued sentences of length up to 20. Use your trained model from experiment __C__ in task 1.1.\n",
    "    __Input format sentences.continuation__ One sentence (of length less than 20) per line:<br>\n",
    "         ```beside her ,\n",
    "         i\n",
    "         people might not know```<br>\n",
    "    The `<bos>` symbol is not explicitly in the file, but you should still use it as the first input.<br>\n",
    "    __Required output format groupXX.continuation__ (where XX is your __group number__)<br>\n",
    "         ```beside her , something happened ! <eos>\n",
    "         i do n’t recall making a noise , but i must have , because bob just looked up from his\n",
    "         people might not know the answer . <eos>```\n",
    "    - You have to submit at https://cmt3.research.microsoft.com/NLUETHZ2018\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T06:26:00.314606Z",
     "start_time": "2018-03-18T06:26:00.303743Z"
    }
   },
   "source": [
    "__Infrastructure__\n",
    "\n",
    "You must use Tensorflow, but any programming language is allowed. However, we strongly recommend `python3`. You have access to two compute resources: Unlimited CPU usage on Euler and GPU usage on Leonhard. Note that the difference in speed is typically a factor between 10 and 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
