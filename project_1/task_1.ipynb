{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Description-Task-1:-RNN-Language-Modelling-(30-+10-Points)\" data-toc-modified-id=\"Description-Task-1:-RNN-Language-Modelling-(30-+10-Points)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Description Task 1: RNN Language Modelling (30 +10 Points)</a></span><ul class=\"toc-item\"><li><span><a href=\"#1a)-Language-Modelling-(30-Points)\" data-toc-modified-id=\"1a)-Language-Modelling-(30-Points)-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>1a) Language Modelling (30 Points)</a></span></li><li><span><a href=\"#Conditional-Generation-(10-Points)\" data-toc-modified-id=\"Conditional-Generation-(10-Points)-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Conditional Generation (10 Points)</a></span></li></ul></li><li><span><a href=\"#Code-for-Task-1\" data-toc-modified-id=\"Code-for-Task-1-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Code for Task 1</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setup-and-preparation\" data-toc-modified-id=\"Setup-and-preparation-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Setup and preparation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-preprocessing\" data-toc-modified-id=\"Data-preprocessing-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Data preprocessing</a></span></li></ul></li><li><span><a href=\"#RNN\" data-toc-modified-id=\"RNN-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>RNN</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Evaluation</a></span></li><li><span><a href=\"#Conditional-Generation/Sampling-(Task-1.2)\" data-toc-modified-id=\"Conditional-Generation/Sampling-(Task-1.2)-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Conditional Generation/Sampling (Task 1.2)</a></span></li><li><span><a href=\"#Pulling-everything-together\" data-toc-modified-id=\"Pulling-everything-together-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Pulling everything together</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Understanding: Project 1\n",
    "\n",
    "[__Natural Language Understanding, Spring 2018, ETHZ__](http://www.da.inf.ethz.ch/teaching/2018/NLU/)\n",
    "\n",
    "[__Project 1__ (ETHZ network)](http://www.da.inf.ethz.ch/teaching/2018/NLU/material/project.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project to-do list:\n",
    "\n",
    "Somewhat in order of importance:\n",
    "\n",
    "- ~~change code to unroll RNN in time instead of using dynamic_rnn~~\n",
    "- ~~make sure the target data fed into the crossentropy metric is really in correct form~~\n",
    "- ~~try own implementation of basic RNN cell instead of TF-prefab RNN or LSTM cell~~\n",
    "- ~~change implementation to use the Xavier initializer instead of the uniform distribution currently used (see below)~~\n",
    "- ~~change all `tf.Variable` variable inits to the better practice form like `W = tf.get_variable(name='example', shape=[784, 256], initializer=tf.contrib.layers.xavier_initializer())` which also includes the proper weight init~~\n",
    "- ~~include dropout at input and/or RNN cell level for regularization~~\n",
    "- ~~clean up namespaces, tensor naming~~\n",
    "- ~~(Started 2.4.2018, but TBC)** build in all reporting for Tensorboard~~\n",
    "- ~~adapt code to allow for differently sized timesteps~~\n",
    "- ~~make arrangements to save trained model~~\n",
    "- ~~implement perplexity function~~\n",
    "- ~~Maybe needs rewrite to use stock LSTM cell again** implement sampling function for conditional text generation~~\n",
    "- ~~implement result output function~~\n",
    "- ~~adapt code to allow for use of pretrained word2vec embedding~~\n",
    "- ~~cleanup code~~\n",
    "- run actual experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description Task 1: RNN Language Modelling (30 +10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a) Language Modelling (30 Points)\n",
    "Your task is to build a simple LSTM language model. To be precise, we assume that words are independent given the recurrent hidden state; we compute a new hidden state given the last hidden state and last word, and predict the next word given the hidden state:\n",
    "$$ P(w_1,\\dots,w_n) = 􏰀\\prod_{t=1}^{n}P(w_t|\\mathbf{h}_t)$$\n",
    "$$ P(w_t|\\mathbf{h}_t) = \\text{softmax}(\\mathbf{Wh}_t)$$\n",
    "$$ \\mathbf{h}_t = f(\\mathbf{h}_{t−1}, w_{t-1}^{*})$$\n",
    "\n",
    "where $f$ is the LSTM recurrent function, $\\mathbf{W} \\in \\mathbb{R}^{|V|×d}$ are softmax weights and $\\mathbf{h_0}$ is either an all-zero constant or a trainable parameter.\n",
    "You can use the tensorflow cell implementation __[1](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell)__ to carry out the recurrent computation in $f$. However, you must construct the actual RNN yourself (e.g. don’t use tensorflow’s `static_rnn` or `dynamic_rnn` or any other RNN library). That means, you will need to use a python loop that sets up the unrolled graph. To make your life simpler, please follow these design choices:\n",
    "\n",
    "__Model and Data specification__\n",
    "\n",
    "- Use a special sentence-beginning symbol `<bos>` and a sentence-end symbol `<eos>` (please use exactly these, including brackets). The `<bos>` symbol is the input, when predicting the first word and the `<eos>` symbol you require your model to predict at the end of every sentence.\n",
    "- Use a maximum sentence length of 30 (including the `<bos>` and `<eos>` symbol). Ignore longer sentences during training and testing.\n",
    "- Use a special padding symbol `<pad>` (please use exactly this, including brackets) to fill up sentences of length shorter than 30. This way, all your input will have the same size.\n",
    "- Use a vocabulary consisting of the 20K most frequent words in the training set, including the symbols `<bos>`, `<eos>`, `<pad>` and `<unk>`. Replace out-of-vocabulary words with the `<unk>` symbol before feeding them into your network (don’t change the file content).\n",
    "- Provide the ground truth last word as input to the RNN, not the last word you predicted. This is common practice.\n",
    "- Language models are usually trained to minimize the cross-entropy. Use tensorflow’s `tf.nn.sparse_softmax_cross_entropy_with_logits` to compute the loss (*This operation fuses the computation of the soft-max and the cross entropy loss given the logits. For numerical stability, it’s very important to use this function.*). Use the AdamOptimizer with default parameters to minimize the loss. Use `tf.clip_by_global_norm` to clip the norm of the gradients to 5.\n",
    "- Use a batch size of 64.\n",
    "- Use the data at __[6](https://polybox.ethz.ch/index.php/s/qUc2NvUh2eONfEB)__. Don’t pre-process the input further. All the data is already white-space tokenized and\n",
    "lower-cased. One sentence per line.\n",
    "- To initialize your weight matrices, use the `tf.contrib.layers.xavier_initializer()` initializer introduced in __[5](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)__.\n",
    "\n",
    "__Experiments__\n",
    "All experiments should not run for longer than, say, four hours on the GPU. For this task, your\n",
    "grade won’t improve with performance.\n",
    "\n",
    "- __Experiment A__: Train your model with word-embedding dimensionality of 100 and a hidden state size of 512 and compute sentence perplexity on the evaluation set (see submission format below).\n",
    "- __Experiment B__: It is common practice, to pretrain word embeddings using e.g. `word2vec`. This should make your model train faster as words will come already with some useful representation. Use the code at __[3](http://da.inf.ethz.ch/teaching/2018/NLU/material/load_embeddings.py)__ to load these word embeddings __[4](https://polybox.ethz.ch/index.php/s/cpicEJeC2G4tq9U)__ trained on the same corpus. Train your model again and compute evaluation perplexity.\n",
    "- __Experiment C__: It is often desirable to make the LSTM more powerful, by increasing the hidden dimensionality. However, this will naturally increase the parameters $\\mathbf{W}$ of the softmax. As a compromise, one can use a larger hidden state, but down-project it before the softmax. Increase the hidden state dimensionality from 512 to 1024, but down-project $h_t$ to dimensionality 512 before predicting $w_t$ as in\n",
    "$$ \\mathbf{\\tilde{h}}_t = \\mathbf{W}_P\\mathbf{h}_t$$\n",
    "where $W_P$ are parameters. Train your model again and compute evaluation perplexity.\n",
    "\n",
    "__Submission and grading__\n",
    "- Grading scheme: 100% correctness.\n",
    "- Deadline April 20th, 23:59:59.\n",
    "- You are not allowed to copy-paste any larger code blocks from existing implementations.\n",
    "- Hand in\n",
    "    - Your python code\n",
    "    - __Three__ result files containing sentence-level perplexity numbers on the __test__ set (to be distributed) for\n",
    "all three experiments. Recall that perplexity of a sentence $S = ⟨w_1, \\dots , w_n⟩$ with respect to your model $p(w_t|w_1, \\dots, w_{t−1})$ is defined as\n",
    "$$ \\text{Perp} = 2^{-\\frac{1}{n} \\sum_{t=1}^{n}\\log_2 p(w_t|w_1,\\dots,w_{t−1})}$$\n",
    "The `<eos>` symbol is part of the sequence, while the `<pad>` symbols (if any) are not. Be sure to have the basis of the exponential and the logarithm match.<br>\n",
    "__Input format sentences.test__<br>\n",
    "One sentence (none of them is longer than 28 tokens) per line:<br>\n",
    "         ```beside her , jamie bounced on his seat .\n",
    "         i looked and saw claire montgomery looking up at me .\n",
    "         people might not know who alex was , but they knew to listen to him .```<br>\n",
    "__Required output format groupXX.perplexityY__<br>\n",
    "(where XX is your group __number__ and Y ∈ {A,B,C} is the experiment). One perplexity number per line:<br>\n",
    "         $10.232$<br>\n",
    "         $2.434$<br>\n",
    "         $5.232$<br>\n",
    "Make sure to have equally many lines in the output as there are in the input – otherwise your submission will be rejected automatically.\n",
    "    - You have to submit at https://cmt3.research.microsoft.com/NLUETHZ2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Generation (10 Points)\n",
    "Let’s use your trained language model from above to generate sentences. Given an initial sequence of words, your are asked to __greedily__ generate words until either your model decides to finish the sentence (it generated `<eos>`) or a given maximum length has been reached. Note, that this task does not involve any training. Please see the tensorflow documentation on how to save and restore your model from above.\n",
    "There are several ways how to implement the generation. For example, you can define a graph that computes just one step of the RNN given the last input and the last state (both from a new placeholder).\n",
    "$$ \\text{state}_t, p_t = f(\\text{state}_{t−1},w_{t−1}) $$\n",
    "That means, for a prefix of size $m$ and a desired length of $n$, you run this graph $n$ times. The first $m + 1$ times you take the input form the prefix. For the rest of the sequence, you take the most likely2 word $w^{t−1} = \\text{argmax}_w p_{t−1}(w)$ from the last step.\n",
    "\n",
    "- Grading scheme: 100% correctness.\n",
    "- Deadline April 20th, 23:59:59.\n",
    "- You are not allowed to copy-paste any larger code blocks from existing implementations.\n",
    "- Hand in\n",
    "    - Your python code\n",
    "    - Your continued sentences of length up to 20. Use your trained model from experiment __C__ in task 1.1.\n",
    "    __Input format sentences.continuation__ One sentence (of length less than 20) per line:<br>\n",
    "         ```beside her ,\n",
    "         i\n",
    "         people might not know```<br>\n",
    "    The `<bos>` symbol is not explicitly in the file, but you should still use it as the first input.<br>\n",
    "    __Required output format groupXX.continuation__ (where XX is your __group number__)<br>\n",
    "         ```beside her , something happened ! <eos>\n",
    "         i do n’t recall making a noise , but i must have , because bob just looked up from his\n",
    "         people might not know the answer . <eos>```\n",
    "    - You have to submit at https://cmt3.research.microsoft.com/NLUETHZ2018\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T06:26:00.314606Z",
     "start_time": "2018-03-18T06:26:00.303743Z"
    }
   },
   "source": [
    "__Infrastructure__\n",
    "\n",
    "You must use Tensorflow, but any programming language is allowed. However, we strongly recommend `python3`. You have access to two compute resources: Unlimited CPU usage on Euler and GPU usage on Leonhard. Note that the difference in speed is typically a factor between 10 and 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Task 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T08:22:42.147335Z",
     "start_time": "2018-03-18T08:22:42.144046Z"
    }
   },
   "source": [
    "### Setup and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T07:22:12.364855Z",
     "start_time": "2018-03-18T07:22:12.360292Z"
    }
   },
   "source": [
    "Make sure you have done the following:\n",
    "\n",
    "- Download data from https://polybox.ethz.ch/index.php/s/qUc2NvUh2eONfEB and unpack into `./data/` subdirectory\n",
    "- Download embeddings from https://polybox.ethz.ch/index.php/s/cpicEJeC2G4tq9U and unpack into `./data/` subdirectory\n",
    "- Download helper function from http://da.inf.ethz.ch/teaching/2018/NLU/material/load_embeddings.py and put into `./helpers/` subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:01:53.400306Z",
     "start_time": "2018-04-11T13:01:53.392573Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # 0: ANY, 1: filter INFO, 2: filter WARNINGS, 3: filter ERROR\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from gensim import models\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:01:54.869691Z",
     "start_time": "2018-04-11T13:01:54.852417Z"
    },
    "code_folding": [
     0,
     5
    ]
   },
   "outputs": [],
   "source": [
    "class RNNConfig():\n",
    "    '''Class holding all configuration vars for training the RNN\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 data_dir='./data',\n",
    "                 out_dir='./outputs',\n",
    "                 save_dir='./{}-checkpoints',\n",
    "                 validation_split=.1,\n",
    "                 max_sentence_length=30,\n",
    "                 vocab_length=20000,\n",
    "                 embedding_size=100,\n",
    "                 external_embedding=False,\n",
    "                 rnn_size=512,\n",
    "                 rnn_size_factor=1,\n",
    "                 n_steps=30,\n",
    "                 learning_rate=0.001,\n",
    "                 keep_prob=0.5,\n",
    "                 grad_clip=5,\n",
    "                 batch_size=64,\n",
    "                 num_epochs=1,\n",
    "                 save_every_n=500,\n",
    "                 validate_every_n=250,\n",
    "                 report_every_n=20,\n",
    "                 summary_every_n=10,\n",
    "                 max_to_keep=5,\n",
    "                 init_scale=.1):\n",
    "        \n",
    "        # Data directory \n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        # Output directory \n",
    "        self.out_dir = out_dir\n",
    "        \n",
    "        # Save directory\n",
    "        self.save_dir = save_dir\n",
    "        \n",
    "        # Percentage of the training data used for validation (default: 10%)\n",
    "        self.validation_split = validation_split\n",
    "        \n",
    "        # Maximum source sentence length as given by task description (default: 30)\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        \n",
    "        # Size of the vocabulary (default: 20k)\n",
    "        self.vocab_length = vocab_length\n",
    "        \n",
    "        # Dimensionality of word embedding layer (default: 100)\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # Whether to use externally fed embedding (default: False)\n",
    "        self.external_embedding = external_embedding\n",
    "        \n",
    "        # Dimensionality of RNN (i.e. hidden) layer (default: 512)\n",
    "        self.rnn_size = rnn_size\n",
    "        \n",
    "        # Integer to factor the size of the hidden layer [Task 1.1C] (default: 1)\n",
    "        self.rnn_size_factor = rnn_size_factor\n",
    "        \n",
    "        # Number of time steps for RNN (default: 30)\n",
    "        self.n_steps = n_steps\n",
    "        \n",
    "        # Learning rate (default: 0.001)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Dropout rate (default: 0.5)\n",
    "        self.keep_prob = keep_prob\n",
    "        \n",
    "        # Gradient clipping treshold (default: 5.0)\n",
    "        self.grad_clip = grad_clip\n",
    "        \n",
    "        # Batch Size (default: 64)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Number of training epochs (default: 10)\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "        # Save model after this many steps (default: 500)\n",
    "        self.save_every_n = save_every_n\n",
    "        \n",
    "        # Validate after this many steps (default: 250)\n",
    "        self.validate_every_n = validate_every_n\n",
    "        \n",
    "        # Print report after this many steps (default: 20)\n",
    "        self.report_every_n = report_every_n\n",
    "        \n",
    "        # Record summary after this many steps (default: 10)\n",
    "        self.summary_every_n = summary_every_n\n",
    "        \n",
    "        # Number of checkpoints to save (default: 5)\n",
    "        self.max_to_keep = max_to_keep\n",
    "        \n",
    "        # Scale range for uniform variable inits (default: 0.1)\n",
    "        self.init_scale = init_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:01:57.622359Z",
     "start_time": "2018-04-11T13:01:57.348211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# Initialize config variables to defaults\n",
    "CONFIG = RNNConfig()\n",
    "\n",
    "# Define shorthands for common initializers used all over code\n",
    "ones = tf.ones_initializer()\n",
    "unif = tf.random_uniform_initializer(-CONFIG.init_scale, CONFIG.init_scale)\n",
    "xavi = tf.contrib.layers.xavier_initializer()\n",
    "zeros = tf.zeros_initializer()\n",
    "\n",
    "# Read all data from files\n",
    "data_dir = CONFIG.data_dir\n",
    "\n",
    "with open(data_dir+'/sentences.train', 'r') as f:\n",
    "    train_data = f.read()\n",
    "    \n",
    "with open(data_dir+'/sentences.eval', 'r') as f:\n",
    "    eval_data = f.read()\n",
    "    \n",
    "with open(data_dir+'/sentences.continuation', 'r') as f:\n",
    "    continuation_data = f.read()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:01:58.765030Z",
     "start_time": "2018-04-11T13:01:58.755795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data sample:\n",
      " ====================\n",
      "`` i 've never had any ice-cream for myself , my father never let me have any . ''\n",
      "`` and even if sh \n",
      " ................................................................................\n",
      "\n",
      " Evaluation data sample:\n",
      " ====================\n",
      "he took my face in his hands and held my face where he wanted it as he kissed me senseless .\n",
      "`` on t \n",
      " ................................................................................\n",
      "\n",
      " Continuation data sample:\n",
      " ====================\n",
      "`` no ,\n",
      "correct ...\n",
      "`` i\n",
      "peter\n",
      "what was i\n",
      "when he\n",
      "take some of\n",
      "id told\n",
      "throw\n",
      "we have no place\n",
      "i felt \n",
      " ................................................................................\n"
     ]
    }
   ],
   "source": [
    "# Have a peek at the given raw data\n",
    "\n",
    "print('Training data sample:\\n', 20*'=')\n",
    "print(train_data[:100], '\\n', 80*'.')\n",
    "\n",
    "print('\\n Evaluation data sample:\\n', 20*'=')\n",
    "print(eval_data[:100], '\\n', 80*'.')\n",
    "\n",
    "print('\\n Continuation data sample:\\n', 20*'=')\n",
    "print(continuation_data[:100], '\\n', 80*'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:02:12.539008Z",
     "start_time": "2018-04-11T13:02:02.427761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training sentences:\n",
      " [\"`` i 've never had any ice-cream for myself , my father never let me have any . ''\", \"`` and even if she was , the gargoyle could hardly halt me from harming her . ''\", 'pigafetta repeated the threat , which had no apparent effect .', \"best to barricade oneself , no ? ''\", \"`` why would he lie ? ''\"] \n",
      "\n",
      "Sample words:\n",
      " ['``', 'i', \"'ve\", 'never', 'had', 'any', 'ice-cream', 'for', 'myself', ',', 'my', 'father', 'never', 'let', 'me', 'have', 'any', '.', \"''\", '``'] \n",
      "\n",
      "Top frequency words:\n",
      " ['.', ',', 'the', 'i', 'to', 'and', '``', \"''\", 'a', 'he', 'of', 'you', 'was', 'her', 'it', 'she', 'in', 'his', 'that', '?'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting data into sentences\n",
    "def split_data2sentences(data):\n",
    "    text = ''.join(data)\n",
    "    sentences = text.split('\\n')\n",
    "    return sentences\n",
    "\n",
    "train_sentences = split_data2sentences(train_data)\n",
    "eval_sentences = split_data2sentences(eval_data)\n",
    "continuation_sentences = split_data2sentences(continuation_data)\n",
    "\n",
    "# Get sentences from training data and look at sample\n",
    "print('Sample training sentences:\\n', train_sentences[:5], '\\n')\n",
    "\n",
    "# Make text contiguous again, break into words for vocabulary and look at sample\n",
    "words = ' '.join(train_sentences).split()\n",
    "print('Sample words:\\n', words[:20], '\\n')\n",
    "\n",
    "## Generate the dictionary from the training data\n",
    "# Make a word counter and show top frequency words\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "print('Top frequency words:\\n', vocab[:20], '\\n')\n",
    "\n",
    "# Clip word counter to defined length and append special symbol words\n",
    "symbols = ['<bos>', '<eos>', '<pad>', '<unk>']\n",
    "vocab = vocab[:CONFIG.vocab_length-len(symbols)] # Limit to CONFIG.vocab_length minus the last 4 to replace w/ symbols\n",
    "for each in symbols:\n",
    "    vocab.append(each)\n",
    "    \n",
    "# Make a vocabulary to convert words to integers\n",
    "vocab_to_int = {word: i for i, word in enumerate(vocab, 0)} # consider starting with 1 if 0 gives dead cells\n",
    "\n",
    "# Make a vocabulary to get words from integers at the end\n",
    "int_to_vocab = dict(enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:02:48.922323Z",
     "start_time": "2018-04-11T13:02:16.300742Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 2000001/2000001 [00:18<00:00, 110044.67it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 10001/10001 [00:00<00:00, 148454.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# Encode sentences to integers and insert symbol words where necessary\n",
    "\n",
    "### ATTN: Remove next line after finishing, keeping data set small for speedup\n",
    "# train_sentences = train_sentences[:2000]\n",
    "\n",
    "def encode_sentences(sentences):\n",
    "    max_sentence_length = CONFIG.max_sentence_length # Given by task description\n",
    "    sentences_ints = [] # List to hold converted-to-int sentences\n",
    "    for each in tqdm(sentences):\n",
    "        sentence = each.split()\n",
    "        if len(sentence) <= max_sentence_length-2: # -2 to allow for <bos>, <eos>\n",
    "            sentence_int = [vocab_to_int['<bos>']] # Start sentence list w/ <bos>\n",
    "            sentence_int += [vocab_to_int.get(word, vocab_to_int['<unk>']) for word in sentence]            \n",
    "            sentence_int.append(vocab_to_int['<eos>']) # End sentence w/ <eos>\n",
    "            while len(sentence_int) < max_sentence_length: # Pad length if necessary\n",
    "                sentence_int.append(vocab_to_int['<pad>'])\n",
    "            sentences_ints.append(sentence_int) \n",
    "    encoded = np.array(sentences_ints) # Convert list of sentences to np array\n",
    "    return encoded\n",
    "\n",
    "train_encoded = encode_sentences(train_sentences)\n",
    "eval_encoded = encode_sentences(eval_sentences)\n",
    "\n",
    "# Set data preparation complete flag\n",
    "data_ready = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:02:52.890018Z",
     "start_time": "2018-04-11T13:02:52.869500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Modified load_embedding code not to mess with own session handling code\n",
    "\n",
    "def load_embedding(vocab, path, dim_embedding, vocab_size):\n",
    "    '''\n",
    "      vocab          A dictionary mapping token strings to vocabulary IDs\n",
    "      path           Path to embedding file\n",
    "      dim_embedding  Dimensionality of the external embedding.\n",
    "      vocab_size     Size of the vocabulary to be embedded\n",
    "      Returns np.ndarray of size (vocab_size, dim_embedding) containing pretrained embedding\n",
    "    '''\n",
    "\n",
    "    print(\"Loading external embeddings from %s\" % path)\n",
    "\n",
    "    model = models.KeyedVectors.load_word2vec_format(path, binary=False)  \n",
    "    external_embedding = np.zeros(shape=(vocab_size, dim_embedding))\n",
    "    matches = 0\n",
    "\n",
    "    for tok, idx in vocab.items():\n",
    "        if tok in model.vocab:\n",
    "            external_embedding[idx] = model[tok]\n",
    "            matches += 1\n",
    "        else:\n",
    "            print(\"%s not in embedding file\" % tok)\n",
    "            external_embedding[idx] = np.random.uniform(low=-0.25, high=0.25, size=dim_embedding)\n",
    "        \n",
    "    print(\"%d words out of %d could be loaded\" % (matches, vocab_size))\n",
    "    return external_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:03:14.623482Z",
     "start_time": "2018-04-11T13:02:57.101103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading external embeddings from ./data/wordembeddings-dim100.word2vec\n",
      "<bos> not in embedding file\n",
      "<eos> not in embedding file\n",
      "<pad> not in embedding file\n",
      "<unk> not in embedding file\n",
      "19996 words out of 20000 could be loaded\n"
     ]
    }
   ],
   "source": [
    "# Load external embedding\n",
    "filename = 'wordembeddings-dim100.word2vec'\n",
    "external_embedding = load_embedding(vocab_to_int, data_dir+'/'+filename, CONFIG.embedding_size, CONFIG.vocab_length)\n",
    "\n",
    "# Set external embeddingready flag\n",
    "external_emb_ready = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:03:19.058271Z",
     "start_time": "2018-04-11T13:03:19.051520Z"
    }
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    '''Attach a lot of summaries to a Tensor (for TensorBoard visualization).\n",
    "    \n",
    "        From TensorBoard documentation\n",
    "    '''\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev', stddev)\n",
    "    tf.summary.scalar('max', tf.reduce_max(var))\n",
    "    tf.summary.scalar('min', tf.reduce_min(var))\n",
    "    tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:25:04.831721Z",
     "start_time": "2018-04-11T13:25:04.794700Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNNLanguageModel:\n",
    "    '''Main element: Class representing the complete RNN language model\n",
    "    \n",
    "    TBC\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_length,\n",
    "                 batch_size,\n",
    "                 n_steps,\n",
    "                 rnn_size,\n",
    "                 rnn_size_factor,\n",
    "                 learning_rate,\n",
    "                 grad_clip,\n",
    "                 embedding_size,\n",
    "                 external_embedding):\n",
    "        \n",
    "        # Reset tensorflow graph for clean slate\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # TF Placeholders:\n",
    "        with tf.name_scope('input_layer'):\n",
    "            self.inputs = tf.placeholder(tf.int32, [batch_size, n_steps], name='inputs')\n",
    "            self.targets = tf.placeholder(tf.int64, [batch_size, n_steps], name='targets')\n",
    "            self.target_weights = tf.placeholder(tf.float32, [batch_size, n_steps],\n",
    "                                                 name='target_weights')\n",
    "\n",
    "            self.ext_embedding_matrix = tf.placeholder(tf.float32, [vocab_length, embedding_size],\n",
    "                                                       name='ext_embedding_matrix')\n",
    "\n",
    "            self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.name_scope('embedding_layer'):\n",
    "            # Create embedding matrix\n",
    "            embedding_matrix = tf.get_variable(name='embedding_matrix',\n",
    "                                           shape=[vocab_length, embedding_size],\n",
    "                                           initializer=unif)\n",
    "\n",
    "            # If we're using the pretrained embedding\n",
    "            if (external_embedding == True):\n",
    "                embedding_matrix.assign(self.ext_embedding_matrix)\n",
    "\n",
    "            # Lookup inputs in embedding matrix\n",
    "            self.embeddings = tf.nn.embedding_lookup(embedding_matrix, self.inputs,\n",
    "                                                         name='embeddings')\n",
    "\n",
    "            tf.summary.histogram('embeddings', self.embeddings)\n",
    "\n",
    "            # Embedding layer dropout (use during training)\n",
    "            self.embeddings = tf.nn.dropout(self.embeddings, self.keep_prob, name='embeddings_dropout')\n",
    "            \n",
    "           # if (self.keep_prob < 1):\n",
    "           #     self.embeddings = tf.nn.dropout(self.embeddings, keep_prob, name='embeddings_dropout')\n",
    "\n",
    "        # RNN layer\n",
    "        with tf.name_scope('hidden_layer'):\n",
    "\n",
    "            # RNN cell: GRUCell trains faster than BasicLSTMCell w/ similar results\n",
    "            cell = tf.nn.rnn_cell.GRUCell(rnn_size*rnn_size_factor)\n",
    "\n",
    "            # Dropout wrapper (use during training)\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=self.keep_prob)\n",
    "            \n",
    "            # Initialize cell to zero state\n",
    "            self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "            # Unroll RNN through time\n",
    "            # dynamic_rnn solution would be:\n",
    "            # self.rnn_output, self.final_state = tf.nn.dynamic_rnn(cell, self.embeddings, initial_state=self.initial_state)\n",
    "\n",
    "            state = self.initial_state \n",
    "            outputs = []\n",
    "            with tf.variable_scope('RNN'):\n",
    "                for i in range(n_steps):\n",
    "                    if i > 0:\n",
    "                        tf.get_variable_scope().reuse_variables()\n",
    "                    output, state = cell(self.embeddings[:, i, :], state)\n",
    "                    outputs.append(output)\n",
    "            self.rnn_output, self.final_state = tf.stack(outputs, axis=1), state\n",
    "\n",
    "            # to keep only last output would be:\n",
    "            # self.rnn_output = self.rnn_output[:, -1, :]\n",
    "\n",
    "            # Reshape hidden layer output: one row per input and step, i.e. ((batch_size*n_steps), rnn_size)            \n",
    "            self.rnn_output = tf.reshape(self.rnn_output, [-1, rnn_size*rnn_size_factor])\n",
    "\n",
    "            variable_summaries(self.rnn_output)    \n",
    "\n",
    "            # If hidden layer increased by factor [1.1C], project down\n",
    "            if (rnn_size_factor > 1):\n",
    "                W_p = tf.get_variable(name='downprojection_weight', shape=[rnn_size*rnn_size_factor,\n",
    "                                                                          rnn_size],\n",
    "                                      initializer=xavi)\n",
    "                b_p = tf.get_variable(name='downprojection_bias', shape=[rnn_size,],\n",
    "                                      initializer=zeros)\n",
    "\n",
    "                self.rnn_output = tf.nn.xw_plus_b(self.rnn_output, W_p, b_p)\n",
    "\n",
    "                variable_summaries(W_p)\n",
    "                variable_summaries(b_p)\n",
    "\n",
    "        # Softmax output layer\n",
    "        with tf.name_scope('softmax_layer'):\n",
    "\n",
    "            # out_size is vocab_length\n",
    "            out_size = vocab_length\n",
    "\n",
    "            # RNN outputs to softmax layer:\n",
    "            W_softmax = tf.get_variable(name=\"softmax_weight\", shape=[rnn_size, out_size],\n",
    "                                        initializer=xavi)\n",
    "            b_softmax = tf.get_variable(name=\"softmax_bias\", shape=[out_size],\n",
    "                                        initializer=zeros)\n",
    "\n",
    "            variable_summaries(W_softmax)\n",
    "            variable_summaries(b_softmax)\n",
    "\n",
    "            # Calculate logits from softmax layer\n",
    "            self.logits = tf.nn.xw_plus_b(self.rnn_output, W_softmax, b_softmax, name='logits')\n",
    "            \n",
    "            variable_summaries(self.logits)\n",
    "            \n",
    "            # Finally, get word probabilities from logits\n",
    "            self.predictions = tf.nn.softmax(self.logits, name='predictions')\n",
    "\n",
    "        # Metrics\n",
    "        with tf.name_scope('metrics'):\n",
    "\n",
    "            # Reshape targets and their weights from (batch_size, n_steps) to 1D\n",
    "            y = tf.reshape(self.targets, [-1])\n",
    "            y_weights = tf.reshape(self.target_weights, [-1])\n",
    "\n",
    "            # Calculate crossentropy: Use sparse routine for numerical stability\n",
    "            # and also to avoid having to one-hot encode the targets\n",
    "            crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                                      logits=self.logits)\n",
    "\n",
    "            # Multiply crossentropy vector by 1D-reshaped weights to nullify effect of <pad>\n",
    "            crossent *= y_weights\n",
    "\n",
    "            # Get scalar crossentropy: With aim to average over both n_steps and batch_size\n",
    "            crossent = tf.reduce_sum(crossent)\n",
    "\n",
    "            # Calculate total weight and make sure it doesn't equal zero for the division below\n",
    "            total_weight = tf.reduce_sum(y_weights)\n",
    "            total_weight += 1e-12\n",
    "\n",
    "            # Calculate total loss (across batch_size AND n_steps): Divide summed crossentropy by total weight\n",
    "            self.loss = crossent / total_weight\n",
    "\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "\n",
    "            # Calculate perplexity: Uses natural logarithm as does sparse_softmax_cross_entropy_with_logits\n",
    "            self.perplexity = tf.exp(self.loss)\n",
    "\n",
    "            tf.summary.scalar('perplexity', self.perplexity)\n",
    "\n",
    "            # Best prediction\n",
    "            self.best_prediction = tf.argmax(self.predictions, 1, name='best_prediction')\n",
    "\n",
    "            # Accuracy\n",
    "            correct_predictions = tf.cast(tf.equal(self.best_prediction, y), tf.float32)\n",
    "            correct_predictions *= y_weights      \n",
    "            correct_predictions = tf.reduce_sum(correct_predictions)\n",
    "            self.accuracy = correct_predictions / total_weight\n",
    "\n",
    "            tf.summary.scalar('accuracy', self.accuracy)\n",
    "\n",
    "        # Optimizer\n",
    "        with tf.name_scope('optimizer'):\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), grad_clip)\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "            self.optimizer = train_op.apply_gradients(zip(grads, tvars))      \n",
    "\n",
    "        # Merge summaries for TensorBoard    \n",
    "        self.merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:03:28.751315Z",
     "start_time": "2018-04-11T13:03:28.734690Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build batch generator for training    \n",
    "def get_batches(source_arr, batch_size, n_steps):\n",
    "    '''Generator which returns features x and targets y\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    source_arr: A np.ndarray of sentences in rows to generate features and targets from\n",
    "    batch_size: An int number of sequences required per batch\n",
    "    n_steps: Number of time steps for RNN to consider; defines sequence length\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    x: A np.ndarray of feature sequences according to parameters above\n",
    "    y: A np.ndarray of target sequences according to parameters above\n",
    "    y_weights: A np.ndarray of floats: 0.0 were target sequence element = <pad>, 1.0 else\n",
    "    '''\n",
    "    \n",
    "    source_rows = source_arr.shape[0]\n",
    "    source_cols = source_arr.shape[1]\n",
    "    \n",
    "    # Make sure time steps doesn't exceed available information in sentence (+1 is the wraparound)\n",
    "    assert (n_steps <  source_cols+1), \"No point in looking further back than source is long\"\n",
    "    \n",
    "    # Wrap around source col 0 <bos> to end for full length unpadded source_cols, otherwise <pad>\n",
    "    wrap_around = np.array([vocab_to_int['<bos>'] if (source_arr[i, -1] == vocab_to_int['<eos>']) \\\n",
    "                            else vocab_to_int['<pad>'] for i in range(source_rows)])\n",
    "    source_arr = np.hstack((source_arr, wrap_around.reshape((-1, 1))))\n",
    "    \n",
    "    # How many sequences we can get from an input sentence (row)\n",
    "    n_seq_per_row = source_cols+1 - n_steps \n",
    "        \n",
    "    # How many 'batch blocks' we can get\n",
    "    n_blocks = source_rows // batch_size\n",
    "   \n",
    "    # Shuffle rows of sequences to improve training\n",
    "    np.random.shuffle(source_arr)\n",
    "    \n",
    "    # Crop array to only produce full batches\n",
    "    source_arr = source_arr[:n_blocks*batch_size, :]\n",
    "\n",
    "    # Reshape source_arr for easier batch generation\n",
    "    source_arr = source_arr.reshape((batch_size, -1))\n",
    "\n",
    "    # Generate batches\n",
    "    for j in range(0, source_arr.shape[1], source_cols+1):\n",
    "        for jj in range(n_seq_per_row):\n",
    "            # Feature sequence:\n",
    "            x = source_arr[:, j+jj : j+jj+n_steps]\n",
    "            # Target:\n",
    "            y = source_arr[:, j+jj+1:j+jj+n_steps+1]\n",
    "            # Target weights:\n",
    "            y_weights = np.array([[0. if (int_to_vocab[j] == '<pad>') else 1. for j in i] for i in y])\n",
    "            yield x, y, y_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " (3, 30) \n",
      " [[19996    56    22     8   338    10    23    18  3929    39    60   116\n",
      "    443     0 19997 19998 19998 19998 19998 19998 19998 19998 19998 19998\n",
      "  19998 19998 19998 19998 19998 19998]\n",
      " [19996     3  1357     0 19997 19998 19998 19998 19998 19998 19998 19998\n",
      "  19998 19998 19998 19998 19998 19998 19998 19998 19998 19998 19998 19998\n",
      "  19998 19998 19998 19998 19998 19998]\n",
      " [19996     3    74 19999    24     8  1593    84     0 19997 19998 19998\n",
      "  19998 19998 19998 19998 19998 19998 19998 19998 19998 19998 19998 19998\n",
      "  19998 19998 19998 19998 19998 19998]] \n",
      "\n",
      "y\n",
      " (3, 30) \n",
      " [[   56    22     8   338    10    23    18  3929    39    60   116   443\n",
      "      0 19997 19998 19998 19998 19998 19998 19998 19998 19998 19998 19998\n",
      "  19998 19998 19998 19998 19998 19998]\n",
      " [    3  1357     0 19997 19998 19998 19998 19998 19998 19998 19998 19998\n",
      "  19998 19998 19998 19998 19998 19998 19998 19998 19998 19998 19998 19998\n",
      "  19998 19998 19998 19998 19998 19998]\n",
      " [    3    74 19999    24     8  1593    84     0 19997 19998 19998 19998\n",
      "  19998 19998 19998 19998 19998 19998 19998 19998 19998 19998 19998 19998\n",
      "  19998 19998 19998 19998 19998 19998]] \n",
      "\n",
      "y_weights\n",
      " (3, 30) \n",
      " [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]] \n",
      "\n",
      "['<bos>', 'there', \"'s\", 'a', 'part', 'of', 'me', 'that', 'wishes', 'we', \"'d\", 'never', 'met', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']  ->  ['there', \"'s\", 'a', 'part', 'of', 'me', 'that', 'wishes', 'we', \"'d\", 'never', 'met', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<bos>', 'i', 'hesitated', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']  ->  ['i', 'hesitated', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<bos>', 'i', \"'m\", '<unk>', 'with', 'a', 'golden', 'head', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']  ->  ['i', \"'m\", '<unk>', 'with', 'a', 'golden', 'head', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# Development only: playing around with batches\n",
    "\n",
    "shuffled_rows_ind = np.random.permutation(len(train_encoded))\n",
    "validation_split_ind = int(0.1 * len(train_encoded))\n",
    "source_train = train_encoded[shuffled_rows_ind[validation_split_ind:]]\n",
    "batches = get_batches(source_train, 3, 30)\n",
    "\n",
    "x, y, y_weights = next(batches)\n",
    "print('x\\n',x.shape,'\\n', x, '\\n')\n",
    "print('y\\n',y.shape,'\\n', y, '\\n')\n",
    "print('y_weights\\n',y_weights.shape,'\\n', y_weights, '\\n')\n",
    "for i in range(x.shape[0]):\n",
    "        print([int_to_vocab[x[i,j]] for j in range(x[i].shape[0])], ' -> ', [int_to_vocab[y[i,j]] for j in range(y[i].shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:30:37.790919Z",
     "start_time": "2018-04-11T13:30:37.747005Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run training on exp for epochs with rnn_size_factor factor and external_embedding emb and save to save_dir\n",
    "def train(exp, CONFIG):\n",
    "    # Check data preparation complete, external embedding ready\n",
    "    assert data_ready == True, 'Need to run data preparation first'\n",
    "    assert external_emb_ready == True, 'Need to import external embedding first'\n",
    "\n",
    "    # Split training/validation data\n",
    "    np.random.seed(42)\n",
    "    shuffled_rows_ind = np.random.permutation(len(train_encoded))\n",
    "    validation_split_ind = int(CONFIG.validation_split * len(train_encoded))\n",
    "    source_train = train_encoded[shuffled_rows_ind[validation_split_ind:]]\n",
    "    source_validation = train_encoded[shuffled_rows_ind[:validation_split_ind]]\n",
    "\n",
    "    # Set shorter handles for training loop variables\n",
    "    save_every_n = CONFIG.save_every_n\n",
    "    validate_every_n = CONFIG.validate_every_n\n",
    "    report_every_n = CONFIG.report_every_n\n",
    "    summary_every_n = CONFIG.summary_every_n\n",
    "    keep_prob = CONFIG.keep_prob\n",
    "    epochs = CONFIG.num_epochs\n",
    "    save_dir = CONFIG.save_dir.format(exp)\n",
    "    \n",
    "    # Set some helper variables\n",
    "    num_training_steps = len(source_train) // CONFIG.batch_size \\\n",
    "                         * (source_train.shape[1]+1 - CONFIG.n_steps) * epochs\n",
    "    report_detail = False\n",
    "    if CONFIG.n_steps < 15: # This is just to keep the screen from overflowing\n",
    "        report_detail = True # Only eye-candy anyway\n",
    "    def str_hms(delta_time):\n",
    "        h, rem = divmod(delta_time, 3600)\n",
    "        m, s = divmod(rem, 60)\n",
    "        return '{:0>2}:{:0>2}:{:05.2f}'.format(int(h),int(m),s)\n",
    "\n",
    "    # Create model instance\n",
    "    model = RNNLanguageModel(vocab_length=CONFIG.vocab_length,\n",
    "                             batch_size=CONFIG.batch_size,\n",
    "                             n_steps=CONFIG.n_steps,\n",
    "                             rnn_size=CONFIG.rnn_size,\n",
    "                             rnn_size_factor=CONFIG.rnn_size_factor,\n",
    "                             learning_rate=CONFIG.learning_rate,\n",
    "                             grad_clip=CONFIG.grad_clip,\n",
    "                             embedding_size=CONFIG.embedding_size,\n",
    "                             external_embedding=CONFIG.external_embedding)\n",
    "\n",
    "    # Setup TensorBoard logging\n",
    "    # Run in project directory to view: \"tensorboard --logdir ./runs\"\n",
    "    now = time.strftime('%y-%m-%d-%H-%M-%S')\n",
    "    log_dir = './runs'\n",
    "    log_subdir = '{}/{}-run-{}/'.format(log_dir, exp, now)\n",
    "    writer = tf.summary.FileWriter(log_subdir, tf.get_default_graph())\n",
    "    \n",
    "    # Setup model saving\n",
    "    saver = tf.train.Saver(max_to_keep=CONFIG.max_to_keep)\n",
    "\n",
    "    # Run training\n",
    "    print('Starting training on experiment {}:'.format(exp))\n",
    "    print('Run \"tensorboard --logdir {}\" in project dir to monitor, current run is {}'.format(log_dir, log_subdir))\n",
    "    print('Batch size: {}\\t Time steps: {}\\t RNN layer size: {}\\t External embedding: {}'.format(CONFIG.batch_size,\n",
    "                                                                                           CONFIG.n_steps,\n",
    "                                                                                           CONFIG.rnn_size*CONFIG.rnn_size_factor,\n",
    "                                                                                           CONFIG.external_embedding))\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        train_start = time.time()\n",
    "        counter = 1\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            new_state = sess.run(model.initial_state)\n",
    "            loss = 0.\n",
    "\n",
    "            validation_batches = get_batches(source_validation, batch_size=CONFIG.batch_size,\n",
    "                                    n_steps=CONFIG.n_steps)\n",
    "\n",
    "            for x_train, y_train, y_train_weights in get_batches(source_train, batch_size=CONFIG.batch_size,\n",
    "                                    n_steps=CONFIG.n_steps):\n",
    "                \n",
    "                start = time.time()\n",
    "\n",
    "                feed_dict = {model.inputs: x_train,\n",
    "                             model.targets: y_train,\n",
    "                             model.target_weights: y_train_weights,\n",
    "                             model.keep_prob: keep_prob,\n",
    "                             model.ext_embedding_matrix: external_embedding,\n",
    "                             model.initial_state: new_state}\n",
    "\n",
    "                loss, perplexity, accuracy, best_prediction, new_state, _ = \\\n",
    "                sess.run([model.loss, model.perplexity, model.accuracy, model.best_prediction,\n",
    "                          model.final_state, model.optimizer], feed_dict=feed_dict)\n",
    "\n",
    "                batch_time = time.time() - start\n",
    "\n",
    "                if (counter % summary_every_n == 0):\n",
    "                    writer.add_summary(sess.run(model.merged, feed_dict=feed_dict), counter)\n",
    "\n",
    "                if (counter % report_every_n == 0):\n",
    "                    print('Epoch: {}/{}\\t'.format(ep+1, epochs),\n",
    "                          'Train step: {}/{}\\t'.format(counter, num_training_steps-1),\n",
    "                          'Batch loss: {:.3f}\\t'.format(loss),\n",
    "                          'Batch perplexity: {:.3f}\\t'.format(perplexity),\n",
    "                          'Batch accuracy: {:.1%}\\t'.format(accuracy),\n",
    "                          '{:.2f}s/batch'.format(batch_time))\n",
    "                    if (report_detail == True):\n",
    "                        print(' '.join([int_to_vocab[x_train[0,j]] for j in range(x_train[0].shape[0])]),\n",
    "                              ' -> ', int_to_vocab[best_prediction[0]] , ' vs. ', int_to_vocab[y_train[0, -1]])\n",
    "\n",
    "                if (counter % save_every_n == 0):\n",
    "                    saver.save(sess, '{}/run-{}_i{}_s{}.ckpt'.format(save_dir, now,\n",
    "                                                                     counter, CONFIG.rnn_size))\n",
    "                if (counter % validate_every_n == 0):\n",
    "                    x_validation, y_validation, y_validation_weights = next(validation_batches)\n",
    "                    feed_dict = {model.inputs: x_validation,\n",
    "                                 model.targets: y_validation,\n",
    "                                 model.target_weights: y_validation_weights,\n",
    "                                 model.keep_prob: 1.0,\n",
    "                                 model.ext_embedding_matrix: external_embedding,\n",
    "                                 model.initial_state: new_state}\n",
    "\n",
    "                    loss, perplexity, accuracy = sess.run([model.loss, model.perplexity, model.accuracy],\n",
    "                                                          feed_dict=feed_dict)\n",
    "                    \n",
    "                    print('Epoch: {}/{}\\t'.format(ep+1, epochs),\n",
    "                          '* Validation batch *\\t',\n",
    "                          'Batch loss: {:.3f}\\t'.format(loss),\n",
    "                          'Batch perplexity: {:.3f}\\t'.format(perplexity),\n",
    "                          'Batch accuracy: {:.1%}\\t***'.format(accuracy))\n",
    "                \n",
    "                counter += 1\n",
    "           \n",
    "        train_time = time.time() - train_start\n",
    "\n",
    "        writer.close()\n",
    "        saver.save(sess, '{}/run-{}_i{}_s{}.ckpt'.format(save_dir, now, counter, CONFIG.rnn_size))\n",
    "\n",
    "        print('*** Experiment {}: Training complete:\\t{} epoch(s) with {} batches each'.format(exp,\n",
    "                                                                                               epochs,\n",
    "                                                                                               num_training_steps))\n",
    "        print('*** Training time:\\t', str_hms(train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:03:39.486950Z",
     "start_time": "2018-04-11T13:03:39.469204Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_perplexities(checkpoint, CONFIG):\n",
    "    perplexities = []\n",
    "    counter = 1\n",
    "    \n",
    "    model = RNNLanguageModel(vocab_length=CONFIG.vocab_length,\n",
    "                             batch_size=1,\n",
    "                             n_steps=CONFIG.n_steps,\n",
    "                             rnn_size=CONFIG.rnn_size,\n",
    "                             rnn_size_factor=CONFIG.rnn_size_factor,\n",
    "                             learning_rate=CONFIG.learning_rate,\n",
    "                             grad_clip=CONFIG.grad_clip,\n",
    "                             embedding_size=CONFIG.embedding_size,\n",
    "                             external_embedding=CONFIG.external_embedding)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        for x_eval, y_eval, y_eval_weights in get_batches(eval_encoded, batch_size=1,\n",
    "                                    n_steps=CONFIG.n_steps):\n",
    "\n",
    "            if (counter % 50 == 0): print('Evaluating sentence {}/{}'.format(counter, len(eval_encoded)))\n",
    "            #if counter == 100 : break ### Dev only\n",
    "\n",
    "            new_state = sess.run(model.initial_state)\n",
    "            feed_dict = {model.inputs: x_eval,\n",
    "                         model.targets: y_eval,\n",
    "                         model.target_weights: y_eval_weights,\n",
    "                         model.keep_prob: 1,\n",
    "                         model.ext_embedding_matrix: external_embedding,\n",
    "                         model.initial_state: new_state}\n",
    "\n",
    "            perplexity = sess.run(model.perplexity, feed_dict=feed_dict)\n",
    "            perplexities.append(perplexity)\n",
    "            counter += 1          \n",
    "    return perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:03:45.465665Z",
     "start_time": "2018-04-11T13:03:45.456788Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get perplexities for experiment based on last checkpoint in save_dir and write to file\n",
    "def perp(exp, CONFIG):\n",
    "    \n",
    "    save_dir = CONFIG.save_dir.format(exp)\n",
    "    \n",
    "    # Get latest checkpoint\n",
    "    checkpoint = tf.train.latest_checkpoint(save_dir)\n",
    "\n",
    "    # Calculate perplexities:\n",
    "    perplexities = eval_perplexities(checkpoint, CONFIG)\n",
    "    \n",
    "    # Write to file\n",
    "    filename = 'group01.perplexity{}'.format(exp)\n",
    "    if not os.path.exists(CONFIG.out_dir):\n",
    "            os.makedirs(CONFIG.out_dir)\n",
    "    with open('{}/{}'.format(CONFIG.out_dir, filename), 'w') as f:\n",
    "        for p in perplexities:\n",
    "            f.write('{:.3f}\\n'.format(p))\n",
    "    print('Experiment {}: Using {}, evalulation perplexity avg: {:.3f}, min: {:.3f}, max: {:.3f}'.format(exp,\n",
    "                                                                                                      checkpoint,\n",
    "                                                                                                      np.mean(perplexities),\n",
    "                                                                                                      np.min(perplexities),\n",
    "                                                                                                      np.max(perplexities)))\n",
    "    print('{} perplexity values written to {}/{}'.format(len(eval_encoded), CONFIG.out_dir, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T15:48:35.010377Z",
     "start_time": "2018-04-08T15:48:35.000049Z"
    }
   },
   "source": [
    "### Conditional Generation/Sampling (Task 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:03:51.434839Z",
     "start_time": "2018-04-11T13:03:51.414383Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_samples(checkpoint, max_generate_n, CONFIG):\n",
    "    \n",
    "    # Generate continuation text\n",
    "    # n_samples = 100 # Dev only, use next line for production\n",
    "    n_samples = len(continuation_sentences)\n",
    "   \n",
    "    continued_sentences = []\n",
    "    counter = 1\n",
    "    \n",
    "    model = RNNLanguageModel(vocab_length=CONFIG.vocab_length,\n",
    "                             batch_size=1, # for sampling: Feeding single words\n",
    "                             n_steps=1,# for sampling: Feeding single words\n",
    "                             rnn_size=CONFIG.rnn_size,\n",
    "                             rnn_size_factor=CONFIG.rnn_size_factor,\n",
    "                             learning_rate=CONFIG.learning_rate,\n",
    "                             grad_clip=CONFIG.grad_clip,\n",
    "                             embedding_size=CONFIG.embedding_size,\n",
    "                             external_embedding=CONFIG.external_embedding)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess: \n",
    "        saver.restore(sess, checkpoint)\n",
    "        for i in range(n_samples):\n",
    "            new_state = sess.run(model.initial_state)\n",
    "            \n",
    "            sentence = [w for w in continuation_sentences[i].split()]\n",
    "            primer_feed = [vocab_to_int['<bos>']]\n",
    "            [primer_feed.append(vocab_to_int.get(word, vocab_to_int['<unk>'])) for word in sentence]\n",
    "            \n",
    "            for w in primer_feed:\n",
    "                x = np.atleast_2d(w)\n",
    "                feed_dict = {model.inputs: x,\n",
    "                             model.keep_prob: 1.0,\n",
    "                             model.initial_state: new_state}\n",
    "                best_prediction, new_state = \\\n",
    "                sess.run([model.best_prediction, model.final_state], feed_dict=feed_dict)\n",
    "            sentence.append(int_to_vocab[best_prediction[0]]) # First generated word from trigger samples\n",
    "            while len(sentence) < max_generate_n: # Generating remaining words from predictions\n",
    "                x[0, 0] = best_prediction\n",
    "                feed_dict = {model.inputs: x,\n",
    "                             model.keep_prob: 1.0,\n",
    "                             model.initial_state: new_state}\n",
    "                best_prediction, new_state = \\\n",
    "                sess.run([model.best_prediction, model.final_state], feed_dict=feed_dict)\n",
    "                sentence.append(int_to_vocab[best_prediction[0]])\n",
    "                if (sentence[-1] == '<eos>'):\n",
    "                    break \n",
    "            output = ' '.join(sentence)\n",
    "            if (counter % 50 == 0):\n",
    "                print('Generated sentence {}/{}'.format(counter, n_samples))\n",
    "                print(continuation_sentences[i], ' -> ', output)\n",
    "  \n",
    "            continued_sentences.append(output)\n",
    "            counter += 1\n",
    "    return continued_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:03:56.466481Z",
     "start_time": "2018-04-11T13:03:56.459773Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate samples based on save_dir and write to out_dir\n",
    "def sample(exp, CONFIG):\n",
    "    save_dir = CONFIG.save_dir.format(exp)\n",
    "    \n",
    "    # Get latest checkpoint\n",
    "    checkpoint = tf.train.latest_checkpoint(save_dir)\n",
    "\n",
    "     # Sentence generation length limit\n",
    "    max_generate_n = 20 # Hard-coded by task description\n",
    "    \n",
    "    # Generate samples\n",
    "    continued_sentences = generate_samples(checkpoint, max_generate_n, CONFIG)\n",
    "    \n",
    "    # Write to file\n",
    "    filename = 'group01.continuation'\n",
    "    if not os.path.exists(CONFIG.out_dir):\n",
    "            os.makedirs(CONFIG.out_dir)\n",
    "    with open('{}/{}'.format(CONFIG.out_dir, filename), 'w') as f:\n",
    "        for s in continued_sentences:\n",
    "            f.write('{}\\n'.format(s))\n",
    "    print('Using {} :'.format(save_dir))\n",
    "    print('{} continued sentences written to {}/{}'.format(len(continuation_sentences), CONFIG.out_dir, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:09:36.583859Z",
     "start_time": "2018-04-11T13:09:36.577273Z"
    }
   },
   "outputs": [],
   "source": [
    "### Finally, set up and run all tasks\n",
    "def run():    \n",
    "    # Define all experiments\n",
    "    exp_dict = {}\n",
    "    exp_parameters = [#('A', [('epochs', 1), ('factor', 1), ('emb', False)])]#, # Experiment A\n",
    "                      #('B', [('epochs', 2), ('factor', 1), ('emb', True )])]#, # Experiment B\n",
    "                       ('C', [('epochs', 2), ('factor', 2), ('emb', True )])]   # Experiment C\n",
    "    for label, para_list in exp_parameters:\n",
    "        exp_dict.setdefault(label, dict(para_list))\n",
    "    \n",
    "    # Define source experiment for conditional generation\n",
    "    sample_source = 'C'\n",
    "\n",
    "    # Setup up CONFIGs and run experiments\n",
    "    for exp in exp_dict:\n",
    "        CONFIG = RNNConfig(num_epochs=exp_dict[exp]['epochs'],\n",
    "                           rnn_size_factor=exp_dict[exp]['factor'],\n",
    "                           external_embedding=exp_dict[exp]['emb'])\n",
    "        train(exp, CONFIG)\n",
    "        perp(exp, CONFIG)\n",
    "        if (exp == sample_source): sample(exp, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:29:57.208342Z",
     "start_time": "2018-04-11T13:25:44.213Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on experiment C:\n",
      "Run \"tensorboard --logdir ./runs\" in project dir to monitor, current run is ./runs/C-run-18-04-17-09-16-16/\n",
      "Batch size: 64\t Time steps: 30\t RNN layer size: 1024\t External embedding: True\n",
      "Epoch: 1/2\t Train step: 20/27699\t Batch loss: 6.335\t Batch perplexity: 563.904\t Batch accuracy: 5.8%\t 0.72s/batch\n",
      "Epoch: 1/2\t Train step: 40/27699\t Batch loss: 6.282\t Batch perplexity: 535.010\t Batch accuracy: 6.9%\t 0.76s/batch\n",
      "Epoch: 1/2\t Train step: 60/27699\t Batch loss: 6.197\t Batch perplexity: 491.030\t Batch accuracy: 9.5%\t 0.72s/batch\n",
      "Epoch: 1/2\t Train step: 80/27699\t Batch loss: 6.085\t Batch perplexity: 439.157\t Batch accuracy: 13.9%\t 0.80s/batch\n",
      "Epoch: 1/2\t Train step: 100/27699\t Batch loss: 6.035\t Batch perplexity: 417.658\t Batch accuracy: 12.0%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 120/27699\t Batch loss: 6.160\t Batch perplexity: 473.384\t Batch accuracy: 14.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 140/27699\t Batch loss: 5.775\t Batch perplexity: 322.101\t Batch accuracy: 15.0%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 160/27699\t Batch loss: 5.642\t Batch perplexity: 282.027\t Batch accuracy: 14.3%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 180/27699\t Batch loss: 5.633\t Batch perplexity: 279.433\t Batch accuracy: 14.2%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 200/27699\t Batch loss: 5.693\t Batch perplexity: 296.924\t Batch accuracy: 15.4%\t 0.74s/batch\n",
      "Epoch: 1/2\t Train step: 220/27699\t Batch loss: 5.684\t Batch perplexity: 294.025\t Batch accuracy: 16.2%\t 0.75s/batch\n",
      "Epoch: 1/2\t Train step: 240/27699\t Batch loss: 5.714\t Batch perplexity: 303.180\t Batch accuracy: 17.6%\t 0.81s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 5.658\t Batch perplexity: 286.453\t Batch accuracy: 14.1%\t***\n",
      "Epoch: 1/2\t Train step: 260/27699\t Batch loss: 5.561\t Batch perplexity: 260.001\t Batch accuracy: 15.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 280/27699\t Batch loss: 5.594\t Batch perplexity: 268.736\t Batch accuracy: 15.6%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 300/27699\t Batch loss: 5.621\t Batch perplexity: 276.279\t Batch accuracy: 16.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 320/27699\t Batch loss: 5.698\t Batch perplexity: 298.219\t Batch accuracy: 15.9%\t 0.74s/batch\n",
      "Epoch: 1/2\t Train step: 340/27699\t Batch loss: 5.538\t Batch perplexity: 254.104\t Batch accuracy: 17.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 360/27699\t Batch loss: 5.499\t Batch perplexity: 244.350\t Batch accuracy: 15.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 380/27699\t Batch loss: 5.400\t Batch perplexity: 221.479\t Batch accuracy: 18.9%\t 0.75s/batch\n",
      "Epoch: 1/2\t Train step: 400/27699\t Batch loss: 5.509\t Batch perplexity: 246.883\t Batch accuracy: 16.8%\t 0.75s/batch\n",
      "Epoch: 1/2\t Train step: 420/27699\t Batch loss: 5.283\t Batch perplexity: 196.941\t Batch accuracy: 17.7%\t 0.74s/batch\n",
      "Epoch: 1/2\t Train step: 440/27699\t Batch loss: 5.249\t Batch perplexity: 190.392\t Batch accuracy: 18.4%\t 0.74s/batch\n",
      "Epoch: 1/2\t Train step: 460/27699\t Batch loss: 5.444\t Batch perplexity: 231.420\t Batch accuracy: 19.5%\t 0.75s/batch\n",
      "Epoch: 1/2\t Train step: 480/27699\t Batch loss: 5.353\t Batch perplexity: 211.164\t Batch accuracy: 17.1%\t 0.74s/batch\n",
      "Epoch: 1/2\t Train step: 500/27699\t Batch loss: 5.215\t Batch perplexity: 183.973\t Batch accuracy: 19.8%\t 0.77s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 5.336\t Batch perplexity: 207.673\t Batch accuracy: 19.3%\t***\n",
      "Epoch: 1/2\t Train step: 520/27699\t Batch loss: 5.508\t Batch perplexity: 246.611\t Batch accuracy: 17.1%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 540/27699\t Batch loss: 5.199\t Batch perplexity: 181.035\t Batch accuracy: 20.3%\t 0.74s/batch\n",
      "Epoch: 1/2\t Train step: 560/27699\t Batch loss: 5.204\t Batch perplexity: 181.955\t Batch accuracy: 19.4%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 580/27699\t Batch loss: 5.121\t Batch perplexity: 167.463\t Batch accuracy: 19.8%\t 0.75s/batch\n",
      "Epoch: 1/2\t Train step: 600/27699\t Batch loss: 4.995\t Batch perplexity: 147.603\t Batch accuracy: 20.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 620/27699\t Batch loss: 5.354\t Batch perplexity: 211.548\t Batch accuracy: 18.8%\t 0.72s/batch\n",
      "Epoch: 1/2\t Train step: 640/27699\t Batch loss: 5.083\t Batch perplexity: 161.236\t Batch accuracy: 19.9%\t 0.74s/batch\n",
      "Epoch: 1/2\t Train step: 660/27699\t Batch loss: 5.062\t Batch perplexity: 157.956\t Batch accuracy: 21.8%\t 0.78s/batch\n",
      "Epoch: 1/2\t Train step: 680/27699\t Batch loss: 4.939\t Batch perplexity: 139.638\t Batch accuracy: 22.0%\t 0.80s/batch\n",
      "Epoch: 1/2\t Train step: 700/27699\t Batch loss: 5.165\t Batch perplexity: 174.965\t Batch accuracy: 19.0%\t 0.79s/batch\n",
      "Epoch: 1/2\t Train step: 720/27699\t Batch loss: 5.122\t Batch perplexity: 167.688\t Batch accuracy: 19.0%\t 0.77s/batch\n",
      "Epoch: 1/2\t Train step: 740/27699\t Batch loss: 4.908\t Batch perplexity: 135.306\t Batch accuracy: 24.5%\t 0.79s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.923\t Batch perplexity: 137.480\t Batch accuracy: 23.9%\t***\n",
      "Epoch: 1/2\t Train step: 760/27699\t Batch loss: 5.189\t Batch perplexity: 179.368\t Batch accuracy: 20.7%\t 0.78s/batch\n",
      "Epoch: 1/2\t Train step: 780/27699\t Batch loss: 4.829\t Batch perplexity: 125.147\t Batch accuracy: 24.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 800/27699\t Batch loss: 4.890\t Batch perplexity: 132.951\t Batch accuracy: 23.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 820/27699\t Batch loss: 4.845\t Batch perplexity: 127.068\t Batch accuracy: 23.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 840/27699\t Batch loss: 4.836\t Batch perplexity: 125.988\t Batch accuracy: 22.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 860/27699\t Batch loss: 4.902\t Batch perplexity: 134.575\t Batch accuracy: 24.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 880/27699\t Batch loss: 5.133\t Batch perplexity: 169.555\t Batch accuracy: 21.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 900/27699\t Batch loss: 5.137\t Batch perplexity: 170.184\t Batch accuracy: 19.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 920/27699\t Batch loss: 5.029\t Batch perplexity: 152.815\t Batch accuracy: 21.3%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 940/27699\t Batch loss: 4.838\t Batch perplexity: 126.245\t Batch accuracy: 22.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 960/27699\t Batch loss: 4.776\t Batch perplexity: 118.625\t Batch accuracy: 24.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 980/27699\t Batch loss: 4.894\t Batch perplexity: 133.531\t Batch accuracy: 22.2%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 1000/27699\t Batch loss: 5.070\t Batch perplexity: 159.248\t Batch accuracy: 20.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.979\t Batch perplexity: 145.288\t Batch accuracy: 22.6%\t***\n",
      "Epoch: 1/2\t Train step: 1020/27699\t Batch loss: 4.808\t Batch perplexity: 122.457\t Batch accuracy: 24.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 1040/27699\t Batch loss: 4.886\t Batch perplexity: 132.368\t Batch accuracy: 22.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 1060/27699\t Batch loss: 4.937\t Batch perplexity: 139.406\t Batch accuracy: 20.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 1080/27699\t Batch loss: 4.595\t Batch perplexity: 98.980\t Batch accuracy: 25.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 1100/27699\t Batch loss: 4.819\t Batch perplexity: 123.787\t Batch accuracy: 22.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 1120/27699\t Batch loss: 4.817\t Batch perplexity: 123.538\t Batch accuracy: 21.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 1140/27699\t Batch loss: 4.961\t Batch perplexity: 142.687\t Batch accuracy: 22.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 1160/27699\t Batch loss: 4.563\t Batch perplexity: 95.870\t Batch accuracy: 24.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 1180/27699\t Batch loss: 4.681\t Batch perplexity: 107.829\t Batch accuracy: 23.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 1200/27699\t Batch loss: 4.795\t Batch perplexity: 120.879\t Batch accuracy: 21.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 1220/27699\t Batch loss: 4.472\t Batch perplexity: 87.543\t Batch accuracy: 26.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 1240/27699\t Batch loss: 4.852\t Batch perplexity: 128.042\t Batch accuracy: 22.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.848\t Batch perplexity: 127.440\t Batch accuracy: 21.7%\t***\n",
      "Epoch: 1/2\t Train step: 1260/27699\t Batch loss: 4.754\t Batch perplexity: 115.996\t Batch accuracy: 22.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 1280/27699\t Batch loss: 4.743\t Batch perplexity: 114.729\t Batch accuracy: 21.0%\t 0.68s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\t Train step: 1300/27699\t Batch loss: 4.906\t Batch perplexity: 135.069\t Batch accuracy: 21.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 1320/27699\t Batch loss: 4.910\t Batch perplexity: 135.644\t Batch accuracy: 22.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 1340/27699\t Batch loss: 4.640\t Batch perplexity: 103.577\t Batch accuracy: 25.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 1360/27699\t Batch loss: 4.644\t Batch perplexity: 103.960\t Batch accuracy: 24.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 1380/27699\t Batch loss: 4.778\t Batch perplexity: 118.882\t Batch accuracy: 24.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 1400/27699\t Batch loss: 4.923\t Batch perplexity: 137.403\t Batch accuracy: 23.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 1420/27699\t Batch loss: 4.724\t Batch perplexity: 112.643\t Batch accuracy: 24.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 1440/27699\t Batch loss: 4.585\t Batch perplexity: 98.052\t Batch accuracy: 24.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 1460/27699\t Batch loss: 4.916\t Batch perplexity: 136.466\t Batch accuracy: 22.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 1480/27699\t Batch loss: 4.834\t Batch perplexity: 125.769\t Batch accuracy: 22.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 1500/27699\t Batch loss: 4.468\t Batch perplexity: 87.186\t Batch accuracy: 23.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.710\t Batch perplexity: 111.069\t Batch accuracy: 24.3%\t***\n",
      "Epoch: 1/2\t Train step: 1520/27699\t Batch loss: 4.840\t Batch perplexity: 126.522\t Batch accuracy: 22.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 1540/27699\t Batch loss: 4.700\t Batch perplexity: 109.937\t Batch accuracy: 23.9%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 1560/27699\t Batch loss: 4.623\t Batch perplexity: 101.805\t Batch accuracy: 22.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 1580/27699\t Batch loss: 4.661\t Batch perplexity: 105.762\t Batch accuracy: 24.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 1600/27699\t Batch loss: 4.754\t Batch perplexity: 116.084\t Batch accuracy: 25.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 1620/27699\t Batch loss: 4.504\t Batch perplexity: 90.389\t Batch accuracy: 25.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 1640/27699\t Batch loss: 4.633\t Batch perplexity: 102.776\t Batch accuracy: 24.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 1660/27699\t Batch loss: 4.637\t Batch perplexity: 103.262\t Batch accuracy: 24.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 1680/27699\t Batch loss: 4.986\t Batch perplexity: 146.301\t Batch accuracy: 23.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 1700/27699\t Batch loss: 4.602\t Batch perplexity: 99.729\t Batch accuracy: 24.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 1720/27699\t Batch loss: 4.578\t Batch perplexity: 97.357\t Batch accuracy: 23.2%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 1740/27699\t Batch loss: 4.832\t Batch perplexity: 125.492\t Batch accuracy: 21.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.622\t Batch perplexity: 101.671\t Batch accuracy: 27.5%\t***\n",
      "Epoch: 1/2\t Train step: 1760/27699\t Batch loss: 4.484\t Batch perplexity: 88.561\t Batch accuracy: 25.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 1780/27699\t Batch loss: 4.419\t Batch perplexity: 83.042\t Batch accuracy: 26.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 1800/27699\t Batch loss: 4.516\t Batch perplexity: 91.493\t Batch accuracy: 25.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 1820/27699\t Batch loss: 4.695\t Batch perplexity: 109.430\t Batch accuracy: 24.8%\t 0.73s/batch\n",
      "Epoch: 1/2\t Train step: 1840/27699\t Batch loss: 4.745\t Batch perplexity: 114.974\t Batch accuracy: 25.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 1860/27699\t Batch loss: 4.442\t Batch perplexity: 84.938\t Batch accuracy: 26.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 1880/27699\t Batch loss: 4.454\t Batch perplexity: 86.005\t Batch accuracy: 28.3%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 1900/27699\t Batch loss: 4.750\t Batch perplexity: 115.590\t Batch accuracy: 23.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 1920/27699\t Batch loss: 4.449\t Batch perplexity: 85.573\t Batch accuracy: 23.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 1940/27699\t Batch loss: 4.654\t Batch perplexity: 105.018\t Batch accuracy: 21.7%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 1960/27699\t Batch loss: 4.714\t Batch perplexity: 111.493\t Batch accuracy: 25.9%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 1980/27699\t Batch loss: 4.461\t Batch perplexity: 86.539\t Batch accuracy: 27.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 2000/27699\t Batch loss: 4.735\t Batch perplexity: 113.837\t Batch accuracy: 24.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.667\t Batch perplexity: 106.327\t Batch accuracy: 24.5%\t***\n",
      "Epoch: 1/2\t Train step: 2020/27699\t Batch loss: 4.433\t Batch perplexity: 84.222\t Batch accuracy: 25.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 2040/27699\t Batch loss: 4.919\t Batch perplexity: 136.817\t Batch accuracy: 22.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 2060/27699\t Batch loss: 4.669\t Batch perplexity: 106.542\t Batch accuracy: 23.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 2080/27699\t Batch loss: 4.791\t Batch perplexity: 120.388\t Batch accuracy: 22.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 2100/27699\t Batch loss: 4.410\t Batch perplexity: 82.262\t Batch accuracy: 26.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 2120/27699\t Batch loss: 4.686\t Batch perplexity: 108.421\t Batch accuracy: 22.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 2140/27699\t Batch loss: 4.387\t Batch perplexity: 80.381\t Batch accuracy: 25.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 2160/27699\t Batch loss: 4.484\t Batch perplexity: 88.573\t Batch accuracy: 26.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 2180/27699\t Batch loss: 4.714\t Batch perplexity: 111.487\t Batch accuracy: 24.5%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 2200/27699\t Batch loss: 4.371\t Batch perplexity: 79.123\t Batch accuracy: 26.8%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 2220/27699\t Batch loss: 4.701\t Batch perplexity: 110.073\t Batch accuracy: 24.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 2240/27699\t Batch loss: 4.690\t Batch perplexity: 108.893\t Batch accuracy: 24.0%\t 0.72s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.514\t Batch perplexity: 91.257\t Batch accuracy: 25.4%\t***\n",
      "Epoch: 1/2\t Train step: 2260/27699\t Batch loss: 4.430\t Batch perplexity: 83.902\t Batch accuracy: 26.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 2280/27699\t Batch loss: 4.570\t Batch perplexity: 96.535\t Batch accuracy: 26.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 2300/27699\t Batch loss: 4.635\t Batch perplexity: 103.017\t Batch accuracy: 25.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 2320/27699\t Batch loss: 4.384\t Batch perplexity: 80.171\t Batch accuracy: 28.0%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 2340/27699\t Batch loss: 4.599\t Batch perplexity: 99.404\t Batch accuracy: 24.8%\t 0.73s/batch\n",
      "Epoch: 1/2\t Train step: 2360/27699\t Batch loss: 4.756\t Batch perplexity: 116.309\t Batch accuracy: 21.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 2380/27699\t Batch loss: 4.649\t Batch perplexity: 104.504\t Batch accuracy: 23.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 2400/27699\t Batch loss: 4.557\t Batch perplexity: 95.344\t Batch accuracy: 23.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 2420/27699\t Batch loss: 4.368\t Batch perplexity: 78.885\t Batch accuracy: 28.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 2440/27699\t Batch loss: 4.651\t Batch perplexity: 104.642\t Batch accuracy: 24.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 2460/27699\t Batch loss: 4.572\t Batch perplexity: 96.769\t Batch accuracy: 24.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 2480/27699\t Batch loss: 4.765\t Batch perplexity: 117.335\t Batch accuracy: 25.2%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 2500/27699\t Batch loss: 4.767\t Batch perplexity: 117.530\t Batch accuracy: 25.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.453\t Batch perplexity: 85.916\t Batch accuracy: 26.0%\t***\n",
      "Epoch: 1/2\t Train step: 2520/27699\t Batch loss: 4.576\t Batch perplexity: 97.096\t Batch accuracy: 25.9%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 2540/27699\t Batch loss: 4.433\t Batch perplexity: 84.165\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 2560/27699\t Batch loss: 4.540\t Batch perplexity: 93.665\t Batch accuracy: 24.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 2580/27699\t Batch loss: 4.628\t Batch perplexity: 102.327\t Batch accuracy: 21.9%\t 0.68s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\t Train step: 2600/27699\t Batch loss: 4.498\t Batch perplexity: 89.812\t Batch accuracy: 27.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 2620/27699\t Batch loss: 4.556\t Batch perplexity: 95.162\t Batch accuracy: 25.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 2640/27699\t Batch loss: 4.390\t Batch perplexity: 80.663\t Batch accuracy: 25.8%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 2660/27699\t Batch loss: 4.622\t Batch perplexity: 101.721\t Batch accuracy: 23.9%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 2680/27699\t Batch loss: 4.419\t Batch perplexity: 83.025\t Batch accuracy: 25.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 2700/27699\t Batch loss: 4.368\t Batch perplexity: 78.913\t Batch accuracy: 27.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 2720/27699\t Batch loss: 4.270\t Batch perplexity: 71.539\t Batch accuracy: 27.4%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 2740/27699\t Batch loss: 4.513\t Batch perplexity: 91.219\t Batch accuracy: 26.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.698\t Batch perplexity: 109.724\t Batch accuracy: 24.3%\t***\n",
      "Epoch: 1/2\t Train step: 2760/27699\t Batch loss: 4.817\t Batch perplexity: 123.606\t Batch accuracy: 22.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 2780/27699\t Batch loss: 4.529\t Batch perplexity: 92.685\t Batch accuracy: 26.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 2800/27699\t Batch loss: 4.594\t Batch perplexity: 98.911\t Batch accuracy: 25.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 2820/27699\t Batch loss: 4.341\t Batch perplexity: 76.820\t Batch accuracy: 24.7%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 2840/27699\t Batch loss: 4.537\t Batch perplexity: 93.366\t Batch accuracy: 25.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 2860/27699\t Batch loss: 4.484\t Batch perplexity: 88.616\t Batch accuracy: 27.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 2880/27699\t Batch loss: 4.808\t Batch perplexity: 122.504\t Batch accuracy: 23.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 2900/27699\t Batch loss: 4.745\t Batch perplexity: 115.016\t Batch accuracy: 24.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 2920/27699\t Batch loss: 4.654\t Batch perplexity: 105.003\t Batch accuracy: 22.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 2940/27699\t Batch loss: 4.457\t Batch perplexity: 86.233\t Batch accuracy: 26.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 2960/27699\t Batch loss: 4.750\t Batch perplexity: 115.608\t Batch accuracy: 22.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 2980/27699\t Batch loss: 4.546\t Batch perplexity: 94.208\t Batch accuracy: 24.6%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 3000/27699\t Batch loss: 4.744\t Batch perplexity: 114.861\t Batch accuracy: 23.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.436\t Batch perplexity: 84.442\t Batch accuracy: 25.7%\t***\n",
      "Epoch: 1/2\t Train step: 3020/27699\t Batch loss: 4.588\t Batch perplexity: 98.337\t Batch accuracy: 23.2%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 3040/27699\t Batch loss: 4.395\t Batch perplexity: 81.015\t Batch accuracy: 25.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 3060/27699\t Batch loss: 4.317\t Batch perplexity: 74.997\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 3080/27699\t Batch loss: 4.425\t Batch perplexity: 83.516\t Batch accuracy: 23.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 3100/27699\t Batch loss: 4.587\t Batch perplexity: 98.176\t Batch accuracy: 22.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 3120/27699\t Batch loss: 4.475\t Batch perplexity: 87.762\t Batch accuracy: 26.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 3140/27699\t Batch loss: 4.448\t Batch perplexity: 85.480\t Batch accuracy: 24.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 3160/27699\t Batch loss: 4.569\t Batch perplexity: 96.423\t Batch accuracy: 25.1%\t 0.74s/batch\n",
      "Epoch: 1/2\t Train step: 3180/27699\t Batch loss: 4.471\t Batch perplexity: 87.423\t Batch accuracy: 25.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 3200/27699\t Batch loss: 4.600\t Batch perplexity: 99.487\t Batch accuracy: 22.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 3220/27699\t Batch loss: 4.426\t Batch perplexity: 83.635\t Batch accuracy: 25.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 3240/27699\t Batch loss: 4.578\t Batch perplexity: 97.321\t Batch accuracy: 25.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.527\t Batch perplexity: 92.447\t Batch accuracy: 25.9%\t***\n",
      "Epoch: 1/2\t Train step: 3260/27699\t Batch loss: 4.491\t Batch perplexity: 89.250\t Batch accuracy: 28.1%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 3280/27699\t Batch loss: 4.726\t Batch perplexity: 112.841\t Batch accuracy: 22.8%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 3300/27699\t Batch loss: 4.526\t Batch perplexity: 92.413\t Batch accuracy: 24.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 3320/27699\t Batch loss: 4.542\t Batch perplexity: 93.840\t Batch accuracy: 25.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 3340/27699\t Batch loss: 4.691\t Batch perplexity: 108.922\t Batch accuracy: 22.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 3360/27699\t Batch loss: 4.452\t Batch perplexity: 85.771\t Batch accuracy: 26.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 3380/27699\t Batch loss: 4.586\t Batch perplexity: 98.150\t Batch accuracy: 25.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 3400/27699\t Batch loss: 4.434\t Batch perplexity: 84.308\t Batch accuracy: 27.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 3420/27699\t Batch loss: 4.541\t Batch perplexity: 93.812\t Batch accuracy: 25.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 3440/27699\t Batch loss: 4.660\t Batch perplexity: 105.682\t Batch accuracy: 26.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 3460/27699\t Batch loss: 4.367\t Batch perplexity: 78.811\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 3480/27699\t Batch loss: 4.530\t Batch perplexity: 92.764\t Batch accuracy: 25.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 3500/27699\t Batch loss: 4.505\t Batch perplexity: 90.501\t Batch accuracy: 23.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.506\t Batch perplexity: 90.524\t Batch accuracy: 24.2%\t***\n",
      "Epoch: 1/2\t Train step: 3520/27699\t Batch loss: 4.494\t Batch perplexity: 89.518\t Batch accuracy: 25.6%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 3540/27699\t Batch loss: 4.783\t Batch perplexity: 119.440\t Batch accuracy: 22.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 3560/27699\t Batch loss: 4.683\t Batch perplexity: 108.135\t Batch accuracy: 23.4%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 3580/27699\t Batch loss: 4.377\t Batch perplexity: 79.611\t Batch accuracy: 26.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 3600/27699\t Batch loss: 4.519\t Batch perplexity: 91.732\t Batch accuracy: 23.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 3620/27699\t Batch loss: 4.616\t Batch perplexity: 101.072\t Batch accuracy: 24.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 3640/27699\t Batch loss: 4.461\t Batch perplexity: 86.539\t Batch accuracy: 24.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 3660/27699\t Batch loss: 4.546\t Batch perplexity: 94.253\t Batch accuracy: 26.5%\t 0.72s/batch\n",
      "Epoch: 1/2\t Train step: 3680/27699\t Batch loss: 4.670\t Batch perplexity: 106.702\t Batch accuracy: 21.0%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 3700/27699\t Batch loss: 4.507\t Batch perplexity: 90.625\t Batch accuracy: 25.1%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 3720/27699\t Batch loss: 4.563\t Batch perplexity: 95.851\t Batch accuracy: 24.2%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 3740/27699\t Batch loss: 4.389\t Batch perplexity: 80.541\t Batch accuracy: 26.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.383\t Batch perplexity: 80.101\t Batch accuracy: 25.2%\t***\n",
      "Epoch: 1/2\t Train step: 3760/27699\t Batch loss: 4.602\t Batch perplexity: 99.657\t Batch accuracy: 23.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 3780/27699\t Batch loss: 4.442\t Batch perplexity: 84.938\t Batch accuracy: 24.2%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 3800/27699\t Batch loss: 4.770\t Batch perplexity: 117.932\t Batch accuracy: 21.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 3820/27699\t Batch loss: 4.455\t Batch perplexity: 86.026\t Batch accuracy: 25.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 3840/27699\t Batch loss: 4.570\t Batch perplexity: 96.563\t Batch accuracy: 23.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 3860/27699\t Batch loss: 4.465\t Batch perplexity: 86.901\t Batch accuracy: 28.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 3880/27699\t Batch loss: 4.655\t Batch perplexity: 105.092\t Batch accuracy: 23.1%\t 0.75s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\t Train step: 3900/27699\t Batch loss: 4.620\t Batch perplexity: 101.456\t Batch accuracy: 23.2%\t 0.75s/batch\n",
      "Epoch: 1/2\t Train step: 3920/27699\t Batch loss: 4.310\t Batch perplexity: 74.446\t Batch accuracy: 26.7%\t 0.74s/batch\n",
      "Epoch: 1/2\t Train step: 3940/27699\t Batch loss: 4.337\t Batch perplexity: 76.493\t Batch accuracy: 27.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 3960/27699\t Batch loss: 4.562\t Batch perplexity: 95.752\t Batch accuracy: 26.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 3980/27699\t Batch loss: 4.558\t Batch perplexity: 95.367\t Batch accuracy: 25.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4000/27699\t Batch loss: 4.651\t Batch perplexity: 104.642\t Batch accuracy: 23.5%\t 0.71s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.396\t Batch perplexity: 81.158\t Batch accuracy: 24.5%\t***\n",
      "Epoch: 1/2\t Train step: 4020/27699\t Batch loss: 4.517\t Batch perplexity: 91.599\t Batch accuracy: 23.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 4040/27699\t Batch loss: 4.542\t Batch perplexity: 93.847\t Batch accuracy: 25.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4060/27699\t Batch loss: 4.536\t Batch perplexity: 93.317\t Batch accuracy: 24.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4080/27699\t Batch loss: 4.522\t Batch perplexity: 91.988\t Batch accuracy: 27.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4100/27699\t Batch loss: 4.585\t Batch perplexity: 98.050\t Batch accuracy: 23.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 4120/27699\t Batch loss: 4.264\t Batch perplexity: 71.059\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 4140/27699\t Batch loss: 4.426\t Batch perplexity: 83.636\t Batch accuracy: 25.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 4160/27699\t Batch loss: 4.540\t Batch perplexity: 93.722\t Batch accuracy: 24.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4180/27699\t Batch loss: 4.584\t Batch perplexity: 97.872\t Batch accuracy: 24.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 4200/27699\t Batch loss: 4.703\t Batch perplexity: 110.332\t Batch accuracy: 23.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 4220/27699\t Batch loss: 4.572\t Batch perplexity: 96.692\t Batch accuracy: 27.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4240/27699\t Batch loss: 4.513\t Batch perplexity: 91.179\t Batch accuracy: 22.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.230\t Batch perplexity: 68.684\t Batch accuracy: 28.2%\t***\n",
      "Epoch: 1/2\t Train step: 4260/27699\t Batch loss: 4.362\t Batch perplexity: 78.427\t Batch accuracy: 26.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 4280/27699\t Batch loss: 4.442\t Batch perplexity: 84.950\t Batch accuracy: 27.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 4300/27699\t Batch loss: 4.353\t Batch perplexity: 77.678\t Batch accuracy: 26.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 4320/27699\t Batch loss: 4.530\t Batch perplexity: 92.771\t Batch accuracy: 23.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4340/27699\t Batch loss: 4.770\t Batch perplexity: 117.927\t Batch accuracy: 25.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4360/27699\t Batch loss: 4.505\t Batch perplexity: 90.427\t Batch accuracy: 24.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4380/27699\t Batch loss: 4.406\t Batch perplexity: 81.931\t Batch accuracy: 29.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4400/27699\t Batch loss: 4.752\t Batch perplexity: 115.808\t Batch accuracy: 23.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 4420/27699\t Batch loss: 4.511\t Batch perplexity: 91.042\t Batch accuracy: 23.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 4440/27699\t Batch loss: 4.402\t Batch perplexity: 81.645\t Batch accuracy: 26.8%\t 0.74s/batch\n",
      "Epoch: 1/2\t Train step: 4460/27699\t Batch loss: 4.414\t Batch perplexity: 82.575\t Batch accuracy: 26.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4480/27699\t Batch loss: 4.527\t Batch perplexity: 92.510\t Batch accuracy: 22.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4500/27699\t Batch loss: 4.563\t Batch perplexity: 95.915\t Batch accuracy: 25.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.252\t Batch perplexity: 70.226\t Batch accuracy: 27.0%\t***\n",
      "Epoch: 1/2\t Train step: 4520/27699\t Batch loss: 4.554\t Batch perplexity: 95.056\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4540/27699\t Batch loss: 4.664\t Batch perplexity: 106.047\t Batch accuracy: 25.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 4560/27699\t Batch loss: 4.545\t Batch perplexity: 94.129\t Batch accuracy: 27.3%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 4580/27699\t Batch loss: 4.537\t Batch perplexity: 93.395\t Batch accuracy: 24.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 4600/27699\t Batch loss: 4.597\t Batch perplexity: 99.175\t Batch accuracy: 23.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4620/27699\t Batch loss: 4.412\t Batch perplexity: 82.474\t Batch accuracy: 21.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4640/27699\t Batch loss: 4.268\t Batch perplexity: 71.352\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4660/27699\t Batch loss: 4.359\t Batch perplexity: 78.204\t Batch accuracy: 24.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4680/27699\t Batch loss: 4.321\t Batch perplexity: 75.244\t Batch accuracy: 25.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 4700/27699\t Batch loss: 4.461\t Batch perplexity: 86.574\t Batch accuracy: 23.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4720/27699\t Batch loss: 4.278\t Batch perplexity: 72.114\t Batch accuracy: 27.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4740/27699\t Batch loss: 4.577\t Batch perplexity: 97.219\t Batch accuracy: 24.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.428\t Batch perplexity: 83.746\t Batch accuracy: 27.4%\t***\n",
      "Epoch: 1/2\t Train step: 4760/27699\t Batch loss: 4.283\t Batch perplexity: 72.425\t Batch accuracy: 27.2%\t 0.73s/batch\n",
      "Epoch: 1/2\t Train step: 4780/27699\t Batch loss: 4.196\t Batch perplexity: 66.387\t Batch accuracy: 26.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4800/27699\t Batch loss: 4.772\t Batch perplexity: 118.209\t Batch accuracy: 23.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 4820/27699\t Batch loss: 4.462\t Batch perplexity: 86.649\t Batch accuracy: 25.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4840/27699\t Batch loss: 4.475\t Batch perplexity: 87.810\t Batch accuracy: 25.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 4860/27699\t Batch loss: 4.496\t Batch perplexity: 89.654\t Batch accuracy: 26.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4880/27699\t Batch loss: 4.577\t Batch perplexity: 97.222\t Batch accuracy: 25.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4900/27699\t Batch loss: 4.554\t Batch perplexity: 95.058\t Batch accuracy: 26.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 4920/27699\t Batch loss: 4.242\t Batch perplexity: 69.563\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 4940/27699\t Batch loss: 4.521\t Batch perplexity: 91.898\t Batch accuracy: 24.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 4960/27699\t Batch loss: 4.268\t Batch perplexity: 71.374\t Batch accuracy: 28.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 4980/27699\t Batch loss: 4.306\t Batch perplexity: 74.165\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 5000/27699\t Batch loss: 4.347\t Batch perplexity: 77.257\t Batch accuracy: 24.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.470\t Batch perplexity: 87.339\t Batch accuracy: 26.4%\t***\n",
      "Epoch: 1/2\t Train step: 5020/27699\t Batch loss: 4.321\t Batch perplexity: 75.287\t Batch accuracy: 26.8%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 5040/27699\t Batch loss: 4.381\t Batch perplexity: 79.901\t Batch accuracy: 26.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 5060/27699\t Batch loss: 4.394\t Batch perplexity: 80.926\t Batch accuracy: 25.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 5080/27699\t Batch loss: 4.480\t Batch perplexity: 88.232\t Batch accuracy: 25.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 5100/27699\t Batch loss: 4.338\t Batch perplexity: 76.586\t Batch accuracy: 25.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 5120/27699\t Batch loss: 4.466\t Batch perplexity: 86.993\t Batch accuracy: 23.2%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 5140/27699\t Batch loss: 4.661\t Batch perplexity: 105.725\t Batch accuracy: 24.6%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 5160/27699\t Batch loss: 4.380\t Batch perplexity: 79.867\t Batch accuracy: 26.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 5180/27699\t Batch loss: 4.236\t Batch perplexity: 69.153\t Batch accuracy: 28.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 5200/27699\t Batch loss: 4.626\t Batch perplexity: 102.155\t Batch accuracy: 25.2%\t 0.69s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\t Train step: 5220/27699\t Batch loss: 4.456\t Batch perplexity: 86.116\t Batch accuracy: 26.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 5240/27699\t Batch loss: 4.444\t Batch perplexity: 85.097\t Batch accuracy: 26.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.350\t Batch perplexity: 77.511\t Batch accuracy: 26.5%\t***\n",
      "Epoch: 1/2\t Train step: 5260/27699\t Batch loss: 4.255\t Batch perplexity: 70.425\t Batch accuracy: 26.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 5280/27699\t Batch loss: 4.478\t Batch perplexity: 88.061\t Batch accuracy: 23.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 5300/27699\t Batch loss: 4.650\t Batch perplexity: 104.602\t Batch accuracy: 23.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 5320/27699\t Batch loss: 4.608\t Batch perplexity: 100.316\t Batch accuracy: 25.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 5340/27699\t Batch loss: 4.650\t Batch perplexity: 104.606\t Batch accuracy: 24.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 5360/27699\t Batch loss: 4.419\t Batch perplexity: 83.034\t Batch accuracy: 24.9%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 5380/27699\t Batch loss: 4.486\t Batch perplexity: 88.807\t Batch accuracy: 25.7%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 5400/27699\t Batch loss: 4.568\t Batch perplexity: 96.345\t Batch accuracy: 24.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 5420/27699\t Batch loss: 4.174\t Batch perplexity: 64.960\t Batch accuracy: 30.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 5440/27699\t Batch loss: 4.438\t Batch perplexity: 84.605\t Batch accuracy: 26.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 5460/27699\t Batch loss: 4.293\t Batch perplexity: 73.208\t Batch accuracy: 25.7%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 5480/27699\t Batch loss: 4.165\t Batch perplexity: 64.399\t Batch accuracy: 27.6%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 5500/27699\t Batch loss: 4.475\t Batch perplexity: 87.801\t Batch accuracy: 27.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.080\t Batch perplexity: 59.136\t Batch accuracy: 28.3%\t***\n",
      "Epoch: 1/2\t Train step: 5520/27699\t Batch loss: 4.334\t Batch perplexity: 76.262\t Batch accuracy: 26.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 5540/27699\t Batch loss: 4.389\t Batch perplexity: 80.553\t Batch accuracy: 29.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 5560/27699\t Batch loss: 4.491\t Batch perplexity: 89.220\t Batch accuracy: 24.8%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 5580/27699\t Batch loss: 4.226\t Batch perplexity: 68.462\t Batch accuracy: 28.9%\t 0.72s/batch\n",
      "Epoch: 1/2\t Train step: 5600/27699\t Batch loss: 4.459\t Batch perplexity: 86.434\t Batch accuracy: 25.3%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 5620/27699\t Batch loss: 4.353\t Batch perplexity: 77.691\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 5640/27699\t Batch loss: 4.400\t Batch perplexity: 81.423\t Batch accuracy: 24.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 5660/27699\t Batch loss: 4.504\t Batch perplexity: 90.338\t Batch accuracy: 23.8%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 5680/27699\t Batch loss: 4.388\t Batch perplexity: 80.449\t Batch accuracy: 23.2%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 5700/27699\t Batch loss: 4.554\t Batch perplexity: 95.058\t Batch accuracy: 25.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 5720/27699\t Batch loss: 4.548\t Batch perplexity: 94.415\t Batch accuracy: 25.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 5740/27699\t Batch loss: 4.453\t Batch perplexity: 85.926\t Batch accuracy: 25.0%\t 0.70s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.361\t Batch perplexity: 78.368\t Batch accuracy: 27.5%\t***\n",
      "Epoch: 1/2\t Train step: 5760/27699\t Batch loss: 4.739\t Batch perplexity: 114.279\t Batch accuracy: 24.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 5780/27699\t Batch loss: 4.258\t Batch perplexity: 70.636\t Batch accuracy: 26.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 5800/27699\t Batch loss: 4.488\t Batch perplexity: 88.928\t Batch accuracy: 24.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 5820/27699\t Batch loss: 4.341\t Batch perplexity: 76.810\t Batch accuracy: 26.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 5840/27699\t Batch loss: 4.297\t Batch perplexity: 73.447\t Batch accuracy: 27.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 5860/27699\t Batch loss: 4.062\t Batch perplexity: 58.111\t Batch accuracy: 28.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 5880/27699\t Batch loss: 4.313\t Batch perplexity: 74.661\t Batch accuracy: 25.2%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 5900/27699\t Batch loss: 4.635\t Batch perplexity: 103.035\t Batch accuracy: 21.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 5920/27699\t Batch loss: 4.322\t Batch perplexity: 75.319\t Batch accuracy: 27.8%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 5940/27699\t Batch loss: 4.399\t Batch perplexity: 81.392\t Batch accuracy: 25.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 5960/27699\t Batch loss: 4.423\t Batch perplexity: 83.387\t Batch accuracy: 26.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 5980/27699\t Batch loss: 4.439\t Batch perplexity: 84.690\t Batch accuracy: 27.8%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 6000/27699\t Batch loss: 4.717\t Batch perplexity: 111.861\t Batch accuracy: 23.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.150\t Batch perplexity: 63.437\t Batch accuracy: 27.3%\t***\n",
      "Epoch: 1/2\t Train step: 6020/27699\t Batch loss: 4.161\t Batch perplexity: 64.165\t Batch accuracy: 26.4%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 6040/27699\t Batch loss: 4.623\t Batch perplexity: 101.807\t Batch accuracy: 23.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 6060/27699\t Batch loss: 4.599\t Batch perplexity: 99.412\t Batch accuracy: 23.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 6080/27699\t Batch loss: 4.366\t Batch perplexity: 78.748\t Batch accuracy: 25.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 6100/27699\t Batch loss: 4.316\t Batch perplexity: 74.872\t Batch accuracy: 29.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 6120/27699\t Batch loss: 4.466\t Batch perplexity: 87.031\t Batch accuracy: 25.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 6140/27699\t Batch loss: 4.498\t Batch perplexity: 89.855\t Batch accuracy: 26.0%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 6160/27699\t Batch loss: 4.367\t Batch perplexity: 78.840\t Batch accuracy: 25.3%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 6180/27699\t Batch loss: 4.515\t Batch perplexity: 91.356\t Batch accuracy: 25.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 6200/27699\t Batch loss: 4.331\t Batch perplexity: 76.032\t Batch accuracy: 28.1%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 6220/27699\t Batch loss: 4.172\t Batch perplexity: 64.814\t Batch accuracy: 29.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 6240/27699\t Batch loss: 4.423\t Batch perplexity: 83.376\t Batch accuracy: 25.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.397\t Batch perplexity: 81.227\t Batch accuracy: 28.1%\t***\n",
      "Epoch: 1/2\t Train step: 6260/27699\t Batch loss: 4.426\t Batch perplexity: 83.621\t Batch accuracy: 25.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 6280/27699\t Batch loss: 4.396\t Batch perplexity: 81.132\t Batch accuracy: 25.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 6300/27699\t Batch loss: 4.299\t Batch perplexity: 73.650\t Batch accuracy: 25.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 6320/27699\t Batch loss: 4.328\t Batch perplexity: 75.766\t Batch accuracy: 27.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 6340/27699\t Batch loss: 4.478\t Batch perplexity: 88.099\t Batch accuracy: 26.3%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 6360/27699\t Batch loss: 4.540\t Batch perplexity: 93.655\t Batch accuracy: 25.6%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 6380/27699\t Batch loss: 4.375\t Batch perplexity: 79.409\t Batch accuracy: 26.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 6400/27699\t Batch loss: 4.325\t Batch perplexity: 75.556\t Batch accuracy: 27.2%\t 0.72s/batch\n",
      "Epoch: 1/2\t Train step: 6420/27699\t Batch loss: 4.512\t Batch perplexity: 91.122\t Batch accuracy: 25.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 6440/27699\t Batch loss: 4.519\t Batch perplexity: 91.752\t Batch accuracy: 23.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 6460/27699\t Batch loss: 4.519\t Batch perplexity: 91.736\t Batch accuracy: 24.4%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 6480/27699\t Batch loss: 4.315\t Batch perplexity: 74.818\t Batch accuracy: 27.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 6500/27699\t Batch loss: 4.285\t Batch perplexity: 72.589\t Batch accuracy: 26.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.279\t Batch perplexity: 72.198\t Batch accuracy: 28.0%\t***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\t Train step: 6520/27699\t Batch loss: 4.595\t Batch perplexity: 98.971\t Batch accuracy: 24.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 6540/27699\t Batch loss: 4.413\t Batch perplexity: 82.520\t Batch accuracy: 26.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 6560/27699\t Batch loss: 4.311\t Batch perplexity: 74.542\t Batch accuracy: 27.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 6580/27699\t Batch loss: 4.280\t Batch perplexity: 72.230\t Batch accuracy: 27.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 6600/27699\t Batch loss: 4.434\t Batch perplexity: 84.284\t Batch accuracy: 26.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 6620/27699\t Batch loss: 4.531\t Batch perplexity: 92.825\t Batch accuracy: 24.3%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 6640/27699\t Batch loss: 4.316\t Batch perplexity: 74.890\t Batch accuracy: 26.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 6660/27699\t Batch loss: 4.274\t Batch perplexity: 71.827\t Batch accuracy: 27.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 6680/27699\t Batch loss: 4.343\t Batch perplexity: 76.974\t Batch accuracy: 25.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 6700/27699\t Batch loss: 4.487\t Batch perplexity: 88.865\t Batch accuracy: 25.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 6720/27699\t Batch loss: 4.307\t Batch perplexity: 74.251\t Batch accuracy: 25.1%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 6740/27699\t Batch loss: 4.582\t Batch perplexity: 97.677\t Batch accuracy: 24.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 3.949\t Batch perplexity: 51.888\t Batch accuracy: 31.0%\t***\n",
      "Epoch: 1/2\t Train step: 6760/27699\t Batch loss: 4.392\t Batch perplexity: 80.798\t Batch accuracy: 26.9%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 6780/27699\t Batch loss: 4.372\t Batch perplexity: 79.201\t Batch accuracy: 28.2%\t 0.76s/batch\n",
      "Epoch: 1/2\t Train step: 6800/27699\t Batch loss: 4.210\t Batch perplexity: 67.340\t Batch accuracy: 28.5%\t 0.72s/batch\n",
      "Epoch: 1/2\t Train step: 6820/27699\t Batch loss: 4.300\t Batch perplexity: 73.716\t Batch accuracy: 27.0%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 6840/27699\t Batch loss: 4.445\t Batch perplexity: 85.182\t Batch accuracy: 26.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 6860/27699\t Batch loss: 4.376\t Batch perplexity: 79.552\t Batch accuracy: 25.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 6880/27699\t Batch loss: 4.480\t Batch perplexity: 88.277\t Batch accuracy: 25.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 6900/27699\t Batch loss: 4.451\t Batch perplexity: 85.748\t Batch accuracy: 27.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 6920/27699\t Batch loss: 4.545\t Batch perplexity: 94.194\t Batch accuracy: 24.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 6940/27699\t Batch loss: 4.318\t Batch perplexity: 75.043\t Batch accuracy: 26.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 6960/27699\t Batch loss: 4.308\t Batch perplexity: 74.303\t Batch accuracy: 24.6%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 6980/27699\t Batch loss: 4.549\t Batch perplexity: 94.529\t Batch accuracy: 24.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 7000/27699\t Batch loss: 4.357\t Batch perplexity: 78.004\t Batch accuracy: 27.3%\t 0.70s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.207\t Batch perplexity: 67.184\t Batch accuracy: 27.2%\t***\n",
      "Epoch: 1/2\t Train step: 7020/27699\t Batch loss: 4.534\t Batch perplexity: 93.107\t Batch accuracy: 26.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 7040/27699\t Batch loss: 4.232\t Batch perplexity: 68.832\t Batch accuracy: 27.8%\t 0.79s/batch\n",
      "Epoch: 1/2\t Train step: 7060/27699\t Batch loss: 4.156\t Batch perplexity: 63.823\t Batch accuracy: 28.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 7080/27699\t Batch loss: 4.545\t Batch perplexity: 94.193\t Batch accuracy: 25.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 7100/27699\t Batch loss: 4.355\t Batch perplexity: 77.837\t Batch accuracy: 27.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 7120/27699\t Batch loss: 4.465\t Batch perplexity: 86.938\t Batch accuracy: 23.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 7140/27699\t Batch loss: 4.422\t Batch perplexity: 83.240\t Batch accuracy: 24.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 7160/27699\t Batch loss: 4.543\t Batch perplexity: 93.978\t Batch accuracy: 25.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 7180/27699\t Batch loss: 4.525\t Batch perplexity: 92.290\t Batch accuracy: 25.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 7200/27699\t Batch loss: 4.126\t Batch perplexity: 61.943\t Batch accuracy: 28.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 7220/27699\t Batch loss: 4.642\t Batch perplexity: 103.783\t Batch accuracy: 22.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 7240/27699\t Batch loss: 4.379\t Batch perplexity: 79.757\t Batch accuracy: 24.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.304\t Batch perplexity: 74.022\t Batch accuracy: 26.7%\t***\n",
      "Epoch: 1/2\t Train step: 7260/27699\t Batch loss: 4.400\t Batch perplexity: 81.481\t Batch accuracy: 26.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 7280/27699\t Batch loss: 4.199\t Batch perplexity: 66.642\t Batch accuracy: 27.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 7300/27699\t Batch loss: 4.289\t Batch perplexity: 72.901\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 7320/27699\t Batch loss: 4.360\t Batch perplexity: 78.270\t Batch accuracy: 25.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 7340/27699\t Batch loss: 4.414\t Batch perplexity: 82.615\t Batch accuracy: 26.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 7360/27699\t Batch loss: 4.339\t Batch perplexity: 76.613\t Batch accuracy: 26.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 7380/27699\t Batch loss: 4.286\t Batch perplexity: 72.694\t Batch accuracy: 29.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 7400/27699\t Batch loss: 4.359\t Batch perplexity: 78.157\t Batch accuracy: 28.0%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 7420/27699\t Batch loss: 4.331\t Batch perplexity: 75.993\t Batch accuracy: 24.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 7440/27699\t Batch loss: 4.191\t Batch perplexity: 66.056\t Batch accuracy: 28.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 7460/27699\t Batch loss: 4.342\t Batch perplexity: 76.898\t Batch accuracy: 25.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 7480/27699\t Batch loss: 4.464\t Batch perplexity: 86.843\t Batch accuracy: 26.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 7500/27699\t Batch loss: 4.233\t Batch perplexity: 68.928\t Batch accuracy: 27.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.266\t Batch perplexity: 71.232\t Batch accuracy: 29.1%\t***\n",
      "Epoch: 1/2\t Train step: 7520/27699\t Batch loss: 4.265\t Batch perplexity: 71.178\t Batch accuracy: 27.3%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 7540/27699\t Batch loss: 4.235\t Batch perplexity: 69.039\t Batch accuracy: 24.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 7560/27699\t Batch loss: 4.402\t Batch perplexity: 81.596\t Batch accuracy: 27.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 7580/27699\t Batch loss: 4.357\t Batch perplexity: 78.053\t Batch accuracy: 27.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 7600/27699\t Batch loss: 4.545\t Batch perplexity: 94.196\t Batch accuracy: 24.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 7620/27699\t Batch loss: 4.316\t Batch perplexity: 74.891\t Batch accuracy: 27.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 7640/27699\t Batch loss: 4.281\t Batch perplexity: 72.336\t Batch accuracy: 27.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 7660/27699\t Batch loss: 4.396\t Batch perplexity: 81.160\t Batch accuracy: 25.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 7680/27699\t Batch loss: 4.317\t Batch perplexity: 74.937\t Batch accuracy: 28.2%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 7700/27699\t Batch loss: 4.183\t Batch perplexity: 65.537\t Batch accuracy: 27.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 7720/27699\t Batch loss: 4.472\t Batch perplexity: 87.516\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 7740/27699\t Batch loss: 4.448\t Batch perplexity: 85.467\t Batch accuracy: 23.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.130\t Batch perplexity: 62.189\t Batch accuracy: 28.2%\t***\n",
      "Epoch: 1/2\t Train step: 7760/27699\t Batch loss: 4.424\t Batch perplexity: 83.395\t Batch accuracy: 25.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 7780/27699\t Batch loss: 4.328\t Batch perplexity: 75.830\t Batch accuracy: 27.4%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 7800/27699\t Batch loss: 4.204\t Batch perplexity: 66.962\t Batch accuracy: 27.2%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 7820/27699\t Batch loss: 4.456\t Batch perplexity: 86.147\t Batch accuracy: 26.2%\t 0.67s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\t Train step: 7840/27699\t Batch loss: 4.139\t Batch perplexity: 62.712\t Batch accuracy: 29.9%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 7860/27699\t Batch loss: 4.337\t Batch perplexity: 76.514\t Batch accuracy: 26.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 7880/27699\t Batch loss: 4.348\t Batch perplexity: 77.357\t Batch accuracy: 28.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 7900/27699\t Batch loss: 4.118\t Batch perplexity: 61.414\t Batch accuracy: 27.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 7920/27699\t Batch loss: 4.500\t Batch perplexity: 90.014\t Batch accuracy: 24.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 7940/27699\t Batch loss: 4.153\t Batch perplexity: 63.619\t Batch accuracy: 28.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 7960/27699\t Batch loss: 4.295\t Batch perplexity: 73.313\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 7980/27699\t Batch loss: 4.223\t Batch perplexity: 68.269\t Batch accuracy: 27.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 8000/27699\t Batch loss: 4.627\t Batch perplexity: 102.198\t Batch accuracy: 24.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.299\t Batch perplexity: 73.606\t Batch accuracy: 26.6%\t***\n",
      "Epoch: 1/2\t Train step: 8020/27699\t Batch loss: 4.377\t Batch perplexity: 79.571\t Batch accuracy: 27.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 8040/27699\t Batch loss: 4.456\t Batch perplexity: 86.157\t Batch accuracy: 24.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 8060/27699\t Batch loss: 4.282\t Batch perplexity: 72.415\t Batch accuracy: 27.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 8080/27699\t Batch loss: 4.260\t Batch perplexity: 70.838\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 8100/27699\t Batch loss: 4.433\t Batch perplexity: 84.156\t Batch accuracy: 25.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 8120/27699\t Batch loss: 4.277\t Batch perplexity: 72.011\t Batch accuracy: 27.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 8140/27699\t Batch loss: 4.390\t Batch perplexity: 80.657\t Batch accuracy: 29.1%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 8160/27699\t Batch loss: 4.328\t Batch perplexity: 75.803\t Batch accuracy: 27.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 8180/27699\t Batch loss: 4.182\t Batch perplexity: 65.488\t Batch accuracy: 27.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 8200/27699\t Batch loss: 4.194\t Batch perplexity: 66.312\t Batch accuracy: 27.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 8220/27699\t Batch loss: 4.318\t Batch perplexity: 75.035\t Batch accuracy: 27.0%\t 0.74s/batch\n",
      "Epoch: 1/2\t Train step: 8240/27699\t Batch loss: 4.589\t Batch perplexity: 98.408\t Batch accuracy: 25.0%\t 0.70s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.149\t Batch perplexity: 63.390\t Batch accuracy: 27.3%\t***\n",
      "Epoch: 1/2\t Train step: 8260/27699\t Batch loss: 4.167\t Batch perplexity: 64.510\t Batch accuracy: 28.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 8280/27699\t Batch loss: 4.360\t Batch perplexity: 78.293\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 8300/27699\t Batch loss: 4.236\t Batch perplexity: 69.160\t Batch accuracy: 27.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 8320/27699\t Batch loss: 4.583\t Batch perplexity: 97.844\t Batch accuracy: 24.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 8340/27699\t Batch loss: 4.483\t Batch perplexity: 88.484\t Batch accuracy: 27.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 8360/27699\t Batch loss: 4.326\t Batch perplexity: 75.634\t Batch accuracy: 25.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 8380/27699\t Batch loss: 4.264\t Batch perplexity: 71.093\t Batch accuracy: 28.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 8400/27699\t Batch loss: 4.495\t Batch perplexity: 89.566\t Batch accuracy: 26.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 8420/27699\t Batch loss: 4.224\t Batch perplexity: 68.294\t Batch accuracy: 28.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 8440/27699\t Batch loss: 4.269\t Batch perplexity: 71.467\t Batch accuracy: 28.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 8460/27699\t Batch loss: 4.318\t Batch perplexity: 75.038\t Batch accuracy: 26.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 8480/27699\t Batch loss: 4.324\t Batch perplexity: 75.521\t Batch accuracy: 29.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 8500/27699\t Batch loss: 4.207\t Batch perplexity: 67.177\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.024\t Batch perplexity: 55.931\t Batch accuracy: 29.9%\t***\n",
      "Epoch: 1/2\t Train step: 8520/27699\t Batch loss: 4.243\t Batch perplexity: 69.629\t Batch accuracy: 26.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 8540/27699\t Batch loss: 4.284\t Batch perplexity: 72.512\t Batch accuracy: 27.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 8560/27699\t Batch loss: 4.285\t Batch perplexity: 72.605\t Batch accuracy: 25.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 8580/27699\t Batch loss: 4.397\t Batch perplexity: 81.211\t Batch accuracy: 24.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 8600/27699\t Batch loss: 4.401\t Batch perplexity: 81.523\t Batch accuracy: 26.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 8620/27699\t Batch loss: 4.354\t Batch perplexity: 77.756\t Batch accuracy: 26.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 8640/27699\t Batch loss: 4.291\t Batch perplexity: 73.039\t Batch accuracy: 30.2%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 8660/27699\t Batch loss: 4.351\t Batch perplexity: 77.532\t Batch accuracy: 26.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 8680/27699\t Batch loss: 4.379\t Batch perplexity: 79.768\t Batch accuracy: 24.4%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 8700/27699\t Batch loss: 4.511\t Batch perplexity: 91.051\t Batch accuracy: 26.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 8720/27699\t Batch loss: 4.237\t Batch perplexity: 69.186\t Batch accuracy: 28.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 8740/27699\t Batch loss: 4.529\t Batch perplexity: 92.635\t Batch accuracy: 25.6%\t 0.65s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.312\t Batch perplexity: 74.604\t Batch accuracy: 26.3%\t***\n",
      "Epoch: 1/2\t Train step: 8760/27699\t Batch loss: 4.424\t Batch perplexity: 83.394\t Batch accuracy: 25.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 8780/27699\t Batch loss: 4.501\t Batch perplexity: 90.068\t Batch accuracy: 23.9%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 8800/27699\t Batch loss: 4.611\t Batch perplexity: 100.562\t Batch accuracy: 23.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 8820/27699\t Batch loss: 4.390\t Batch perplexity: 80.613\t Batch accuracy: 26.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 8840/27699\t Batch loss: 4.301\t Batch perplexity: 73.775\t Batch accuracy: 26.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 8860/27699\t Batch loss: 4.400\t Batch perplexity: 81.428\t Batch accuracy: 28.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 8880/27699\t Batch loss: 4.307\t Batch perplexity: 74.189\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 8900/27699\t Batch loss: 4.350\t Batch perplexity: 77.502\t Batch accuracy: 27.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 8920/27699\t Batch loss: 4.288\t Batch perplexity: 72.814\t Batch accuracy: 29.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 8940/27699\t Batch loss: 4.305\t Batch perplexity: 74.069\t Batch accuracy: 26.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 8960/27699\t Batch loss: 4.390\t Batch perplexity: 80.629\t Batch accuracy: 26.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 8980/27699\t Batch loss: 4.384\t Batch perplexity: 80.180\t Batch accuracy: 26.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 9000/27699\t Batch loss: 4.443\t Batch perplexity: 85.040\t Batch accuracy: 26.5%\t 0.73s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.415\t Batch perplexity: 82.721\t Batch accuracy: 27.5%\t***\n",
      "Epoch: 1/2\t Train step: 9020/27699\t Batch loss: 4.185\t Batch perplexity: 65.690\t Batch accuracy: 28.2%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 9040/27699\t Batch loss: 4.341\t Batch perplexity: 76.792\t Batch accuracy: 26.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 9060/27699\t Batch loss: 4.385\t Batch perplexity: 80.252\t Batch accuracy: 26.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 9080/27699\t Batch loss: 4.527\t Batch perplexity: 92.505\t Batch accuracy: 26.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 9100/27699\t Batch loss: 4.460\t Batch perplexity: 86.448\t Batch accuracy: 25.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 9120/27699\t Batch loss: 4.184\t Batch perplexity: 65.619\t Batch accuracy: 27.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 9140/27699\t Batch loss: 4.193\t Batch perplexity: 66.219\t Batch accuracy: 26.0%\t 0.69s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\t Train step: 9160/27699\t Batch loss: 4.329\t Batch perplexity: 75.885\t Batch accuracy: 26.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 9180/27699\t Batch loss: 4.325\t Batch perplexity: 75.559\t Batch accuracy: 26.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 9200/27699\t Batch loss: 4.049\t Batch perplexity: 57.362\t Batch accuracy: 30.6%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 9220/27699\t Batch loss: 4.328\t Batch perplexity: 75.807\t Batch accuracy: 25.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 9240/27699\t Batch loss: 4.266\t Batch perplexity: 71.220\t Batch accuracy: 25.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.309\t Batch perplexity: 74.333\t Batch accuracy: 28.8%\t***\n",
      "Epoch: 1/2\t Train step: 9260/27699\t Batch loss: 4.481\t Batch perplexity: 88.290\t Batch accuracy: 22.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 9280/27699\t Batch loss: 4.270\t Batch perplexity: 71.541\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 9300/27699\t Batch loss: 4.490\t Batch perplexity: 89.114\t Batch accuracy: 24.8%\t 0.73s/batch\n",
      "Epoch: 1/2\t Train step: 9320/27699\t Batch loss: 4.496\t Batch perplexity: 89.660\t Batch accuracy: 26.6%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 9340/27699\t Batch loss: 4.271\t Batch perplexity: 71.589\t Batch accuracy: 29.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 9360/27699\t Batch loss: 4.266\t Batch perplexity: 71.208\t Batch accuracy: 26.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 9380/27699\t Batch loss: 4.100\t Batch perplexity: 60.319\t Batch accuracy: 28.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 9400/27699\t Batch loss: 4.419\t Batch perplexity: 83.025\t Batch accuracy: 24.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 9420/27699\t Batch loss: 4.388\t Batch perplexity: 80.464\t Batch accuracy: 25.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 9440/27699\t Batch loss: 4.535\t Batch perplexity: 93.233\t Batch accuracy: 24.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 9460/27699\t Batch loss: 4.398\t Batch perplexity: 81.273\t Batch accuracy: 26.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 9480/27699\t Batch loss: 4.396\t Batch perplexity: 81.130\t Batch accuracy: 28.2%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 9500/27699\t Batch loss: 4.453\t Batch perplexity: 85.861\t Batch accuracy: 25.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.128\t Batch perplexity: 62.063\t Batch accuracy: 30.1%\t***\n",
      "Epoch: 1/2\t Train step: 9520/27699\t Batch loss: 4.304\t Batch perplexity: 73.970\t Batch accuracy: 27.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 9540/27699\t Batch loss: 4.465\t Batch perplexity: 86.934\t Batch accuracy: 26.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 9560/27699\t Batch loss: 4.329\t Batch perplexity: 75.867\t Batch accuracy: 25.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 9580/27699\t Batch loss: 4.260\t Batch perplexity: 70.810\t Batch accuracy: 24.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 9600/27699\t Batch loss: 4.619\t Batch perplexity: 101.364\t Batch accuracy: 22.2%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 9620/27699\t Batch loss: 4.270\t Batch perplexity: 71.532\t Batch accuracy: 30.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 9640/27699\t Batch loss: 4.261\t Batch perplexity: 70.866\t Batch accuracy: 26.4%\t 0.76s/batch\n",
      "Epoch: 1/2\t Train step: 9660/27699\t Batch loss: 4.347\t Batch perplexity: 77.247\t Batch accuracy: 26.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 9680/27699\t Batch loss: 4.155\t Batch perplexity: 63.778\t Batch accuracy: 25.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 9700/27699\t Batch loss: 4.174\t Batch perplexity: 64.956\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 9720/27699\t Batch loss: 4.156\t Batch perplexity: 63.832\t Batch accuracy: 28.1%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 9740/27699\t Batch loss: 4.482\t Batch perplexity: 88.443\t Batch accuracy: 27.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.169\t Batch perplexity: 64.640\t Batch accuracy: 27.9%\t***\n",
      "Epoch: 1/2\t Train step: 9760/27699\t Batch loss: 4.265\t Batch perplexity: 71.131\t Batch accuracy: 26.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 9780/27699\t Batch loss: 4.521\t Batch perplexity: 91.919\t Batch accuracy: 25.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 9800/27699\t Batch loss: 4.226\t Batch perplexity: 68.455\t Batch accuracy: 28.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 9820/27699\t Batch loss: 4.098\t Batch perplexity: 60.224\t Batch accuracy: 29.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 9840/27699\t Batch loss: 4.337\t Batch perplexity: 76.514\t Batch accuracy: 27.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 9860/27699\t Batch loss: 4.322\t Batch perplexity: 75.370\t Batch accuracy: 27.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 9880/27699\t Batch loss: 4.161\t Batch perplexity: 64.155\t Batch accuracy: 29.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 9900/27699\t Batch loss: 4.519\t Batch perplexity: 91.764\t Batch accuracy: 26.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 9920/27699\t Batch loss: 4.329\t Batch perplexity: 75.900\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 9940/27699\t Batch loss: 4.392\t Batch perplexity: 80.762\t Batch accuracy: 28.2%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 9960/27699\t Batch loss: 4.480\t Batch perplexity: 88.209\t Batch accuracy: 27.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 9980/27699\t Batch loss: 4.338\t Batch perplexity: 76.582\t Batch accuracy: 27.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 10000/27699\t Batch loss: 4.248\t Batch perplexity: 69.958\t Batch accuracy: 28.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.134\t Batch perplexity: 62.406\t Batch accuracy: 28.7%\t***\n",
      "Epoch: 1/2\t Train step: 10020/27699\t Batch loss: 4.581\t Batch perplexity: 97.658\t Batch accuracy: 25.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 10040/27699\t Batch loss: 4.130\t Batch perplexity: 62.147\t Batch accuracy: 28.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 10060/27699\t Batch loss: 4.458\t Batch perplexity: 86.317\t Batch accuracy: 26.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 10080/27699\t Batch loss: 4.446\t Batch perplexity: 85.251\t Batch accuracy: 25.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 10100/27699\t Batch loss: 4.399\t Batch perplexity: 81.350\t Batch accuracy: 23.0%\t 0.72s/batch\n",
      "Epoch: 1/2\t Train step: 10120/27699\t Batch loss: 4.336\t Batch perplexity: 76.388\t Batch accuracy: 25.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 10140/27699\t Batch loss: 4.413\t Batch perplexity: 82.511\t Batch accuracy: 26.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 10160/27699\t Batch loss: 4.483\t Batch perplexity: 88.523\t Batch accuracy: 27.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 10180/27699\t Batch loss: 4.432\t Batch perplexity: 84.117\t Batch accuracy: 25.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 10200/27699\t Batch loss: 4.320\t Batch perplexity: 75.218\t Batch accuracy: 26.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 10220/27699\t Batch loss: 4.368\t Batch perplexity: 78.897\t Batch accuracy: 27.2%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 10240/27699\t Batch loss: 4.092\t Batch perplexity: 59.869\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.051\t Batch perplexity: 57.458\t Batch accuracy: 28.3%\t***\n",
      "Epoch: 1/2\t Train step: 10260/27699\t Batch loss: 4.286\t Batch perplexity: 72.645\t Batch accuracy: 26.2%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 10280/27699\t Batch loss: 4.369\t Batch perplexity: 78.938\t Batch accuracy: 26.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 10300/27699\t Batch loss: 4.354\t Batch perplexity: 77.763\t Batch accuracy: 27.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 10320/27699\t Batch loss: 4.377\t Batch perplexity: 79.609\t Batch accuracy: 25.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 10340/27699\t Batch loss: 4.276\t Batch perplexity: 71.927\t Batch accuracy: 25.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 10360/27699\t Batch loss: 4.308\t Batch perplexity: 74.305\t Batch accuracy: 26.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 10380/27699\t Batch loss: 4.402\t Batch perplexity: 81.605\t Batch accuracy: 26.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 10400/27699\t Batch loss: 4.210\t Batch perplexity: 67.323\t Batch accuracy: 28.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 10420/27699\t Batch loss: 4.400\t Batch perplexity: 81.464\t Batch accuracy: 26.5%\t 0.74s/batch\n",
      "Epoch: 1/2\t Train step: 10440/27699\t Batch loss: 4.319\t Batch perplexity: 75.098\t Batch accuracy: 26.5%\t 0.67s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\t Train step: 10460/27699\t Batch loss: 4.345\t Batch perplexity: 77.122\t Batch accuracy: 30.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 10480/27699\t Batch loss: 3.925\t Batch perplexity: 50.637\t Batch accuracy: 29.2%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 10500/27699\t Batch loss: 4.241\t Batch perplexity: 69.494\t Batch accuracy: 26.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.247\t Batch perplexity: 69.918\t Batch accuracy: 28.6%\t***\n",
      "Epoch: 1/2\t Train step: 10520/27699\t Batch loss: 4.156\t Batch perplexity: 63.797\t Batch accuracy: 26.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 10540/27699\t Batch loss: 4.297\t Batch perplexity: 73.499\t Batch accuracy: 26.1%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 10560/27699\t Batch loss: 4.451\t Batch perplexity: 85.690\t Batch accuracy: 25.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 10580/27699\t Batch loss: 4.196\t Batch perplexity: 66.439\t Batch accuracy: 30.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 10600/27699\t Batch loss: 4.361\t Batch perplexity: 78.310\t Batch accuracy: 27.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 10620/27699\t Batch loss: 4.486\t Batch perplexity: 88.777\t Batch accuracy: 26.0%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 10640/27699\t Batch loss: 4.325\t Batch perplexity: 75.534\t Batch accuracy: 27.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 10660/27699\t Batch loss: 4.276\t Batch perplexity: 71.966\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 10680/27699\t Batch loss: 4.404\t Batch perplexity: 81.786\t Batch accuracy: 24.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 10700/27699\t Batch loss: 4.419\t Batch perplexity: 83.027\t Batch accuracy: 25.2%\t 0.74s/batch\n",
      "Epoch: 1/2\t Train step: 10720/27699\t Batch loss: 4.235\t Batch perplexity: 69.039\t Batch accuracy: 28.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 10740/27699\t Batch loss: 4.230\t Batch perplexity: 68.724\t Batch accuracy: 25.8%\t 0.73s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.228\t Batch perplexity: 68.585\t Batch accuracy: 25.6%\t***\n",
      "Epoch: 1/2\t Train step: 10760/27699\t Batch loss: 4.271\t Batch perplexity: 71.606\t Batch accuracy: 29.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 10780/27699\t Batch loss: 4.417\t Batch perplexity: 82.855\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 10800/27699\t Batch loss: 4.424\t Batch perplexity: 83.465\t Batch accuracy: 27.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 10820/27699\t Batch loss: 4.172\t Batch perplexity: 64.855\t Batch accuracy: 28.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 10840/27699\t Batch loss: 4.348\t Batch perplexity: 77.349\t Batch accuracy: 26.9%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 10860/27699\t Batch loss: 4.218\t Batch perplexity: 67.923\t Batch accuracy: 27.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 10880/27699\t Batch loss: 4.276\t Batch perplexity: 71.960\t Batch accuracy: 29.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 10900/27699\t Batch loss: 4.552\t Batch perplexity: 94.830\t Batch accuracy: 26.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 10920/27699\t Batch loss: 4.329\t Batch perplexity: 75.855\t Batch accuracy: 27.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 10940/27699\t Batch loss: 3.997\t Batch perplexity: 54.438\t Batch accuracy: 28.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 10960/27699\t Batch loss: 4.338\t Batch perplexity: 76.575\t Batch accuracy: 28.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 10980/27699\t Batch loss: 4.012\t Batch perplexity: 55.261\t Batch accuracy: 30.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 11000/27699\t Batch loss: 4.296\t Batch perplexity: 73.418\t Batch accuracy: 27.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.178\t Batch perplexity: 65.229\t Batch accuracy: 27.1%\t***\n",
      "Epoch: 1/2\t Train step: 11020/27699\t Batch loss: 4.291\t Batch perplexity: 73.037\t Batch accuracy: 24.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 11040/27699\t Batch loss: 4.302\t Batch perplexity: 73.877\t Batch accuracy: 27.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 11060/27699\t Batch loss: 4.556\t Batch perplexity: 95.231\t Batch accuracy: 25.7%\t 0.72s/batch\n",
      "Epoch: 1/2\t Train step: 11080/27699\t Batch loss: 4.294\t Batch perplexity: 73.272\t Batch accuracy: 26.4%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 11100/27699\t Batch loss: 4.288\t Batch perplexity: 72.795\t Batch accuracy: 29.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 11120/27699\t Batch loss: 4.420\t Batch perplexity: 83.132\t Batch accuracy: 25.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 11140/27699\t Batch loss: 4.216\t Batch perplexity: 67.740\t Batch accuracy: 26.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 11160/27699\t Batch loss: 4.401\t Batch perplexity: 81.533\t Batch accuracy: 27.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 11180/27699\t Batch loss: 4.193\t Batch perplexity: 66.235\t Batch accuracy: 25.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 11200/27699\t Batch loss: 4.387\t Batch perplexity: 80.416\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 11220/27699\t Batch loss: 4.512\t Batch perplexity: 91.060\t Batch accuracy: 25.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 11240/27699\t Batch loss: 4.454\t Batch perplexity: 85.968\t Batch accuracy: 27.9%\t 0.70s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.265\t Batch perplexity: 71.152\t Batch accuracy: 25.2%\t***\n",
      "Epoch: 1/2\t Train step: 11260/27699\t Batch loss: 4.515\t Batch perplexity: 91.364\t Batch accuracy: 26.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 11280/27699\t Batch loss: 4.103\t Batch perplexity: 60.541\t Batch accuracy: 29.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 11300/27699\t Batch loss: 4.293\t Batch perplexity: 73.161\t Batch accuracy: 26.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 11320/27699\t Batch loss: 4.286\t Batch perplexity: 72.680\t Batch accuracy: 27.6%\t 0.72s/batch\n",
      "Epoch: 1/2\t Train step: 11340/27699\t Batch loss: 4.275\t Batch perplexity: 71.878\t Batch accuracy: 26.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 11360/27699\t Batch loss: 4.464\t Batch perplexity: 86.793\t Batch accuracy: 27.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 11380/27699\t Batch loss: 4.190\t Batch perplexity: 65.993\t Batch accuracy: 26.8%\t 0.73s/batch\n",
      "Epoch: 1/2\t Train step: 11400/27699\t Batch loss: 4.473\t Batch perplexity: 87.639\t Batch accuracy: 26.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 11420/27699\t Batch loss: 4.369\t Batch perplexity: 78.942\t Batch accuracy: 25.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 11440/27699\t Batch loss: 4.042\t Batch perplexity: 56.955\t Batch accuracy: 30.0%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 11460/27699\t Batch loss: 4.258\t Batch perplexity: 70.637\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 11480/27699\t Batch loss: 4.467\t Batch perplexity: 87.136\t Batch accuracy: 26.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 11500/27699\t Batch loss: 4.368\t Batch perplexity: 78.903\t Batch accuracy: 26.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.169\t Batch perplexity: 64.644\t Batch accuracy: 28.1%\t***\n",
      "Epoch: 1/2\t Train step: 11520/27699\t Batch loss: 4.517\t Batch perplexity: 91.526\t Batch accuracy: 24.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 11540/27699\t Batch loss: 4.430\t Batch perplexity: 83.936\t Batch accuracy: 26.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 11560/27699\t Batch loss: 4.524\t Batch perplexity: 92.221\t Batch accuracy: 24.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 11580/27699\t Batch loss: 4.290\t Batch perplexity: 72.935\t Batch accuracy: 26.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 11600/27699\t Batch loss: 4.300\t Batch perplexity: 73.723\t Batch accuracy: 27.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 11620/27699\t Batch loss: 4.363\t Batch perplexity: 78.494\t Batch accuracy: 26.7%\t 0.72s/batch\n",
      "Epoch: 1/2\t Train step: 11640/27699\t Batch loss: 4.323\t Batch perplexity: 75.438\t Batch accuracy: 28.7%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 11660/27699\t Batch loss: 4.342\t Batch perplexity: 76.882\t Batch accuracy: 25.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 11680/27699\t Batch loss: 4.082\t Batch perplexity: 59.260\t Batch accuracy: 29.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 11700/27699\t Batch loss: 4.408\t Batch perplexity: 82.077\t Batch accuracy: 26.7%\t 0.80s/batch\n",
      "Epoch: 1/2\t Train step: 11720/27699\t Batch loss: 4.280\t Batch perplexity: 72.276\t Batch accuracy: 28.5%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 11740/27699\t Batch loss: 4.179\t Batch perplexity: 65.282\t Batch accuracy: 27.0%\t 0.70s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.104\t Batch perplexity: 60.561\t Batch accuracy: 27.6%\t***\n",
      "Epoch: 1/2\t Train step: 11760/27699\t Batch loss: 4.125\t Batch perplexity: 61.871\t Batch accuracy: 26.8%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 11780/27699\t Batch loss: 4.360\t Batch perplexity: 78.236\t Batch accuracy: 28.1%\t 0.76s/batch\n",
      "Epoch: 1/2\t Train step: 11800/27699\t Batch loss: 4.260\t Batch perplexity: 70.795\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 11820/27699\t Batch loss: 4.219\t Batch perplexity: 67.936\t Batch accuracy: 26.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 11840/27699\t Batch loss: 4.208\t Batch perplexity: 67.209\t Batch accuracy: 28.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 11860/27699\t Batch loss: 4.311\t Batch perplexity: 74.515\t Batch accuracy: 23.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 11880/27699\t Batch loss: 4.462\t Batch perplexity: 86.687\t Batch accuracy: 26.2%\t 0.65s/batch\n",
      "Epoch: 1/2\t Train step: 11900/27699\t Batch loss: 4.082\t Batch perplexity: 59.260\t Batch accuracy: 28.8%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 11920/27699\t Batch loss: 4.147\t Batch perplexity: 63.272\t Batch accuracy: 26.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 11940/27699\t Batch loss: 4.286\t Batch perplexity: 72.670\t Batch accuracy: 28.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 11960/27699\t Batch loss: 4.256\t Batch perplexity: 70.509\t Batch accuracy: 28.2%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 11980/27699\t Batch loss: 4.321\t Batch perplexity: 75.263\t Batch accuracy: 25.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 12000/27699\t Batch loss: 4.192\t Batch perplexity: 66.160\t Batch accuracy: 28.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.004\t Batch perplexity: 54.802\t Batch accuracy: 29.2%\t***\n",
      "Epoch: 1/2\t Train step: 12020/27699\t Batch loss: 4.344\t Batch perplexity: 76.999\t Batch accuracy: 28.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 12040/27699\t Batch loss: 4.371\t Batch perplexity: 79.126\t Batch accuracy: 26.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 12060/27699\t Batch loss: 4.104\t Batch perplexity: 60.606\t Batch accuracy: 28.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 12080/27699\t Batch loss: 4.153\t Batch perplexity: 63.634\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 12100/27699\t Batch loss: 4.243\t Batch perplexity: 69.607\t Batch accuracy: 28.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 12120/27699\t Batch loss: 4.163\t Batch perplexity: 64.256\t Batch accuracy: 27.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 12140/27699\t Batch loss: 4.218\t Batch perplexity: 67.864\t Batch accuracy: 29.6%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 12160/27699\t Batch loss: 4.296\t Batch perplexity: 73.398\t Batch accuracy: 26.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 12180/27699\t Batch loss: 4.236\t Batch perplexity: 69.116\t Batch accuracy: 27.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 12200/27699\t Batch loss: 4.391\t Batch perplexity: 80.695\t Batch accuracy: 26.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 12220/27699\t Batch loss: 4.335\t Batch perplexity: 76.294\t Batch accuracy: 27.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 12240/27699\t Batch loss: 4.486\t Batch perplexity: 88.754\t Batch accuracy: 24.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 3.903\t Batch perplexity: 49.569\t Batch accuracy: 29.8%\t***\n",
      "Epoch: 1/2\t Train step: 12260/27699\t Batch loss: 4.204\t Batch perplexity: 66.960\t Batch accuracy: 25.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 12280/27699\t Batch loss: 4.325\t Batch perplexity: 75.536\t Batch accuracy: 25.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 12300/27699\t Batch loss: 4.104\t Batch perplexity: 60.558\t Batch accuracy: 29.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 12320/27699\t Batch loss: 4.324\t Batch perplexity: 75.494\t Batch accuracy: 25.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 12340/27699\t Batch loss: 4.374\t Batch perplexity: 79.376\t Batch accuracy: 27.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 12360/27699\t Batch loss: 4.245\t Batch perplexity: 69.783\t Batch accuracy: 27.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 12380/27699\t Batch loss: 4.418\t Batch perplexity: 82.924\t Batch accuracy: 23.1%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 12400/27699\t Batch loss: 4.201\t Batch perplexity: 66.760\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 12420/27699\t Batch loss: 4.222\t Batch perplexity: 68.200\t Batch accuracy: 25.7%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 12440/27699\t Batch loss: 4.052\t Batch perplexity: 57.540\t Batch accuracy: 26.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 12460/27699\t Batch loss: 4.142\t Batch perplexity: 62.959\t Batch accuracy: 27.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 12480/27699\t Batch loss: 4.340\t Batch perplexity: 76.675\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 12500/27699\t Batch loss: 4.182\t Batch perplexity: 65.485\t Batch accuracy: 29.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.135\t Batch perplexity: 62.491\t Batch accuracy: 28.5%\t***\n",
      "Epoch: 1/2\t Train step: 12520/27699\t Batch loss: 4.244\t Batch perplexity: 69.714\t Batch accuracy: 27.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 12540/27699\t Batch loss: 4.428\t Batch perplexity: 83.795\t Batch accuracy: 26.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 12560/27699\t Batch loss: 4.169\t Batch perplexity: 64.648\t Batch accuracy: 26.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 12580/27699\t Batch loss: 4.171\t Batch perplexity: 64.764\t Batch accuracy: 29.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 12600/27699\t Batch loss: 4.339\t Batch perplexity: 76.642\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 12620/27699\t Batch loss: 4.305\t Batch perplexity: 74.064\t Batch accuracy: 27.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 12640/27699\t Batch loss: 4.193\t Batch perplexity: 66.197\t Batch accuracy: 28.2%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 12660/27699\t Batch loss: 3.906\t Batch perplexity: 49.708\t Batch accuracy: 28.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 12680/27699\t Batch loss: 4.065\t Batch perplexity: 58.290\t Batch accuracy: 27.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 12700/27699\t Batch loss: 4.274\t Batch perplexity: 71.796\t Batch accuracy: 25.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 12720/27699\t Batch loss: 4.458\t Batch perplexity: 86.317\t Batch accuracy: 25.6%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 12740/27699\t Batch loss: 4.233\t Batch perplexity: 68.927\t Batch accuracy: 29.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.155\t Batch perplexity: 63.766\t Batch accuracy: 29.7%\t***\n",
      "Epoch: 1/2\t Train step: 12760/27699\t Batch loss: 4.266\t Batch perplexity: 71.238\t Batch accuracy: 25.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 12780/27699\t Batch loss: 4.227\t Batch perplexity: 68.494\t Batch accuracy: 29.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 12800/27699\t Batch loss: 4.492\t Batch perplexity: 89.314\t Batch accuracy: 26.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 12820/27699\t Batch loss: 4.561\t Batch perplexity: 95.709\t Batch accuracy: 27.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 12840/27699\t Batch loss: 4.039\t Batch perplexity: 56.792\t Batch accuracy: 28.3%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 12860/27699\t Batch loss: 4.448\t Batch perplexity: 85.428\t Batch accuracy: 25.7%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 12880/27699\t Batch loss: 4.469\t Batch perplexity: 87.230\t Batch accuracy: 25.6%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 12900/27699\t Batch loss: 4.237\t Batch perplexity: 69.191\t Batch accuracy: 28.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 12920/27699\t Batch loss: 4.228\t Batch perplexity: 68.549\t Batch accuracy: 29.6%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 12940/27699\t Batch loss: 4.143\t Batch perplexity: 62.983\t Batch accuracy: 29.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 12960/27699\t Batch loss: 4.230\t Batch perplexity: 68.693\t Batch accuracy: 26.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 12980/27699\t Batch loss: 4.355\t Batch perplexity: 77.859\t Batch accuracy: 24.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 13000/27699\t Batch loss: 4.171\t Batch perplexity: 64.770\t Batch accuracy: 27.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.004\t Batch perplexity: 54.800\t Batch accuracy: 28.7%\t***\n",
      "Epoch: 1/2\t Train step: 13020/27699\t Batch loss: 4.281\t Batch perplexity: 72.303\t Batch accuracy: 27.6%\t 0.68s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\t Train step: 13040/27699\t Batch loss: 4.467\t Batch perplexity: 87.136\t Batch accuracy: 24.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 13060/27699\t Batch loss: 4.070\t Batch perplexity: 58.541\t Batch accuracy: 29.3%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 13080/27699\t Batch loss: 4.315\t Batch perplexity: 74.799\t Batch accuracy: 27.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 13100/27699\t Batch loss: 4.280\t Batch perplexity: 72.253\t Batch accuracy: 30.5%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 13120/27699\t Batch loss: 4.376\t Batch perplexity: 79.487\t Batch accuracy: 25.1%\t 0.75s/batch\n",
      "Epoch: 1/2\t Train step: 13140/27699\t Batch loss: 4.346\t Batch perplexity: 77.171\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 13160/27699\t Batch loss: 4.341\t Batch perplexity: 76.759\t Batch accuracy: 24.6%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 13180/27699\t Batch loss: 4.285\t Batch perplexity: 72.607\t Batch accuracy: 25.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 13200/27699\t Batch loss: 4.451\t Batch perplexity: 85.750\t Batch accuracy: 24.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 13220/27699\t Batch loss: 4.160\t Batch perplexity: 64.059\t Batch accuracy: 28.2%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 13240/27699\t Batch loss: 4.443\t Batch perplexity: 85.021\t Batch accuracy: 25.0%\t 0.70s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.185\t Batch perplexity: 65.717\t Batch accuracy: 28.0%\t***\n",
      "Epoch: 1/2\t Train step: 13260/27699\t Batch loss: 4.355\t Batch perplexity: 77.840\t Batch accuracy: 28.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 13280/27699\t Batch loss: 4.238\t Batch perplexity: 69.253\t Batch accuracy: 27.8%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 13300/27699\t Batch loss: 4.230\t Batch perplexity: 68.738\t Batch accuracy: 28.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 13320/27699\t Batch loss: 4.264\t Batch perplexity: 71.068\t Batch accuracy: 27.2%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 13340/27699\t Batch loss: 4.516\t Batch perplexity: 91.443\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 13360/27699\t Batch loss: 4.361\t Batch perplexity: 78.317\t Batch accuracy: 25.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 13380/27699\t Batch loss: 4.348\t Batch perplexity: 77.337\t Batch accuracy: 28.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 13400/27699\t Batch loss: 4.058\t Batch perplexity: 57.877\t Batch accuracy: 30.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 13420/27699\t Batch loss: 4.414\t Batch perplexity: 82.575\t Batch accuracy: 25.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 13440/27699\t Batch loss: 4.271\t Batch perplexity: 71.614\t Batch accuracy: 25.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 13460/27699\t Batch loss: 4.067\t Batch perplexity: 58.396\t Batch accuracy: 26.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 13480/27699\t Batch loss: 4.155\t Batch perplexity: 63.726\t Batch accuracy: 28.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 13500/27699\t Batch loss: 4.430\t Batch perplexity: 83.956\t Batch accuracy: 26.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.127\t Batch perplexity: 61.989\t Batch accuracy: 30.2%\t***\n",
      "Epoch: 1/2\t Train step: 13520/27699\t Batch loss: 4.294\t Batch perplexity: 73.283\t Batch accuracy: 29.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 13540/27699\t Batch loss: 4.027\t Batch perplexity: 56.085\t Batch accuracy: 26.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 13560/27699\t Batch loss: 4.110\t Batch perplexity: 60.923\t Batch accuracy: 29.4%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 13580/27699\t Batch loss: 4.156\t Batch perplexity: 63.818\t Batch accuracy: 31.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 13600/27699\t Batch loss: 4.008\t Batch perplexity: 55.035\t Batch accuracy: 30.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 13620/27699\t Batch loss: 4.323\t Batch perplexity: 75.384\t Batch accuracy: 27.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 13640/27699\t Batch loss: 4.337\t Batch perplexity: 76.507\t Batch accuracy: 26.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 13660/27699\t Batch loss: 4.153\t Batch perplexity: 63.613\t Batch accuracy: 27.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 13680/27699\t Batch loss: 4.107\t Batch perplexity: 60.745\t Batch accuracy: 29.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 13700/27699\t Batch loss: 4.249\t Batch perplexity: 70.038\t Batch accuracy: 26.9%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 13720/27699\t Batch loss: 4.318\t Batch perplexity: 75.059\t Batch accuracy: 27.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 13740/27699\t Batch loss: 4.152\t Batch perplexity: 63.540\t Batch accuracy: 30.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.181\t Batch perplexity: 65.405\t Batch accuracy: 30.1%\t***\n",
      "Epoch: 1/2\t Train step: 13760/27699\t Batch loss: 4.227\t Batch perplexity: 68.492\t Batch accuracy: 27.1%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 13780/27699\t Batch loss: 4.282\t Batch perplexity: 72.392\t Batch accuracy: 28.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 13800/27699\t Batch loss: 4.181\t Batch perplexity: 65.444\t Batch accuracy: 26.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 13820/27699\t Batch loss: 4.306\t Batch perplexity: 74.132\t Batch accuracy: 26.9%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 13840/27699\t Batch loss: 4.147\t Batch perplexity: 63.243\t Batch accuracy: 26.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 13860/27699\t Batch loss: 4.194\t Batch perplexity: 66.314\t Batch accuracy: 27.5%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 13880/27699\t Batch loss: 4.010\t Batch perplexity: 55.169\t Batch accuracy: 31.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 13900/27699\t Batch loss: 4.280\t Batch perplexity: 72.276\t Batch accuracy: 27.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 13920/27699\t Batch loss: 4.299\t Batch perplexity: 73.597\t Batch accuracy: 26.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 13940/27699\t Batch loss: 4.148\t Batch perplexity: 63.318\t Batch accuracy: 28.0%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 13960/27699\t Batch loss: 4.276\t Batch perplexity: 71.969\t Batch accuracy: 27.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 13980/27699\t Batch loss: 4.217\t Batch perplexity: 67.863\t Batch accuracy: 26.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 14000/27699\t Batch loss: 4.248\t Batch perplexity: 69.984\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.118\t Batch perplexity: 61.413\t Batch accuracy: 30.6%\t***\n",
      "Epoch: 1/2\t Train step: 14020/27699\t Batch loss: 4.123\t Batch perplexity: 61.752\t Batch accuracy: 29.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 14040/27699\t Batch loss: 4.250\t Batch perplexity: 70.135\t Batch accuracy: 29.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 14060/27699\t Batch loss: 4.330\t Batch perplexity: 75.926\t Batch accuracy: 25.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 14080/27699\t Batch loss: 4.141\t Batch perplexity: 62.891\t Batch accuracy: 29.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 14100/27699\t Batch loss: 4.212\t Batch perplexity: 67.518\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 14120/27699\t Batch loss: 4.348\t Batch perplexity: 77.320\t Batch accuracy: 25.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 14140/27699\t Batch loss: 4.309\t Batch perplexity: 74.383\t Batch accuracy: 26.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 14160/27699\t Batch loss: 4.213\t Batch perplexity: 67.525\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 14180/27699\t Batch loss: 4.180\t Batch perplexity: 65.339\t Batch accuracy: 28.6%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 14200/27699\t Batch loss: 4.281\t Batch perplexity: 72.335\t Batch accuracy: 25.9%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 14220/27699\t Batch loss: 4.239\t Batch perplexity: 69.313\t Batch accuracy: 28.5%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 14240/27699\t Batch loss: 4.181\t Batch perplexity: 65.425\t Batch accuracy: 28.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 3.955\t Batch perplexity: 52.170\t Batch accuracy: 29.5%\t***\n",
      "Epoch: 1/2\t Train step: 14260/27699\t Batch loss: 4.238\t Batch perplexity: 69.262\t Batch accuracy: 26.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 14280/27699\t Batch loss: 4.277\t Batch perplexity: 72.049\t Batch accuracy: 27.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 14300/27699\t Batch loss: 4.356\t Batch perplexity: 77.961\t Batch accuracy: 25.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 14320/27699\t Batch loss: 4.242\t Batch perplexity: 69.525\t Batch accuracy: 26.8%\t 0.68s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\t Train step: 14340/27699\t Batch loss: 4.298\t Batch perplexity: 73.570\t Batch accuracy: 25.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 14360/27699\t Batch loss: 4.219\t Batch perplexity: 67.995\t Batch accuracy: 29.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 14380/27699\t Batch loss: 4.013\t Batch perplexity: 55.289\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 14400/27699\t Batch loss: 4.370\t Batch perplexity: 79.016\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 14420/27699\t Batch loss: 4.305\t Batch perplexity: 74.090\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 14440/27699\t Batch loss: 4.124\t Batch perplexity: 61.821\t Batch accuracy: 28.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 14460/27699\t Batch loss: 4.108\t Batch perplexity: 60.806\t Batch accuracy: 28.4%\t 0.73s/batch\n",
      "Epoch: 1/2\t Train step: 14480/27699\t Batch loss: 4.455\t Batch perplexity: 86.088\t Batch accuracy: 24.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 14500/27699\t Batch loss: 4.267\t Batch perplexity: 71.295\t Batch accuracy: 27.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.241\t Batch perplexity: 69.502\t Batch accuracy: 29.1%\t***\n",
      "Epoch: 1/2\t Train step: 14520/27699\t Batch loss: 4.082\t Batch perplexity: 59.243\t Batch accuracy: 29.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 14540/27699\t Batch loss: 4.370\t Batch perplexity: 79.042\t Batch accuracy: 27.6%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 14560/27699\t Batch loss: 4.244\t Batch perplexity: 69.652\t Batch accuracy: 28.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 14580/27699\t Batch loss: 4.222\t Batch perplexity: 68.148\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 14600/27699\t Batch loss: 4.176\t Batch perplexity: 65.073\t Batch accuracy: 28.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 14620/27699\t Batch loss: 4.003\t Batch perplexity: 54.742\t Batch accuracy: 31.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 14640/27699\t Batch loss: 4.170\t Batch perplexity: 64.692\t Batch accuracy: 28.1%\t 0.73s/batch\n",
      "Epoch: 1/2\t Train step: 14660/27699\t Batch loss: 4.199\t Batch perplexity: 66.624\t Batch accuracy: 26.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 14680/27699\t Batch loss: 4.480\t Batch perplexity: 88.246\t Batch accuracy: 27.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 14700/27699\t Batch loss: 4.192\t Batch perplexity: 66.152\t Batch accuracy: 27.5%\t 0.72s/batch\n",
      "Epoch: 1/2\t Train step: 14720/27699\t Batch loss: 4.227\t Batch perplexity: 68.497\t Batch accuracy: 27.9%\t 0.83s/batch\n",
      "Epoch: 1/2\t Train step: 14740/27699\t Batch loss: 4.300\t Batch perplexity: 73.679\t Batch accuracy: 26.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.181\t Batch perplexity: 65.410\t Batch accuracy: 27.2%\t***\n",
      "Epoch: 1/2\t Train step: 14760/27699\t Batch loss: 4.270\t Batch perplexity: 71.497\t Batch accuracy: 28.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 14780/27699\t Batch loss: 4.179\t Batch perplexity: 65.327\t Batch accuracy: 28.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 14800/27699\t Batch loss: 4.436\t Batch perplexity: 84.407\t Batch accuracy: 26.3%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 14820/27699\t Batch loss: 4.188\t Batch perplexity: 65.867\t Batch accuracy: 26.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 14840/27699\t Batch loss: 4.173\t Batch perplexity: 64.912\t Batch accuracy: 28.4%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 14860/27699\t Batch loss: 4.478\t Batch perplexity: 88.021\t Batch accuracy: 25.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 14880/27699\t Batch loss: 4.319\t Batch perplexity: 75.119\t Batch accuracy: 26.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 14900/27699\t Batch loss: 4.216\t Batch perplexity: 67.781\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 14920/27699\t Batch loss: 4.224\t Batch perplexity: 68.313\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 14940/27699\t Batch loss: 4.127\t Batch perplexity: 62.006\t Batch accuracy: 26.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 14960/27699\t Batch loss: 4.122\t Batch perplexity: 61.698\t Batch accuracy: 30.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 14980/27699\t Batch loss: 4.144\t Batch perplexity: 63.042\t Batch accuracy: 28.7%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 15000/27699\t Batch loss: 4.286\t Batch perplexity: 72.675\t Batch accuracy: 27.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.455\t Batch perplexity: 86.063\t Batch accuracy: 27.5%\t***\n",
      "Epoch: 1/2\t Train step: 15020/27699\t Batch loss: 4.341\t Batch perplexity: 76.818\t Batch accuracy: 24.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 15040/27699\t Batch loss: 4.253\t Batch perplexity: 70.324\t Batch accuracy: 27.7%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 15060/27699\t Batch loss: 4.186\t Batch perplexity: 65.730\t Batch accuracy: 26.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 15080/27699\t Batch loss: 4.186\t Batch perplexity: 65.731\t Batch accuracy: 25.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 15100/27699\t Batch loss: 4.306\t Batch perplexity: 74.129\t Batch accuracy: 27.4%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 15120/27699\t Batch loss: 4.390\t Batch perplexity: 80.614\t Batch accuracy: 25.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 15140/27699\t Batch loss: 4.455\t Batch perplexity: 86.028\t Batch accuracy: 24.4%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 15160/27699\t Batch loss: 4.260\t Batch perplexity: 70.799\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 15180/27699\t Batch loss: 4.026\t Batch perplexity: 56.044\t Batch accuracy: 31.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 15200/27699\t Batch loss: 4.265\t Batch perplexity: 71.184\t Batch accuracy: 29.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 15220/27699\t Batch loss: 4.072\t Batch perplexity: 58.698\t Batch accuracy: 28.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 15240/27699\t Batch loss: 4.314\t Batch perplexity: 74.775\t Batch accuracy: 25.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.034\t Batch perplexity: 56.500\t Batch accuracy: 29.7%\t***\n",
      "Epoch: 1/2\t Train step: 15260/27699\t Batch loss: 4.171\t Batch perplexity: 64.788\t Batch accuracy: 25.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 15280/27699\t Batch loss: 4.289\t Batch perplexity: 72.923\t Batch accuracy: 28.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 15300/27699\t Batch loss: 4.107\t Batch perplexity: 60.743\t Batch accuracy: 31.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 15320/27699\t Batch loss: 4.381\t Batch perplexity: 79.908\t Batch accuracy: 27.6%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 15340/27699\t Batch loss: 4.246\t Batch perplexity: 69.796\t Batch accuracy: 28.8%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 15360/27699\t Batch loss: 4.329\t Batch perplexity: 75.878\t Batch accuracy: 25.1%\t 0.78s/batch\n",
      "Epoch: 1/2\t Train step: 15380/27699\t Batch loss: 4.264\t Batch perplexity: 71.074\t Batch accuracy: 26.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 15400/27699\t Batch loss: 4.418\t Batch perplexity: 82.929\t Batch accuracy: 25.7%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 15420/27699\t Batch loss: 4.094\t Batch perplexity: 59.950\t Batch accuracy: 29.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 15440/27699\t Batch loss: 4.241\t Batch perplexity: 69.457\t Batch accuracy: 27.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 15460/27699\t Batch loss: 4.285\t Batch perplexity: 72.570\t Batch accuracy: 27.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 15480/27699\t Batch loss: 4.288\t Batch perplexity: 72.856\t Batch accuracy: 26.8%\t 0.72s/batch\n",
      "Epoch: 1/2\t Train step: 15500/27699\t Batch loss: 4.288\t Batch perplexity: 72.812\t Batch accuracy: 27.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.075\t Batch perplexity: 58.843\t Batch accuracy: 28.1%\t***\n",
      "Epoch: 1/2\t Train step: 15520/27699\t Batch loss: 4.403\t Batch perplexity: 81.683\t Batch accuracy: 28.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 15540/27699\t Batch loss: 4.208\t Batch perplexity: 67.255\t Batch accuracy: 29.6%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 15560/27699\t Batch loss: 4.315\t Batch perplexity: 74.787\t Batch accuracy: 26.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 15580/27699\t Batch loss: 4.188\t Batch perplexity: 65.859\t Batch accuracy: 28.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 15600/27699\t Batch loss: 4.173\t Batch perplexity: 64.906\t Batch accuracy: 28.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 15620/27699\t Batch loss: 4.223\t Batch perplexity: 68.225\t Batch accuracy: 29.6%\t 0.67s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\t Train step: 15640/27699\t Batch loss: 4.130\t Batch perplexity: 62.159\t Batch accuracy: 28.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 15660/27699\t Batch loss: 4.104\t Batch perplexity: 60.573\t Batch accuracy: 26.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 15680/27699\t Batch loss: 4.254\t Batch perplexity: 70.391\t Batch accuracy: 28.8%\t 0.74s/batch\n",
      "Epoch: 1/2\t Train step: 15700/27699\t Batch loss: 4.328\t Batch perplexity: 75.774\t Batch accuracy: 25.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 15720/27699\t Batch loss: 4.278\t Batch perplexity: 72.074\t Batch accuracy: 28.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 15740/27699\t Batch loss: 4.077\t Batch perplexity: 58.955\t Batch accuracy: 27.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.126\t Batch perplexity: 61.929\t Batch accuracy: 28.4%\t***\n",
      "Epoch: 1/2\t Train step: 15760/27699\t Batch loss: 4.413\t Batch perplexity: 82.550\t Batch accuracy: 27.0%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 15780/27699\t Batch loss: 4.320\t Batch perplexity: 75.208\t Batch accuracy: 27.1%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 15800/27699\t Batch loss: 4.423\t Batch perplexity: 83.317\t Batch accuracy: 26.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 15820/27699\t Batch loss: 4.329\t Batch perplexity: 75.850\t Batch accuracy: 27.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 15840/27699\t Batch loss: 4.157\t Batch perplexity: 63.906\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 15860/27699\t Batch loss: 4.056\t Batch perplexity: 57.716\t Batch accuracy: 28.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 15880/27699\t Batch loss: 4.315\t Batch perplexity: 74.823\t Batch accuracy: 26.2%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 15900/27699\t Batch loss: 4.249\t Batch perplexity: 70.052\t Batch accuracy: 30.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 15920/27699\t Batch loss: 4.229\t Batch perplexity: 68.679\t Batch accuracy: 28.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 15940/27699\t Batch loss: 4.105\t Batch perplexity: 60.656\t Batch accuracy: 28.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 15960/27699\t Batch loss: 4.439\t Batch perplexity: 84.716\t Batch accuracy: 26.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 15980/27699\t Batch loss: 4.253\t Batch perplexity: 70.306\t Batch accuracy: 28.7%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 16000/27699\t Batch loss: 4.344\t Batch perplexity: 77.013\t Batch accuracy: 26.2%\t 0.70s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.147\t Batch perplexity: 63.216\t Batch accuracy: 27.7%\t***\n",
      "Epoch: 1/2\t Train step: 16020/27699\t Batch loss: 4.532\t Batch perplexity: 92.952\t Batch accuracy: 23.2%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 16040/27699\t Batch loss: 4.229\t Batch perplexity: 68.628\t Batch accuracy: 25.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 16060/27699\t Batch loss: 4.367\t Batch perplexity: 78.778\t Batch accuracy: 27.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 16080/27699\t Batch loss: 4.193\t Batch perplexity: 66.227\t Batch accuracy: 27.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 16100/27699\t Batch loss: 4.381\t Batch perplexity: 79.939\t Batch accuracy: 25.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 16120/27699\t Batch loss: 4.418\t Batch perplexity: 82.913\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 16140/27699\t Batch loss: 4.272\t Batch perplexity: 71.683\t Batch accuracy: 27.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 16160/27699\t Batch loss: 4.387\t Batch perplexity: 80.397\t Batch accuracy: 25.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 16180/27699\t Batch loss: 4.245\t Batch perplexity: 69.723\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 16200/27699\t Batch loss: 4.262\t Batch perplexity: 70.949\t Batch accuracy: 25.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 16220/27699\t Batch loss: 4.156\t Batch perplexity: 63.828\t Batch accuracy: 26.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 16240/27699\t Batch loss: 4.122\t Batch perplexity: 61.677\t Batch accuracy: 27.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.182\t Batch perplexity: 65.514\t Batch accuracy: 26.8%\t***\n",
      "Epoch: 1/2\t Train step: 16260/27699\t Batch loss: 4.345\t Batch perplexity: 77.068\t Batch accuracy: 27.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 16280/27699\t Batch loss: 4.313\t Batch perplexity: 74.644\t Batch accuracy: 26.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 16300/27699\t Batch loss: 4.244\t Batch perplexity: 69.669\t Batch accuracy: 25.2%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 16320/27699\t Batch loss: 4.102\t Batch perplexity: 60.452\t Batch accuracy: 28.0%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 16340/27699\t Batch loss: 4.456\t Batch perplexity: 86.104\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 16360/27699\t Batch loss: 4.229\t Batch perplexity: 68.619\t Batch accuracy: 29.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 16380/27699\t Batch loss: 4.233\t Batch perplexity: 68.908\t Batch accuracy: 26.9%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 16400/27699\t Batch loss: 4.212\t Batch perplexity: 67.464\t Batch accuracy: 25.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 16420/27699\t Batch loss: 4.237\t Batch perplexity: 69.183\t Batch accuracy: 27.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 16440/27699\t Batch loss: 4.086\t Batch perplexity: 59.510\t Batch accuracy: 26.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 16460/27699\t Batch loss: 4.359\t Batch perplexity: 78.151\t Batch accuracy: 24.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 16480/27699\t Batch loss: 4.271\t Batch perplexity: 71.605\t Batch accuracy: 27.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 16500/27699\t Batch loss: 4.235\t Batch perplexity: 69.028\t Batch accuracy: 27.3%\t 0.70s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.217\t Batch perplexity: 67.804\t Batch accuracy: 26.4%\t***\n",
      "Epoch: 1/2\t Train step: 16520/27699\t Batch loss: 4.314\t Batch perplexity: 74.750\t Batch accuracy: 28.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 16540/27699\t Batch loss: 4.101\t Batch perplexity: 60.401\t Batch accuracy: 26.9%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 16560/27699\t Batch loss: 4.183\t Batch perplexity: 65.563\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 16580/27699\t Batch loss: 4.294\t Batch perplexity: 73.279\t Batch accuracy: 27.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 16600/27699\t Batch loss: 3.876\t Batch perplexity: 48.243\t Batch accuracy: 28.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 16620/27699\t Batch loss: 4.302\t Batch perplexity: 73.852\t Batch accuracy: 24.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 16640/27699\t Batch loss: 4.481\t Batch perplexity: 88.358\t Batch accuracy: 23.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 16660/27699\t Batch loss: 4.314\t Batch perplexity: 74.736\t Batch accuracy: 26.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 16680/27699\t Batch loss: 4.249\t Batch perplexity: 70.046\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 16700/27699\t Batch loss: 4.305\t Batch perplexity: 74.064\t Batch accuracy: 25.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 16720/27699\t Batch loss: 4.333\t Batch perplexity: 76.182\t Batch accuracy: 26.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 16740/27699\t Batch loss: 4.301\t Batch perplexity: 73.787\t Batch accuracy: 28.0%\t 0.70s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.201\t Batch perplexity: 66.748\t Batch accuracy: 28.7%\t***\n",
      "Epoch: 1/2\t Train step: 16760/27699\t Batch loss: 4.110\t Batch perplexity: 60.961\t Batch accuracy: 27.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 16780/27699\t Batch loss: 4.487\t Batch perplexity: 88.825\t Batch accuracy: 27.4%\t 0.83s/batch\n",
      "Epoch: 1/2\t Train step: 16800/27699\t Batch loss: 4.405\t Batch perplexity: 81.832\t Batch accuracy: 26.8%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 16820/27699\t Batch loss: 4.339\t Batch perplexity: 76.665\t Batch accuracy: 26.6%\t 0.74s/batch\n",
      "Epoch: 1/2\t Train step: 16840/27699\t Batch loss: 4.193\t Batch perplexity: 66.245\t Batch accuracy: 26.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 16860/27699\t Batch loss: 4.008\t Batch perplexity: 55.020\t Batch accuracy: 29.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 16880/27699\t Batch loss: 4.212\t Batch perplexity: 67.514\t Batch accuracy: 27.9%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 16900/27699\t Batch loss: 4.427\t Batch perplexity: 83.672\t Batch accuracy: 26.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 16920/27699\t Batch loss: 4.200\t Batch perplexity: 66.692\t Batch accuracy: 27.9%\t 0.69s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\t Train step: 16940/27699\t Batch loss: 4.231\t Batch perplexity: 68.777\t Batch accuracy: 28.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 16960/27699\t Batch loss: 4.171\t Batch perplexity: 64.807\t Batch accuracy: 27.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 16980/27699\t Batch loss: 4.279\t Batch perplexity: 72.198\t Batch accuracy: 27.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 17000/27699\t Batch loss: 4.258\t Batch perplexity: 70.641\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.042\t Batch perplexity: 56.926\t Batch accuracy: 28.8%\t***\n",
      "Epoch: 1/2\t Train step: 17020/27699\t Batch loss: 4.424\t Batch perplexity: 83.397\t Batch accuracy: 29.3%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 17040/27699\t Batch loss: 4.566\t Batch perplexity: 96.119\t Batch accuracy: 24.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 17060/27699\t Batch loss: 4.161\t Batch perplexity: 64.121\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 17080/27699\t Batch loss: 4.061\t Batch perplexity: 58.058\t Batch accuracy: 28.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 17100/27699\t Batch loss: 4.234\t Batch perplexity: 69.021\t Batch accuracy: 29.3%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 17120/27699\t Batch loss: 4.295\t Batch perplexity: 73.299\t Batch accuracy: 26.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 17140/27699\t Batch loss: 4.205\t Batch perplexity: 67.005\t Batch accuracy: 28.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 17160/27699\t Batch loss: 4.294\t Batch perplexity: 73.254\t Batch accuracy: 24.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 17180/27699\t Batch loss: 4.135\t Batch perplexity: 62.464\t Batch accuracy: 27.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 17200/27699\t Batch loss: 4.280\t Batch perplexity: 72.265\t Batch accuracy: 27.6%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 17220/27699\t Batch loss: 4.090\t Batch perplexity: 59.732\t Batch accuracy: 27.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 17240/27699\t Batch loss: 4.288\t Batch perplexity: 72.807\t Batch accuracy: 28.2%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.008\t Batch perplexity: 55.032\t Batch accuracy: 28.0%\t***\n",
      "Epoch: 1/2\t Train step: 17260/27699\t Batch loss: 4.386\t Batch perplexity: 80.354\t Batch accuracy: 26.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 17280/27699\t Batch loss: 4.198\t Batch perplexity: 66.557\t Batch accuracy: 27.2%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 17300/27699\t Batch loss: 4.249\t Batch perplexity: 70.042\t Batch accuracy: 26.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 17320/27699\t Batch loss: 4.282\t Batch perplexity: 72.375\t Batch accuracy: 26.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 17340/27699\t Batch loss: 4.303\t Batch perplexity: 73.914\t Batch accuracy: 29.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 17360/27699\t Batch loss: 4.519\t Batch perplexity: 91.778\t Batch accuracy: 23.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 17380/27699\t Batch loss: 4.367\t Batch perplexity: 78.846\t Batch accuracy: 26.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 17400/27699\t Batch loss: 4.241\t Batch perplexity: 69.453\t Batch accuracy: 24.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 17420/27699\t Batch loss: 4.293\t Batch perplexity: 73.166\t Batch accuracy: 26.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 17440/27699\t Batch loss: 4.279\t Batch perplexity: 72.197\t Batch accuracy: 26.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 17460/27699\t Batch loss: 4.410\t Batch perplexity: 82.247\t Batch accuracy: 26.4%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 17480/27699\t Batch loss: 4.262\t Batch perplexity: 70.928\t Batch accuracy: 25.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 17500/27699\t Batch loss: 4.518\t Batch perplexity: 91.631\t Batch accuracy: 24.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 3.989\t Batch perplexity: 53.979\t Batch accuracy: 27.6%\t***\n",
      "Epoch: 1/2\t Train step: 17520/27699\t Batch loss: 4.471\t Batch perplexity: 87.467\t Batch accuracy: 23.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 17540/27699\t Batch loss: 4.170\t Batch perplexity: 64.705\t Batch accuracy: 28.0%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 17560/27699\t Batch loss: 3.982\t Batch perplexity: 53.640\t Batch accuracy: 30.0%\t 0.75s/batch\n",
      "Epoch: 1/2\t Train step: 17580/27699\t Batch loss: 4.248\t Batch perplexity: 69.966\t Batch accuracy: 29.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 17600/27699\t Batch loss: 4.312\t Batch perplexity: 74.576\t Batch accuracy: 26.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 17620/27699\t Batch loss: 4.178\t Batch perplexity: 65.205\t Batch accuracy: 27.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 17640/27699\t Batch loss: 4.313\t Batch perplexity: 74.683\t Batch accuracy: 28.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 17660/27699\t Batch loss: 4.200\t Batch perplexity: 66.662\t Batch accuracy: 31.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 17680/27699\t Batch loss: 4.375\t Batch perplexity: 79.471\t Batch accuracy: 26.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 17700/27699\t Batch loss: 4.232\t Batch perplexity: 68.888\t Batch accuracy: 26.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 17720/27699\t Batch loss: 4.492\t Batch perplexity: 89.319\t Batch accuracy: 25.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 17740/27699\t Batch loss: 4.463\t Batch perplexity: 86.768\t Batch accuracy: 25.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.265\t Batch perplexity: 71.141\t Batch accuracy: 27.7%\t***\n",
      "Epoch: 1/2\t Train step: 17760/27699\t Batch loss: 4.160\t Batch perplexity: 64.091\t Batch accuracy: 30.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 17780/27699\t Batch loss: 4.279\t Batch perplexity: 72.178\t Batch accuracy: 25.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 17800/27699\t Batch loss: 4.343\t Batch perplexity: 76.913\t Batch accuracy: 27.7%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 17820/27699\t Batch loss: 4.306\t Batch perplexity: 74.172\t Batch accuracy: 26.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 17840/27699\t Batch loss: 4.117\t Batch perplexity: 61.383\t Batch accuracy: 26.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 17860/27699\t Batch loss: 4.449\t Batch perplexity: 85.576\t Batch accuracy: 26.0%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 17880/27699\t Batch loss: 4.365\t Batch perplexity: 78.673\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 17900/27699\t Batch loss: 4.178\t Batch perplexity: 65.257\t Batch accuracy: 26.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 17920/27699\t Batch loss: 4.173\t Batch perplexity: 64.937\t Batch accuracy: 29.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 17940/27699\t Batch loss: 4.264\t Batch perplexity: 71.100\t Batch accuracy: 25.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 17960/27699\t Batch loss: 4.086\t Batch perplexity: 59.492\t Batch accuracy: 27.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 17980/27699\t Batch loss: 4.137\t Batch perplexity: 62.593\t Batch accuracy: 28.1%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 18000/27699\t Batch loss: 4.297\t Batch perplexity: 73.461\t Batch accuracy: 25.2%\t 0.70s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.292\t Batch perplexity: 73.095\t Batch accuracy: 27.6%\t***\n",
      "Epoch: 1/2\t Train step: 18020/27699\t Batch loss: 4.331\t Batch perplexity: 76.023\t Batch accuracy: 26.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 18040/27699\t Batch loss: 4.314\t Batch perplexity: 74.759\t Batch accuracy: 26.6%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 18060/27699\t Batch loss: 4.442\t Batch perplexity: 84.947\t Batch accuracy: 26.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 18080/27699\t Batch loss: 4.196\t Batch perplexity: 66.392\t Batch accuracy: 27.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 18100/27699\t Batch loss: 4.045\t Batch perplexity: 57.104\t Batch accuracy: 30.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 18120/27699\t Batch loss: 4.047\t Batch perplexity: 57.240\t Batch accuracy: 28.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 18140/27699\t Batch loss: 4.334\t Batch perplexity: 76.251\t Batch accuracy: 26.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 18160/27699\t Batch loss: 4.257\t Batch perplexity: 70.599\t Batch accuracy: 28.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 18180/27699\t Batch loss: 4.013\t Batch perplexity: 55.287\t Batch accuracy: 29.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 18200/27699\t Batch loss: 4.430\t Batch perplexity: 83.963\t Batch accuracy: 23.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 18220/27699\t Batch loss: 4.274\t Batch perplexity: 71.800\t Batch accuracy: 29.2%\t 0.67s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\t Train step: 18240/27699\t Batch loss: 4.267\t Batch perplexity: 71.311\t Batch accuracy: 27.7%\t 0.72s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.151\t Batch perplexity: 63.494\t Batch accuracy: 29.9%\t***\n",
      "Epoch: 1/2\t Train step: 18260/27699\t Batch loss: 4.143\t Batch perplexity: 62.979\t Batch accuracy: 28.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 18280/27699\t Batch loss: 4.302\t Batch perplexity: 73.851\t Batch accuracy: 28.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 18300/27699\t Batch loss: 4.243\t Batch perplexity: 69.650\t Batch accuracy: 27.7%\t 0.72s/batch\n",
      "Epoch: 1/2\t Train step: 18320/27699\t Batch loss: 4.129\t Batch perplexity: 62.136\t Batch accuracy: 29.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 18340/27699\t Batch loss: 4.360\t Batch perplexity: 78.278\t Batch accuracy: 26.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 18360/27699\t Batch loss: 4.278\t Batch perplexity: 72.111\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 18380/27699\t Batch loss: 4.309\t Batch perplexity: 74.346\t Batch accuracy: 27.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 18400/27699\t Batch loss: 4.088\t Batch perplexity: 59.623\t Batch accuracy: 28.8%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 18420/27699\t Batch loss: 3.935\t Batch perplexity: 51.140\t Batch accuracy: 29.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 18440/27699\t Batch loss: 4.039\t Batch perplexity: 56.797\t Batch accuracy: 28.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 18460/27699\t Batch loss: 4.235\t Batch perplexity: 69.096\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 18480/27699\t Batch loss: 4.317\t Batch perplexity: 74.960\t Batch accuracy: 25.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 18500/27699\t Batch loss: 4.309\t Batch perplexity: 74.352\t Batch accuracy: 26.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.168\t Batch perplexity: 64.603\t Batch accuracy: 25.7%\t***\n",
      "Epoch: 1/2\t Train step: 18520/27699\t Batch loss: 3.990\t Batch perplexity: 54.057\t Batch accuracy: 31.3%\t 0.75s/batch\n",
      "Epoch: 1/2\t Train step: 18540/27699\t Batch loss: 4.354\t Batch perplexity: 77.815\t Batch accuracy: 27.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 18560/27699\t Batch loss: 4.217\t Batch perplexity: 67.828\t Batch accuracy: 27.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 18580/27699\t Batch loss: 4.023\t Batch perplexity: 55.884\t Batch accuracy: 29.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 18600/27699\t Batch loss: 4.354\t Batch perplexity: 77.762\t Batch accuracy: 27.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 18620/27699\t Batch loss: 4.336\t Batch perplexity: 76.378\t Batch accuracy: 26.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 18640/27699\t Batch loss: 4.256\t Batch perplexity: 70.531\t Batch accuracy: 26.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 18660/27699\t Batch loss: 4.165\t Batch perplexity: 64.364\t Batch accuracy: 29.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 18680/27699\t Batch loss: 4.125\t Batch perplexity: 61.876\t Batch accuracy: 28.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 18700/27699\t Batch loss: 4.283\t Batch perplexity: 72.459\t Batch accuracy: 26.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 18720/27699\t Batch loss: 4.213\t Batch perplexity: 67.564\t Batch accuracy: 27.2%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 18740/27699\t Batch loss: 4.126\t Batch perplexity: 61.931\t Batch accuracy: 28.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.147\t Batch perplexity: 63.242\t Batch accuracy: 28.6%\t***\n",
      "Epoch: 1/2\t Train step: 18760/27699\t Batch loss: 4.371\t Batch perplexity: 79.118\t Batch accuracy: 24.7%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 18780/27699\t Batch loss: 4.226\t Batch perplexity: 68.452\t Batch accuracy: 26.8%\t 0.72s/batch\n",
      "Epoch: 1/2\t Train step: 18800/27699\t Batch loss: 4.327\t Batch perplexity: 75.692\t Batch accuracy: 24.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 18820/27699\t Batch loss: 4.214\t Batch perplexity: 67.651\t Batch accuracy: 26.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 18840/27699\t Batch loss: 4.306\t Batch perplexity: 74.154\t Batch accuracy: 26.1%\t 0.74s/batch\n",
      "Epoch: 1/2\t Train step: 18860/27699\t Batch loss: 4.239\t Batch perplexity: 69.339\t Batch accuracy: 27.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 18880/27699\t Batch loss: 4.217\t Batch perplexity: 67.830\t Batch accuracy: 27.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 18900/27699\t Batch loss: 4.271\t Batch perplexity: 71.563\t Batch accuracy: 28.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 18920/27699\t Batch loss: 4.322\t Batch perplexity: 75.342\t Batch accuracy: 25.1%\t 0.76s/batch\n",
      "Epoch: 1/2\t Train step: 18940/27699\t Batch loss: 4.121\t Batch perplexity: 61.598\t Batch accuracy: 31.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 18960/27699\t Batch loss: 4.186\t Batch perplexity: 65.746\t Batch accuracy: 26.2%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 18980/27699\t Batch loss: 4.296\t Batch perplexity: 73.435\t Batch accuracy: 27.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 19000/27699\t Batch loss: 4.102\t Batch perplexity: 60.462\t Batch accuracy: 27.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.133\t Batch perplexity: 62.358\t Batch accuracy: 32.0%\t***\n",
      "Epoch: 1/2\t Train step: 19020/27699\t Batch loss: 4.137\t Batch perplexity: 62.631\t Batch accuracy: 26.7%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 19040/27699\t Batch loss: 4.280\t Batch perplexity: 72.254\t Batch accuracy: 26.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 19060/27699\t Batch loss: 4.287\t Batch perplexity: 72.727\t Batch accuracy: 27.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 19080/27699\t Batch loss: 4.182\t Batch perplexity: 65.471\t Batch accuracy: 26.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 19100/27699\t Batch loss: 4.123\t Batch perplexity: 61.771\t Batch accuracy: 30.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 19120/27699\t Batch loss: 4.277\t Batch perplexity: 72.025\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 19140/27699\t Batch loss: 4.255\t Batch perplexity: 70.437\t Batch accuracy: 26.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 19160/27699\t Batch loss: 4.281\t Batch perplexity: 72.299\t Batch accuracy: 27.4%\t 0.79s/batch\n",
      "Epoch: 1/2\t Train step: 19180/27699\t Batch loss: 4.234\t Batch perplexity: 69.008\t Batch accuracy: 29.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 19200/27699\t Batch loss: 4.215\t Batch perplexity: 67.671\t Batch accuracy: 27.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 19220/27699\t Batch loss: 4.292\t Batch perplexity: 73.076\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 19240/27699\t Batch loss: 4.313\t Batch perplexity: 74.649\t Batch accuracy: 28.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.166\t Batch perplexity: 64.460\t Batch accuracy: 28.9%\t***\n",
      "Epoch: 1/2\t Train step: 19260/27699\t Batch loss: 4.267\t Batch perplexity: 71.293\t Batch accuracy: 23.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 19280/27699\t Batch loss: 4.105\t Batch perplexity: 60.638\t Batch accuracy: 28.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 19300/27699\t Batch loss: 4.424\t Batch perplexity: 83.397\t Batch accuracy: 26.2%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 19320/27699\t Batch loss: 4.266\t Batch perplexity: 71.206\t Batch accuracy: 25.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 19340/27699\t Batch loss: 4.227\t Batch perplexity: 68.492\t Batch accuracy: 27.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 19360/27699\t Batch loss: 4.401\t Batch perplexity: 81.510\t Batch accuracy: 27.3%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 19380/27699\t Batch loss: 4.479\t Batch perplexity: 88.185\t Batch accuracy: 27.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 19400/27699\t Batch loss: 4.335\t Batch perplexity: 76.290\t Batch accuracy: 27.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 19420/27699\t Batch loss: 4.176\t Batch perplexity: 65.087\t Batch accuracy: 26.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 19440/27699\t Batch loss: 4.134\t Batch perplexity: 62.416\t Batch accuracy: 28.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 19460/27699\t Batch loss: 4.281\t Batch perplexity: 72.279\t Batch accuracy: 26.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 19480/27699\t Batch loss: 4.243\t Batch perplexity: 69.595\t Batch accuracy: 27.6%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 19500/27699\t Batch loss: 4.283\t Batch perplexity: 72.466\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.346\t Batch perplexity: 77.158\t Batch accuracy: 26.9%\t***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\t Train step: 19520/27699\t Batch loss: 4.096\t Batch perplexity: 60.092\t Batch accuracy: 28.9%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 19540/27699\t Batch loss: 4.097\t Batch perplexity: 60.136\t Batch accuracy: 26.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 19560/27699\t Batch loss: 4.177\t Batch perplexity: 65.202\t Batch accuracy: 28.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 19580/27699\t Batch loss: 4.220\t Batch perplexity: 68.049\t Batch accuracy: 27.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 19600/27699\t Batch loss: 4.534\t Batch perplexity: 93.137\t Batch accuracy: 25.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 19620/27699\t Batch loss: 4.119\t Batch perplexity: 61.528\t Batch accuracy: 29.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 19640/27699\t Batch loss: 4.010\t Batch perplexity: 55.143\t Batch accuracy: 26.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 19660/27699\t Batch loss: 4.211\t Batch perplexity: 67.401\t Batch accuracy: 27.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 19680/27699\t Batch loss: 4.180\t Batch perplexity: 65.370\t Batch accuracy: 30.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 19700/27699\t Batch loss: 4.165\t Batch perplexity: 64.371\t Batch accuracy: 27.7%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 19720/27699\t Batch loss: 4.448\t Batch perplexity: 85.485\t Batch accuracy: 25.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 19740/27699\t Batch loss: 4.298\t Batch perplexity: 73.554\t Batch accuracy: 27.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.222\t Batch perplexity: 68.173\t Batch accuracy: 25.8%\t***\n",
      "Epoch: 1/2\t Train step: 19760/27699\t Batch loss: 4.263\t Batch perplexity: 71.005\t Batch accuracy: 27.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 19780/27699\t Batch loss: 4.133\t Batch perplexity: 62.395\t Batch accuracy: 29.9%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 19800/27699\t Batch loss: 4.403\t Batch perplexity: 81.687\t Batch accuracy: 25.8%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 19820/27699\t Batch loss: 4.106\t Batch perplexity: 60.700\t Batch accuracy: 27.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 19840/27699\t Batch loss: 4.178\t Batch perplexity: 65.248\t Batch accuracy: 26.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 19860/27699\t Batch loss: 4.222\t Batch perplexity: 68.139\t Batch accuracy: 27.3%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 19880/27699\t Batch loss: 4.227\t Batch perplexity: 68.491\t Batch accuracy: 25.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 19900/27699\t Batch loss: 4.176\t Batch perplexity: 65.096\t Batch accuracy: 25.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 19920/27699\t Batch loss: 4.294\t Batch perplexity: 73.291\t Batch accuracy: 27.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 19940/27699\t Batch loss: 4.346\t Batch perplexity: 77.177\t Batch accuracy: 24.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 19960/27699\t Batch loss: 4.263\t Batch perplexity: 71.047\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 19980/27699\t Batch loss: 4.143\t Batch perplexity: 63.010\t Batch accuracy: 27.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 20000/27699\t Batch loss: 4.169\t Batch perplexity: 64.635\t Batch accuracy: 29.9%\t 0.66s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.062\t Batch perplexity: 58.079\t Batch accuracy: 31.7%\t***\n",
      "Epoch: 1/2\t Train step: 20020/27699\t Batch loss: 4.230\t Batch perplexity: 68.695\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 20040/27699\t Batch loss: 4.178\t Batch perplexity: 65.223\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 20060/27699\t Batch loss: 4.071\t Batch perplexity: 58.611\t Batch accuracy: 26.8%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 20080/27699\t Batch loss: 4.322\t Batch perplexity: 75.366\t Batch accuracy: 26.5%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 20100/27699\t Batch loss: 4.174\t Batch perplexity: 64.963\t Batch accuracy: 28.3%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 20120/27699\t Batch loss: 4.171\t Batch perplexity: 64.749\t Batch accuracy: 27.9%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 20140/27699\t Batch loss: 4.149\t Batch perplexity: 63.368\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 20160/27699\t Batch loss: 4.046\t Batch perplexity: 57.141\t Batch accuracy: 28.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 20180/27699\t Batch loss: 3.891\t Batch perplexity: 48.938\t Batch accuracy: 32.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 20200/27699\t Batch loss: 4.209\t Batch perplexity: 67.297\t Batch accuracy: 27.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 20220/27699\t Batch loss: 4.386\t Batch perplexity: 80.308\t Batch accuracy: 26.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 20240/27699\t Batch loss: 4.389\t Batch perplexity: 80.551\t Batch accuracy: 27.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 3.914\t Batch perplexity: 50.111\t Batch accuracy: 30.1%\t***\n",
      "Epoch: 1/2\t Train step: 20260/27699\t Batch loss: 4.041\t Batch perplexity: 56.901\t Batch accuracy: 30.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 20280/27699\t Batch loss: 4.052\t Batch perplexity: 57.493\t Batch accuracy: 31.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 20300/27699\t Batch loss: 4.004\t Batch perplexity: 54.791\t Batch accuracy: 31.8%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 20320/27699\t Batch loss: 4.159\t Batch perplexity: 64.026\t Batch accuracy: 26.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 20340/27699\t Batch loss: 4.140\t Batch perplexity: 62.791\t Batch accuracy: 26.6%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 20360/27699\t Batch loss: 4.204\t Batch perplexity: 66.941\t Batch accuracy: 29.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 20380/27699\t Batch loss: 4.132\t Batch perplexity: 62.315\t Batch accuracy: 27.5%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 20400/27699\t Batch loss: 4.086\t Batch perplexity: 59.506\t Batch accuracy: 27.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 20420/27699\t Batch loss: 4.064\t Batch perplexity: 58.180\t Batch accuracy: 31.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 20440/27699\t Batch loss: 4.005\t Batch perplexity: 54.846\t Batch accuracy: 30.1%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 20460/27699\t Batch loss: 4.145\t Batch perplexity: 63.136\t Batch accuracy: 27.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 20480/27699\t Batch loss: 4.115\t Batch perplexity: 61.249\t Batch accuracy: 25.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 20500/27699\t Batch loss: 4.192\t Batch perplexity: 66.175\t Batch accuracy: 27.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.102\t Batch perplexity: 60.473\t Batch accuracy: 30.4%\t***\n",
      "Epoch: 1/2\t Train step: 20520/27699\t Batch loss: 4.299\t Batch perplexity: 73.633\t Batch accuracy: 27.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 20540/27699\t Batch loss: 4.420\t Batch perplexity: 83.083\t Batch accuracy: 27.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 20560/27699\t Batch loss: 4.254\t Batch perplexity: 70.412\t Batch accuracy: 27.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 20580/27699\t Batch loss: 4.183\t Batch perplexity: 65.551\t Batch accuracy: 27.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 20600/27699\t Batch loss: 4.121\t Batch perplexity: 61.630\t Batch accuracy: 28.9%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 20620/27699\t Batch loss: 4.256\t Batch perplexity: 70.514\t Batch accuracy: 24.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 20640/27699\t Batch loss: 4.109\t Batch perplexity: 60.875\t Batch accuracy: 29.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 20660/27699\t Batch loss: 4.216\t Batch perplexity: 67.730\t Batch accuracy: 26.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 20680/27699\t Batch loss: 4.362\t Batch perplexity: 78.402\t Batch accuracy: 27.0%\t 0.72s/batch\n",
      "Epoch: 1/2\t Train step: 20700/27699\t Batch loss: 4.056\t Batch perplexity: 57.730\t Batch accuracy: 29.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 20720/27699\t Batch loss: 4.347\t Batch perplexity: 77.216\t Batch accuracy: 26.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 20740/27699\t Batch loss: 4.300\t Batch perplexity: 73.721\t Batch accuracy: 28.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.000\t Batch perplexity: 54.597\t Batch accuracy: 30.6%\t***\n",
      "Epoch: 1/2\t Train step: 20760/27699\t Batch loss: 4.147\t Batch perplexity: 63.239\t Batch accuracy: 27.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 20780/27699\t Batch loss: 4.221\t Batch perplexity: 68.098\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 20800/27699\t Batch loss: 4.053\t Batch perplexity: 57.579\t Batch accuracy: 29.4%\t 0.68s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\t Train step: 20820/27699\t Batch loss: 4.247\t Batch perplexity: 69.916\t Batch accuracy: 26.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 20840/27699\t Batch loss: 4.165\t Batch perplexity: 64.417\t Batch accuracy: 29.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 20860/27699\t Batch loss: 4.187\t Batch perplexity: 65.834\t Batch accuracy: 29.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 20880/27699\t Batch loss: 4.180\t Batch perplexity: 65.358\t Batch accuracy: 27.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 20900/27699\t Batch loss: 4.388\t Batch perplexity: 80.450\t Batch accuracy: 26.8%\t 0.80s/batch\n",
      "Epoch: 1/2\t Train step: 20920/27699\t Batch loss: 4.247\t Batch perplexity: 69.872\t Batch accuracy: 26.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 20940/27699\t Batch loss: 4.220\t Batch perplexity: 68.027\t Batch accuracy: 28.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 20960/27699\t Batch loss: 4.248\t Batch perplexity: 69.990\t Batch accuracy: 27.0%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 20980/27699\t Batch loss: 4.171\t Batch perplexity: 64.804\t Batch accuracy: 29.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 21000/27699\t Batch loss: 4.137\t Batch perplexity: 62.589\t Batch accuracy: 31.2%\t 0.66s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.163\t Batch perplexity: 64.258\t Batch accuracy: 27.6%\t***\n",
      "Epoch: 1/2\t Train step: 21020/27699\t Batch loss: 4.249\t Batch perplexity: 70.001\t Batch accuracy: 25.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 21040/27699\t Batch loss: 4.154\t Batch perplexity: 63.688\t Batch accuracy: 27.3%\t 0.72s/batch\n",
      "Epoch: 1/2\t Train step: 21060/27699\t Batch loss: 4.130\t Batch perplexity: 62.205\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 21080/27699\t Batch loss: 4.397\t Batch perplexity: 81.172\t Batch accuracy: 26.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 21100/27699\t Batch loss: 4.145\t Batch perplexity: 63.100\t Batch accuracy: 27.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 21120/27699\t Batch loss: 4.371\t Batch perplexity: 79.146\t Batch accuracy: 25.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 21140/27699\t Batch loss: 4.278\t Batch perplexity: 72.081\t Batch accuracy: 26.3%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 21160/27699\t Batch loss: 4.088\t Batch perplexity: 59.638\t Batch accuracy: 28.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 21180/27699\t Batch loss: 4.238\t Batch perplexity: 69.262\t Batch accuracy: 27.5%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 21200/27699\t Batch loss: 4.154\t Batch perplexity: 63.665\t Batch accuracy: 27.4%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 21220/27699\t Batch loss: 4.322\t Batch perplexity: 75.348\t Batch accuracy: 27.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 21240/27699\t Batch loss: 4.148\t Batch perplexity: 63.318\t Batch accuracy: 27.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.079\t Batch perplexity: 59.097\t Batch accuracy: 29.0%\t***\n",
      "Epoch: 1/2\t Train step: 21260/27699\t Batch loss: 4.149\t Batch perplexity: 63.361\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 21280/27699\t Batch loss: 4.365\t Batch perplexity: 78.651\t Batch accuracy: 26.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 21300/27699\t Batch loss: 4.344\t Batch perplexity: 77.040\t Batch accuracy: 26.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 21320/27699\t Batch loss: 4.401\t Batch perplexity: 81.548\t Batch accuracy: 25.1%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 21340/27699\t Batch loss: 4.190\t Batch perplexity: 65.991\t Batch accuracy: 26.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 21360/27699\t Batch loss: 4.288\t Batch perplexity: 72.785\t Batch accuracy: 26.9%\t 0.74s/batch\n",
      "Epoch: 1/2\t Train step: 21380/27699\t Batch loss: 4.124\t Batch perplexity: 61.798\t Batch accuracy: 29.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 21400/27699\t Batch loss: 4.123\t Batch perplexity: 61.751\t Batch accuracy: 28.7%\t 0.72s/batch\n",
      "Epoch: 1/2\t Train step: 21420/27699\t Batch loss: 4.157\t Batch perplexity: 63.849\t Batch accuracy: 27.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 21440/27699\t Batch loss: 4.373\t Batch perplexity: 79.312\t Batch accuracy: 24.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 21460/27699\t Batch loss: 4.299\t Batch perplexity: 73.647\t Batch accuracy: 28.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 21480/27699\t Batch loss: 4.440\t Batch perplexity: 84.768\t Batch accuracy: 25.6%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 21500/27699\t Batch loss: 4.316\t Batch perplexity: 74.925\t Batch accuracy: 26.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 3.931\t Batch perplexity: 50.967\t Batch accuracy: 31.5%\t***\n",
      "Epoch: 1/2\t Train step: 21520/27699\t Batch loss: 4.112\t Batch perplexity: 61.059\t Batch accuracy: 29.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 21540/27699\t Batch loss: 4.251\t Batch perplexity: 70.185\t Batch accuracy: 25.4%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 21560/27699\t Batch loss: 4.102\t Batch perplexity: 60.455\t Batch accuracy: 29.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 21580/27699\t Batch loss: 4.057\t Batch perplexity: 57.792\t Batch accuracy: 28.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 21600/27699\t Batch loss: 4.144\t Batch perplexity: 63.041\t Batch accuracy: 28.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 21620/27699\t Batch loss: 4.255\t Batch perplexity: 70.446\t Batch accuracy: 25.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 21640/27699\t Batch loss: 4.241\t Batch perplexity: 69.490\t Batch accuracy: 28.8%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 21660/27699\t Batch loss: 4.207\t Batch perplexity: 67.154\t Batch accuracy: 29.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 21680/27699\t Batch loss: 4.501\t Batch perplexity: 90.098\t Batch accuracy: 24.3%\t 0.78s/batch\n",
      "Epoch: 1/2\t Train step: 21700/27699\t Batch loss: 4.233\t Batch perplexity: 68.910\t Batch accuracy: 27.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 21720/27699\t Batch loss: 4.191\t Batch perplexity: 66.077\t Batch accuracy: 27.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 21740/27699\t Batch loss: 4.297\t Batch perplexity: 73.445\t Batch accuracy: 26.7%\t 0.70s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 3.916\t Batch perplexity: 50.193\t Batch accuracy: 32.1%\t***\n",
      "Epoch: 1/2\t Train step: 21760/27699\t Batch loss: 4.065\t Batch perplexity: 58.269\t Batch accuracy: 28.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 21780/27699\t Batch loss: 4.343\t Batch perplexity: 76.912\t Batch accuracy: 28.0%\t 0.72s/batch\n",
      "Epoch: 1/2\t Train step: 21800/27699\t Batch loss: 4.350\t Batch perplexity: 77.503\t Batch accuracy: 27.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 21820/27699\t Batch loss: 3.984\t Batch perplexity: 53.728\t Batch accuracy: 32.3%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 21840/27699\t Batch loss: 4.369\t Batch perplexity: 78.992\t Batch accuracy: 26.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 21860/27699\t Batch loss: 4.079\t Batch perplexity: 59.098\t Batch accuracy: 27.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 21880/27699\t Batch loss: 4.406\t Batch perplexity: 81.924\t Batch accuracy: 26.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 21900/27699\t Batch loss: 4.321\t Batch perplexity: 75.251\t Batch accuracy: 27.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 21920/27699\t Batch loss: 3.948\t Batch perplexity: 51.848\t Batch accuracy: 29.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 21940/27699\t Batch loss: 4.219\t Batch perplexity: 67.958\t Batch accuracy: 28.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 21960/27699\t Batch loss: 4.279\t Batch perplexity: 72.201\t Batch accuracy: 25.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 21980/27699\t Batch loss: 4.001\t Batch perplexity: 54.626\t Batch accuracy: 30.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 22000/27699\t Batch loss: 4.153\t Batch perplexity: 63.614\t Batch accuracy: 28.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.090\t Batch perplexity: 59.748\t Batch accuracy: 28.3%\t***\n",
      "Epoch: 1/2\t Train step: 22020/27699\t Batch loss: 4.348\t Batch perplexity: 77.359\t Batch accuracy: 28.5%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 22040/27699\t Batch loss: 4.086\t Batch perplexity: 59.509\t Batch accuracy: 28.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 22060/27699\t Batch loss: 4.299\t Batch perplexity: 73.661\t Batch accuracy: 26.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 22080/27699\t Batch loss: 4.239\t Batch perplexity: 69.335\t Batch accuracy: 27.3%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 22100/27699\t Batch loss: 4.171\t Batch perplexity: 64.750\t Batch accuracy: 26.9%\t 0.68s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\t Train step: 22120/27699\t Batch loss: 4.005\t Batch perplexity: 54.890\t Batch accuracy: 28.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 22140/27699\t Batch loss: 4.279\t Batch perplexity: 72.175\t Batch accuracy: 26.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 22160/27699\t Batch loss: 4.037\t Batch perplexity: 56.653\t Batch accuracy: 29.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 22180/27699\t Batch loss: 4.327\t Batch perplexity: 75.711\t Batch accuracy: 25.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 22200/27699\t Batch loss: 4.314\t Batch perplexity: 74.744\t Batch accuracy: 28.1%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 22220/27699\t Batch loss: 4.425\t Batch perplexity: 83.527\t Batch accuracy: 24.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 22240/27699\t Batch loss: 4.078\t Batch perplexity: 59.027\t Batch accuracy: 29.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.010\t Batch perplexity: 55.133\t Batch accuracy: 29.5%\t***\n",
      "Epoch: 1/2\t Train step: 22260/27699\t Batch loss: 4.119\t Batch perplexity: 61.509\t Batch accuracy: 27.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 22280/27699\t Batch loss: 4.176\t Batch perplexity: 65.127\t Batch accuracy: 27.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 22300/27699\t Batch loss: 4.215\t Batch perplexity: 67.707\t Batch accuracy: 26.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 22320/27699\t Batch loss: 4.133\t Batch perplexity: 62.335\t Batch accuracy: 28.7%\t 0.78s/batch\n",
      "Epoch: 1/2\t Train step: 22340/27699\t Batch loss: 4.261\t Batch perplexity: 70.848\t Batch accuracy: 26.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 22360/27699\t Batch loss: 4.119\t Batch perplexity: 61.495\t Batch accuracy: 27.9%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 22380/27699\t Batch loss: 4.239\t Batch perplexity: 69.323\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 22400/27699\t Batch loss: 4.319\t Batch perplexity: 75.126\t Batch accuracy: 27.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 22420/27699\t Batch loss: 4.291\t Batch perplexity: 73.010\t Batch accuracy: 24.0%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 22440/27699\t Batch loss: 4.134\t Batch perplexity: 62.430\t Batch accuracy: 29.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 22460/27699\t Batch loss: 4.303\t Batch perplexity: 73.947\t Batch accuracy: 27.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 22480/27699\t Batch loss: 4.279\t Batch perplexity: 72.143\t Batch accuracy: 27.2%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 22500/27699\t Batch loss: 4.292\t Batch perplexity: 73.093\t Batch accuracy: 26.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.053\t Batch perplexity: 57.597\t Batch accuracy: 28.3%\t***\n",
      "Epoch: 1/2\t Train step: 22520/27699\t Batch loss: 4.184\t Batch perplexity: 65.611\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 22540/27699\t Batch loss: 4.208\t Batch perplexity: 67.231\t Batch accuracy: 27.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 22560/27699\t Batch loss: 3.996\t Batch perplexity: 54.363\t Batch accuracy: 30.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 22580/27699\t Batch loss: 4.281\t Batch perplexity: 72.284\t Batch accuracy: 26.1%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 22600/27699\t Batch loss: 4.153\t Batch perplexity: 63.606\t Batch accuracy: 26.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 22620/27699\t Batch loss: 4.142\t Batch perplexity: 62.919\t Batch accuracy: 29.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 22640/27699\t Batch loss: 4.147\t Batch perplexity: 63.244\t Batch accuracy: 27.6%\t 0.78s/batch\n",
      "Epoch: 1/2\t Train step: 22660/27699\t Batch loss: 4.159\t Batch perplexity: 64.004\t Batch accuracy: 27.3%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 22680/27699\t Batch loss: 4.120\t Batch perplexity: 61.552\t Batch accuracy: 29.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 22700/27699\t Batch loss: 4.157\t Batch perplexity: 63.873\t Batch accuracy: 28.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 22720/27699\t Batch loss: 4.376\t Batch perplexity: 79.496\t Batch accuracy: 24.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 22740/27699\t Batch loss: 4.259\t Batch perplexity: 70.731\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.052\t Batch perplexity: 57.498\t Batch accuracy: 30.4%\t***\n",
      "Epoch: 1/2\t Train step: 22760/27699\t Batch loss: 4.050\t Batch perplexity: 57.376\t Batch accuracy: 29.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 22780/27699\t Batch loss: 4.200\t Batch perplexity: 66.661\t Batch accuracy: 29.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 22800/27699\t Batch loss: 4.411\t Batch perplexity: 82.352\t Batch accuracy: 26.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 22820/27699\t Batch loss: 4.226\t Batch perplexity: 68.429\t Batch accuracy: 26.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 22840/27699\t Batch loss: 4.351\t Batch perplexity: 77.519\t Batch accuracy: 26.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 22860/27699\t Batch loss: 4.331\t Batch perplexity: 76.015\t Batch accuracy: 28.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 22880/27699\t Batch loss: 4.228\t Batch perplexity: 68.565\t Batch accuracy: 27.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 22900/27699\t Batch loss: 4.311\t Batch perplexity: 74.507\t Batch accuracy: 27.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 22920/27699\t Batch loss: 4.237\t Batch perplexity: 69.208\t Batch accuracy: 26.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 22940/27699\t Batch loss: 4.327\t Batch perplexity: 75.738\t Batch accuracy: 25.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 22960/27699\t Batch loss: 4.063\t Batch perplexity: 58.133\t Batch accuracy: 28.3%\t 0.77s/batch\n",
      "Epoch: 1/2\t Train step: 22980/27699\t Batch loss: 4.365\t Batch perplexity: 78.635\t Batch accuracy: 28.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 23000/27699\t Batch loss: 4.181\t Batch perplexity: 65.430\t Batch accuracy: 29.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.188\t Batch perplexity: 65.922\t Batch accuracy: 29.1%\t***\n",
      "Epoch: 1/2\t Train step: 23020/27699\t Batch loss: 3.911\t Batch perplexity: 49.927\t Batch accuracy: 29.8%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 23040/27699\t Batch loss: 4.359\t Batch perplexity: 78.169\t Batch accuracy: 26.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 23060/27699\t Batch loss: 4.300\t Batch perplexity: 73.728\t Batch accuracy: 29.5%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 23080/27699\t Batch loss: 4.256\t Batch perplexity: 70.499\t Batch accuracy: 27.5%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 23100/27699\t Batch loss: 4.060\t Batch perplexity: 57.967\t Batch accuracy: 29.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 23120/27699\t Batch loss: 4.038\t Batch perplexity: 56.720\t Batch accuracy: 29.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 23140/27699\t Batch loss: 4.049\t Batch perplexity: 57.360\t Batch accuracy: 29.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 23160/27699\t Batch loss: 4.025\t Batch perplexity: 55.982\t Batch accuracy: 30.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 23180/27699\t Batch loss: 4.327\t Batch perplexity: 75.736\t Batch accuracy: 30.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 23200/27699\t Batch loss: 4.053\t Batch perplexity: 57.552\t Batch accuracy: 30.3%\t 0.76s/batch\n",
      "Epoch: 1/2\t Train step: 23220/27699\t Batch loss: 4.385\t Batch perplexity: 80.229\t Batch accuracy: 25.8%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 23240/27699\t Batch loss: 4.074\t Batch perplexity: 58.782\t Batch accuracy: 30.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 3.798\t Batch perplexity: 44.598\t Batch accuracy: 29.2%\t***\n",
      "Epoch: 1/2\t Train step: 23260/27699\t Batch loss: 4.305\t Batch perplexity: 74.096\t Batch accuracy: 26.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 23280/27699\t Batch loss: 4.205\t Batch perplexity: 67.009\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 23300/27699\t Batch loss: 4.279\t Batch perplexity: 72.192\t Batch accuracy: 27.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 23320/27699\t Batch loss: 4.287\t Batch perplexity: 72.769\t Batch accuracy: 27.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 23340/27699\t Batch loss: 4.209\t Batch perplexity: 67.322\t Batch accuracy: 26.7%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 23360/27699\t Batch loss: 4.211\t Batch perplexity: 67.423\t Batch accuracy: 26.0%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 23380/27699\t Batch loss: 4.153\t Batch perplexity: 63.642\t Batch accuracy: 27.5%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 23400/27699\t Batch loss: 4.363\t Batch perplexity: 78.520\t Batch accuracy: 27.6%\t 0.68s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\t Train step: 23420/27699\t Batch loss: 4.267\t Batch perplexity: 71.334\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 23440/27699\t Batch loss: 4.455\t Batch perplexity: 86.093\t Batch accuracy: 27.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 23460/27699\t Batch loss: 4.321\t Batch perplexity: 75.246\t Batch accuracy: 25.9%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 23480/27699\t Batch loss: 3.982\t Batch perplexity: 53.611\t Batch accuracy: 31.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 23500/27699\t Batch loss: 4.260\t Batch perplexity: 70.843\t Batch accuracy: 25.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.006\t Batch perplexity: 54.954\t Batch accuracy: 29.5%\t***\n",
      "Epoch: 1/2\t Train step: 23520/27699\t Batch loss: 4.266\t Batch perplexity: 71.202\t Batch accuracy: 26.4%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 23540/27699\t Batch loss: 4.183\t Batch perplexity: 65.538\t Batch accuracy: 26.0%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 23560/27699\t Batch loss: 4.155\t Batch perplexity: 63.779\t Batch accuracy: 30.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 23580/27699\t Batch loss: 4.175\t Batch perplexity: 65.024\t Batch accuracy: 26.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 23600/27699\t Batch loss: 4.159\t Batch perplexity: 63.992\t Batch accuracy: 28.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 23620/27699\t Batch loss: 4.230\t Batch perplexity: 68.734\t Batch accuracy: 26.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 23640/27699\t Batch loss: 3.999\t Batch perplexity: 54.562\t Batch accuracy: 30.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 23660/27699\t Batch loss: 4.453\t Batch perplexity: 85.911\t Batch accuracy: 24.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 23680/27699\t Batch loss: 4.199\t Batch perplexity: 66.627\t Batch accuracy: 25.9%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 23700/27699\t Batch loss: 4.135\t Batch perplexity: 62.498\t Batch accuracy: 27.5%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 23720/27699\t Batch loss: 4.284\t Batch perplexity: 72.546\t Batch accuracy: 27.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 23740/27699\t Batch loss: 4.141\t Batch perplexity: 62.885\t Batch accuracy: 28.3%\t 0.71s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 3.944\t Batch perplexity: 51.631\t Batch accuracy: 31.3%\t***\n",
      "Epoch: 1/2\t Train step: 23760/27699\t Batch loss: 4.091\t Batch perplexity: 59.797\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 23780/27699\t Batch loss: 4.258\t Batch perplexity: 70.686\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 23800/27699\t Batch loss: 4.317\t Batch perplexity: 74.968\t Batch accuracy: 25.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 23820/27699\t Batch loss: 4.219\t Batch perplexity: 67.993\t Batch accuracy: 27.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 23840/27699\t Batch loss: 4.240\t Batch perplexity: 69.373\t Batch accuracy: 26.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 23860/27699\t Batch loss: 4.183\t Batch perplexity: 65.592\t Batch accuracy: 26.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 23880/27699\t Batch loss: 4.192\t Batch perplexity: 66.149\t Batch accuracy: 27.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 23900/27699\t Batch loss: 4.185\t Batch perplexity: 65.710\t Batch accuracy: 27.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 23920/27699\t Batch loss: 4.404\t Batch perplexity: 81.814\t Batch accuracy: 25.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 23940/27699\t Batch loss: 4.298\t Batch perplexity: 73.555\t Batch accuracy: 25.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 23960/27699\t Batch loss: 4.086\t Batch perplexity: 59.519\t Batch accuracy: 29.3%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 23980/27699\t Batch loss: 4.083\t Batch perplexity: 59.352\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 24000/27699\t Batch loss: 4.137\t Batch perplexity: 62.603\t Batch accuracy: 27.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.222\t Batch perplexity: 68.168\t Batch accuracy: 25.7%\t***\n",
      "Epoch: 1/2\t Train step: 24020/27699\t Batch loss: 4.153\t Batch perplexity: 63.632\t Batch accuracy: 29.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 24040/27699\t Batch loss: 4.185\t Batch perplexity: 65.675\t Batch accuracy: 27.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 24060/27699\t Batch loss: 4.237\t Batch perplexity: 69.231\t Batch accuracy: 26.5%\t 0.75s/batch\n",
      "Epoch: 1/2\t Train step: 24080/27699\t Batch loss: 4.141\t Batch perplexity: 62.852\t Batch accuracy: 27.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 24100/27699\t Batch loss: 4.105\t Batch perplexity: 60.649\t Batch accuracy: 28.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 24120/27699\t Batch loss: 4.037\t Batch perplexity: 56.680\t Batch accuracy: 31.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 24140/27699\t Batch loss: 4.203\t Batch perplexity: 66.917\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 24160/27699\t Batch loss: 4.203\t Batch perplexity: 66.885\t Batch accuracy: 27.9%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 24180/27699\t Batch loss: 4.260\t Batch perplexity: 70.779\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 24200/27699\t Batch loss: 4.107\t Batch perplexity: 60.787\t Batch accuracy: 27.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 24220/27699\t Batch loss: 4.033\t Batch perplexity: 56.405\t Batch accuracy: 30.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 24240/27699\t Batch loss: 4.254\t Batch perplexity: 70.381\t Batch accuracy: 28.2%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.039\t Batch perplexity: 56.771\t Batch accuracy: 28.6%\t***\n",
      "Epoch: 1/2\t Train step: 24260/27699\t Batch loss: 4.159\t Batch perplexity: 64.007\t Batch accuracy: 29.0%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 24280/27699\t Batch loss: 4.324\t Batch perplexity: 75.464\t Batch accuracy: 28.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 24300/27699\t Batch loss: 4.239\t Batch perplexity: 69.359\t Batch accuracy: 26.1%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 24320/27699\t Batch loss: 4.159\t Batch perplexity: 63.986\t Batch accuracy: 27.7%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 24340/27699\t Batch loss: 4.131\t Batch perplexity: 62.257\t Batch accuracy: 28.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 24360/27699\t Batch loss: 4.072\t Batch perplexity: 58.659\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 24380/27699\t Batch loss: 4.235\t Batch perplexity: 69.058\t Batch accuracy: 28.6%\t 0.80s/batch\n",
      "Epoch: 1/2\t Train step: 24400/27699\t Batch loss: 4.381\t Batch perplexity: 79.923\t Batch accuracy: 24.3%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 24420/27699\t Batch loss: 4.175\t Batch perplexity: 65.056\t Batch accuracy: 29.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 24440/27699\t Batch loss: 4.070\t Batch perplexity: 58.534\t Batch accuracy: 28.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 24460/27699\t Batch loss: 4.474\t Batch perplexity: 87.687\t Batch accuracy: 26.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 24480/27699\t Batch loss: 4.282\t Batch perplexity: 72.390\t Batch accuracy: 26.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 24500/27699\t Batch loss: 4.175\t Batch perplexity: 65.035\t Batch accuracy: 26.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.121\t Batch perplexity: 61.593\t Batch accuracy: 28.9%\t***\n",
      "Epoch: 1/2\t Train step: 24520/27699\t Batch loss: 4.306\t Batch perplexity: 74.126\t Batch accuracy: 25.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 24540/27699\t Batch loss: 4.188\t Batch perplexity: 65.908\t Batch accuracy: 26.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 24560/27699\t Batch loss: 4.192\t Batch perplexity: 66.131\t Batch accuracy: 26.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 24580/27699\t Batch loss: 3.957\t Batch perplexity: 52.287\t Batch accuracy: 30.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 24600/27699\t Batch loss: 3.988\t Batch perplexity: 53.955\t Batch accuracy: 29.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 24620/27699\t Batch loss: 4.033\t Batch perplexity: 56.435\t Batch accuracy: 29.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 24640/27699\t Batch loss: 4.048\t Batch perplexity: 57.254\t Batch accuracy: 28.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 24660/27699\t Batch loss: 4.022\t Batch perplexity: 55.808\t Batch accuracy: 29.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 24680/27699\t Batch loss: 4.073\t Batch perplexity: 58.746\t Batch accuracy: 28.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 24700/27699\t Batch loss: 4.201\t Batch perplexity: 66.757\t Batch accuracy: 26.8%\t 0.79s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\t Train step: 24720/27699\t Batch loss: 4.358\t Batch perplexity: 78.125\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 24740/27699\t Batch loss: 4.244\t Batch perplexity: 69.662\t Batch accuracy: 26.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.131\t Batch perplexity: 62.211\t Batch accuracy: 28.6%\t***\n",
      "Epoch: 1/2\t Train step: 24760/27699\t Batch loss: 4.102\t Batch perplexity: 60.483\t Batch accuracy: 28.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 24780/27699\t Batch loss: 4.396\t Batch perplexity: 81.140\t Batch accuracy: 27.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 24800/27699\t Batch loss: 4.200\t Batch perplexity: 66.695\t Batch accuracy: 27.2%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 24820/27699\t Batch loss: 4.162\t Batch perplexity: 64.213\t Batch accuracy: 29.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 24840/27699\t Batch loss: 4.318\t Batch perplexity: 75.038\t Batch accuracy: 24.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 24860/27699\t Batch loss: 4.036\t Batch perplexity: 56.584\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 24880/27699\t Batch loss: 3.998\t Batch perplexity: 54.509\t Batch accuracy: 28.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 24900/27699\t Batch loss: 4.220\t Batch perplexity: 68.054\t Batch accuracy: 29.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 24920/27699\t Batch loss: 4.146\t Batch perplexity: 63.166\t Batch accuracy: 29.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 24940/27699\t Batch loss: 4.230\t Batch perplexity: 68.707\t Batch accuracy: 26.6%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 24960/27699\t Batch loss: 4.135\t Batch perplexity: 62.468\t Batch accuracy: 27.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 24980/27699\t Batch loss: 4.429\t Batch perplexity: 83.831\t Batch accuracy: 26.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 25000/27699\t Batch loss: 4.075\t Batch perplexity: 58.866\t Batch accuracy: 27.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.007\t Batch perplexity: 54.982\t Batch accuracy: 29.8%\t***\n",
      "Epoch: 1/2\t Train step: 25020/27699\t Batch loss: 4.107\t Batch perplexity: 60.740\t Batch accuracy: 30.4%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 25040/27699\t Batch loss: 4.245\t Batch perplexity: 69.778\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 25060/27699\t Batch loss: 4.207\t Batch perplexity: 67.146\t Batch accuracy: 28.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 25080/27699\t Batch loss: 4.221\t Batch perplexity: 68.116\t Batch accuracy: 27.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 25100/27699\t Batch loss: 4.211\t Batch perplexity: 67.398\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 25120/27699\t Batch loss: 4.288\t Batch perplexity: 72.817\t Batch accuracy: 28.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 25140/27699\t Batch loss: 4.248\t Batch perplexity: 69.952\t Batch accuracy: 27.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 25160/27699\t Batch loss: 4.196\t Batch perplexity: 66.393\t Batch accuracy: 27.2%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 25180/27699\t Batch loss: 4.170\t Batch perplexity: 64.728\t Batch accuracy: 26.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 25200/27699\t Batch loss: 4.142\t Batch perplexity: 62.933\t Batch accuracy: 28.1%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 25220/27699\t Batch loss: 4.468\t Batch perplexity: 87.220\t Batch accuracy: 25.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 25240/27699\t Batch loss: 4.150\t Batch perplexity: 63.425\t Batch accuracy: 27.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 3.997\t Batch perplexity: 54.409\t Batch accuracy: 31.9%\t***\n",
      "Epoch: 1/2\t Train step: 25260/27699\t Batch loss: 4.260\t Batch perplexity: 70.805\t Batch accuracy: 26.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 25280/27699\t Batch loss: 4.224\t Batch perplexity: 68.324\t Batch accuracy: 26.2%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 25300/27699\t Batch loss: 4.140\t Batch perplexity: 62.811\t Batch accuracy: 27.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 25320/27699\t Batch loss: 4.129\t Batch perplexity: 62.107\t Batch accuracy: 28.4%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 25340/27699\t Batch loss: 4.357\t Batch perplexity: 78.042\t Batch accuracy: 27.5%\t 0.80s/batch\n",
      "Epoch: 1/2\t Train step: 25360/27699\t Batch loss: 4.234\t Batch perplexity: 69.015\t Batch accuracy: 28.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 25380/27699\t Batch loss: 4.309\t Batch perplexity: 74.371\t Batch accuracy: 28.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 25400/27699\t Batch loss: 4.109\t Batch perplexity: 60.857\t Batch accuracy: 27.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 25420/27699\t Batch loss: 4.406\t Batch perplexity: 81.907\t Batch accuracy: 26.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 25440/27699\t Batch loss: 4.352\t Batch perplexity: 77.652\t Batch accuracy: 23.9%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 25460/27699\t Batch loss: 4.165\t Batch perplexity: 64.371\t Batch accuracy: 26.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 25480/27699\t Batch loss: 4.346\t Batch perplexity: 77.158\t Batch accuracy: 25.7%\t 0.75s/batch\n",
      "Epoch: 1/2\t Train step: 25500/27699\t Batch loss: 4.276\t Batch perplexity: 71.933\t Batch accuracy: 29.0%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 3.844\t Batch perplexity: 46.699\t Batch accuracy: 29.8%\t***\n",
      "Epoch: 1/2\t Train step: 25520/27699\t Batch loss: 4.473\t Batch perplexity: 87.636\t Batch accuracy: 26.1%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 25540/27699\t Batch loss: 4.168\t Batch perplexity: 64.577\t Batch accuracy: 26.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 25560/27699\t Batch loss: 4.127\t Batch perplexity: 62.007\t Batch accuracy: 24.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 25580/27699\t Batch loss: 4.206\t Batch perplexity: 67.087\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 25600/27699\t Batch loss: 4.357\t Batch perplexity: 78.061\t Batch accuracy: 25.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 25620/27699\t Batch loss: 3.977\t Batch perplexity: 53.331\t Batch accuracy: 29.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 25640/27699\t Batch loss: 4.188\t Batch perplexity: 65.875\t Batch accuracy: 25.5%\t 0.81s/batch\n",
      "Epoch: 1/2\t Train step: 25660/27699\t Batch loss: 4.136\t Batch perplexity: 62.576\t Batch accuracy: 30.4%\t 0.73s/batch\n",
      "Epoch: 1/2\t Train step: 25680/27699\t Batch loss: 4.095\t Batch perplexity: 60.056\t Batch accuracy: 29.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 25700/27699\t Batch loss: 4.378\t Batch perplexity: 79.688\t Batch accuracy: 26.0%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 25720/27699\t Batch loss: 4.343\t Batch perplexity: 76.975\t Batch accuracy: 28.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 25740/27699\t Batch loss: 4.337\t Batch perplexity: 76.484\t Batch accuracy: 26.0%\t 0.70s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 3.730\t Batch perplexity: 41.681\t Batch accuracy: 32.1%\t***\n",
      "Epoch: 1/2\t Train step: 25760/27699\t Batch loss: 4.203\t Batch perplexity: 66.910\t Batch accuracy: 27.5%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 25780/27699\t Batch loss: 4.351\t Batch perplexity: 77.526\t Batch accuracy: 26.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 25800/27699\t Batch loss: 4.091\t Batch perplexity: 59.811\t Batch accuracy: 30.1%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 25820/27699\t Batch loss: 4.239\t Batch perplexity: 69.335\t Batch accuracy: 26.8%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 25840/27699\t Batch loss: 4.197\t Batch perplexity: 66.470\t Batch accuracy: 28.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 25860/27699\t Batch loss: 4.114\t Batch perplexity: 61.178\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 25880/27699\t Batch loss: 4.253\t Batch perplexity: 70.293\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 25900/27699\t Batch loss: 4.148\t Batch perplexity: 63.311\t Batch accuracy: 29.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 25920/27699\t Batch loss: 4.208\t Batch perplexity: 67.199\t Batch accuracy: 27.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 25940/27699\t Batch loss: 4.064\t Batch perplexity: 58.212\t Batch accuracy: 28.0%\t 0.76s/batch\n",
      "Epoch: 1/2\t Train step: 25960/27699\t Batch loss: 4.120\t Batch perplexity: 61.551\t Batch accuracy: 29.4%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 25980/27699\t Batch loss: 4.196\t Batch perplexity: 66.425\t Batch accuracy: 29.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 26000/27699\t Batch loss: 4.402\t Batch perplexity: 81.597\t Batch accuracy: 26.1%\t 0.68s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 3.992\t Batch perplexity: 54.140\t Batch accuracy: 29.6%\t***\n",
      "Epoch: 1/2\t Train step: 26020/27699\t Batch loss: 4.085\t Batch perplexity: 59.419\t Batch accuracy: 27.6%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 26040/27699\t Batch loss: 4.057\t Batch perplexity: 57.818\t Batch accuracy: 26.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 26060/27699\t Batch loss: 4.056\t Batch perplexity: 57.743\t Batch accuracy: 30.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 26080/27699\t Batch loss: 4.062\t Batch perplexity: 58.082\t Batch accuracy: 30.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 26100/27699\t Batch loss: 4.338\t Batch perplexity: 76.516\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 26120/27699\t Batch loss: 4.211\t Batch perplexity: 67.424\t Batch accuracy: 28.6%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 26140/27699\t Batch loss: 4.268\t Batch perplexity: 71.359\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 26160/27699\t Batch loss: 4.193\t Batch perplexity: 66.192\t Batch accuracy: 27.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 26180/27699\t Batch loss: 4.362\t Batch perplexity: 78.431\t Batch accuracy: 25.6%\t 0.71s/batch\n",
      "Epoch: 1/2\t Train step: 26200/27699\t Batch loss: 3.984\t Batch perplexity: 53.724\t Batch accuracy: 29.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 26220/27699\t Batch loss: 4.238\t Batch perplexity: 69.288\t Batch accuracy: 26.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 26240/27699\t Batch loss: 4.251\t Batch perplexity: 70.184\t Batch accuracy: 27.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.059\t Batch perplexity: 57.920\t Batch accuracy: 29.9%\t***\n",
      "Epoch: 1/2\t Train step: 26260/27699\t Batch loss: 4.274\t Batch perplexity: 71.805\t Batch accuracy: 28.2%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 26280/27699\t Batch loss: 4.292\t Batch perplexity: 73.126\t Batch accuracy: 26.6%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 26300/27699\t Batch loss: 4.205\t Batch perplexity: 67.025\t Batch accuracy: 27.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 26320/27699\t Batch loss: 4.165\t Batch perplexity: 64.370\t Batch accuracy: 26.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 26340/27699\t Batch loss: 4.103\t Batch perplexity: 60.530\t Batch accuracy: 29.4%\t 0.72s/batch\n",
      "Epoch: 1/2\t Train step: 26360/27699\t Batch loss: 4.209\t Batch perplexity: 67.318\t Batch accuracy: 27.4%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 26380/27699\t Batch loss: 4.061\t Batch perplexity: 58.038\t Batch accuracy: 28.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 26400/27699\t Batch loss: 4.367\t Batch perplexity: 78.814\t Batch accuracy: 28.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 26420/27699\t Batch loss: 4.100\t Batch perplexity: 60.351\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 26440/27699\t Batch loss: 4.266\t Batch perplexity: 71.220\t Batch accuracy: 28.8%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 26460/27699\t Batch loss: 4.267\t Batch perplexity: 71.287\t Batch accuracy: 26.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 26480/27699\t Batch loss: 4.081\t Batch perplexity: 59.195\t Batch accuracy: 28.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 26500/27699\t Batch loss: 4.334\t Batch perplexity: 76.281\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.100\t Batch perplexity: 60.351\t Batch accuracy: 29.8%\t***\n",
      "Epoch: 1/2\t Train step: 26520/27699\t Batch loss: 4.154\t Batch perplexity: 63.660\t Batch accuracy: 25.4%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 26540/27699\t Batch loss: 4.184\t Batch perplexity: 65.613\t Batch accuracy: 29.2%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 26560/27699\t Batch loss: 4.242\t Batch perplexity: 69.562\t Batch accuracy: 28.1%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 26580/27699\t Batch loss: 4.310\t Batch perplexity: 74.460\t Batch accuracy: 27.7%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 26600/27699\t Batch loss: 4.285\t Batch perplexity: 72.587\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 26620/27699\t Batch loss: 4.337\t Batch perplexity: 76.490\t Batch accuracy: 26.5%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 26640/27699\t Batch loss: 4.207\t Batch perplexity: 67.142\t Batch accuracy: 27.8%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 26660/27699\t Batch loss: 4.121\t Batch perplexity: 61.598\t Batch accuracy: 26.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 26680/27699\t Batch loss: 4.299\t Batch perplexity: 73.648\t Batch accuracy: 27.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 26700/27699\t Batch loss: 4.344\t Batch perplexity: 76.991\t Batch accuracy: 25.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 26720/27699\t Batch loss: 4.210\t Batch perplexity: 67.347\t Batch accuracy: 26.8%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 26740/27699\t Batch loss: 4.266\t Batch perplexity: 71.241\t Batch accuracy: 26.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 3.884\t Batch perplexity: 48.600\t Batch accuracy: 30.0%\t***\n",
      "Epoch: 1/2\t Train step: 26760/27699\t Batch loss: 4.385\t Batch perplexity: 80.202\t Batch accuracy: 25.5%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 26780/27699\t Batch loss: 4.207\t Batch perplexity: 67.156\t Batch accuracy: 27.1%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 26800/27699\t Batch loss: 4.168\t Batch perplexity: 64.589\t Batch accuracy: 29.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 26820/27699\t Batch loss: 4.273\t Batch perplexity: 71.749\t Batch accuracy: 26.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 26840/27699\t Batch loss: 4.234\t Batch perplexity: 68.969\t Batch accuracy: 27.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 26860/27699\t Batch loss: 4.174\t Batch perplexity: 64.965\t Batch accuracy: 29.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 26880/27699\t Batch loss: 4.266\t Batch perplexity: 71.244\t Batch accuracy: 29.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 26900/27699\t Batch loss: 4.080\t Batch perplexity: 59.128\t Batch accuracy: 27.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 26920/27699\t Batch loss: 4.204\t Batch perplexity: 66.930\t Batch accuracy: 27.7%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 26940/27699\t Batch loss: 4.043\t Batch perplexity: 56.990\t Batch accuracy: 28.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 26960/27699\t Batch loss: 4.146\t Batch perplexity: 63.155\t Batch accuracy: 27.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 26980/27699\t Batch loss: 4.115\t Batch perplexity: 61.246\t Batch accuracy: 29.4%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 27000/27699\t Batch loss: 4.087\t Batch perplexity: 59.578\t Batch accuracy: 27.2%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 3.857\t Batch perplexity: 47.300\t Batch accuracy: 30.0%\t***\n",
      "Epoch: 1/2\t Train step: 27020/27699\t Batch loss: 4.116\t Batch perplexity: 61.339\t Batch accuracy: 30.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 27040/27699\t Batch loss: 4.274\t Batch perplexity: 71.819\t Batch accuracy: 26.7%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 27060/27699\t Batch loss: 3.942\t Batch perplexity: 51.528\t Batch accuracy: 28.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 27080/27699\t Batch loss: 4.312\t Batch perplexity: 74.619\t Batch accuracy: 25.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 27100/27699\t Batch loss: 4.203\t Batch perplexity: 66.883\t Batch accuracy: 27.3%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 27120/27699\t Batch loss: 4.311\t Batch perplexity: 74.532\t Batch accuracy: 27.9%\t 0.72s/batch\n",
      "Epoch: 1/2\t Train step: 27140/27699\t Batch loss: 3.998\t Batch perplexity: 54.513\t Batch accuracy: 30.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 27160/27699\t Batch loss: 4.141\t Batch perplexity: 62.867\t Batch accuracy: 29.5%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 27180/27699\t Batch loss: 4.141\t Batch perplexity: 62.855\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 27200/27699\t Batch loss: 4.059\t Batch perplexity: 57.888\t Batch accuracy: 29.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 27220/27699\t Batch loss: 4.129\t Batch perplexity: 62.104\t Batch accuracy: 28.7%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 27240/27699\t Batch loss: 4.255\t Batch perplexity: 70.479\t Batch accuracy: 27.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 4.025\t Batch perplexity: 56.006\t Batch accuracy: 27.2%\t***\n",
      "Epoch: 1/2\t Train step: 27260/27699\t Batch loss: 4.069\t Batch perplexity: 58.492\t Batch accuracy: 30.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 27280/27699\t Batch loss: 4.030\t Batch perplexity: 56.234\t Batch accuracy: 30.8%\t 0.69s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\t Train step: 27300/27699\t Batch loss: 4.313\t Batch perplexity: 74.645\t Batch accuracy: 25.5%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 27320/27699\t Batch loss: 4.487\t Batch perplexity: 88.859\t Batch accuracy: 26.0%\t 0.66s/batch\n",
      "Epoch: 1/2\t Train step: 27340/27699\t Batch loss: 4.182\t Batch perplexity: 65.507\t Batch accuracy: 27.1%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 27360/27699\t Batch loss: 4.403\t Batch perplexity: 81.696\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 27380/27699\t Batch loss: 4.327\t Batch perplexity: 75.683\t Batch accuracy: 25.6%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 27400/27699\t Batch loss: 4.260\t Batch perplexity: 70.839\t Batch accuracy: 26.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 27420/27699\t Batch loss: 4.131\t Batch perplexity: 62.231\t Batch accuracy: 28.1%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 27440/27699\t Batch loss: 4.316\t Batch perplexity: 74.904\t Batch accuracy: 29.9%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 27460/27699\t Batch loss: 4.282\t Batch perplexity: 72.405\t Batch accuracy: 25.8%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 27480/27699\t Batch loss: 4.055\t Batch perplexity: 57.695\t Batch accuracy: 29.3%\t 0.76s/batch\n",
      "Epoch: 1/2\t Train step: 27500/27699\t Batch loss: 4.319\t Batch perplexity: 75.089\t Batch accuracy: 26.9%\t 0.68s/batch\n",
      "Epoch: 1/2\t * Validation batch *\t Batch loss: 3.901\t Batch perplexity: 49.448\t Batch accuracy: 31.4%\t***\n",
      "Epoch: 1/2\t Train step: 27520/27699\t Batch loss: 4.238\t Batch perplexity: 69.257\t Batch accuracy: 25.8%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 27540/27699\t Batch loss: 4.224\t Batch perplexity: 68.309\t Batch accuracy: 27.7%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 27560/27699\t Batch loss: 4.266\t Batch perplexity: 71.243\t Batch accuracy: 28.0%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 27580/27699\t Batch loss: 4.267\t Batch perplexity: 71.333\t Batch accuracy: 25.7%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 27600/27699\t Batch loss: 4.220\t Batch perplexity: 68.011\t Batch accuracy: 25.8%\t 0.67s/batch\n",
      "Epoch: 1/2\t Train step: 27620/27699\t Batch loss: 4.126\t Batch perplexity: 61.930\t Batch accuracy: 28.3%\t 0.69s/batch\n",
      "Epoch: 1/2\t Train step: 27640/27699\t Batch loss: 4.018\t Batch perplexity: 55.595\t Batch accuracy: 30.2%\t 0.68s/batch\n",
      "Epoch: 1/2\t Train step: 27660/27699\t Batch loss: 4.244\t Batch perplexity: 69.661\t Batch accuracy: 27.6%\t 0.70s/batch\n",
      "Epoch: 1/2\t Train step: 27680/27699\t Batch loss: 4.264\t Batch perplexity: 71.070\t Batch accuracy: 27.8%\t 0.80s/batch\n",
      "Epoch: 1/2\t Train step: 27700/27699\t Batch loss: 4.319\t Batch perplexity: 75.134\t Batch accuracy: 27.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 20/27699\t Batch loss: 4.054\t Batch perplexity: 57.639\t Batch accuracy: 28.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 40/27699\t Batch loss: 3.995\t Batch perplexity: 54.344\t Batch accuracy: 29.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 60/27699\t Batch loss: 4.210\t Batch perplexity: 67.327\t Batch accuracy: 26.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 80/27699\t Batch loss: 4.107\t Batch perplexity: 60.757\t Batch accuracy: 30.8%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 100/27699\t Batch loss: 4.170\t Batch perplexity: 64.690\t Batch accuracy: 28.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 120/27699\t Batch loss: 4.173\t Batch perplexity: 64.936\t Batch accuracy: 27.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 140/27699\t Batch loss: 4.273\t Batch perplexity: 71.734\t Batch accuracy: 26.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 160/27699\t Batch loss: 4.137\t Batch perplexity: 62.636\t Batch accuracy: 29.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 180/27699\t Batch loss: 4.080\t Batch perplexity: 59.169\t Batch accuracy: 27.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 200/27699\t Batch loss: 4.002\t Batch perplexity: 54.734\t Batch accuracy: 29.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 220/27699\t Batch loss: 3.959\t Batch perplexity: 52.380\t Batch accuracy: 29.3%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 240/27699\t Batch loss: 4.192\t Batch perplexity: 66.145\t Batch accuracy: 27.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.974\t Batch perplexity: 53.202\t Batch accuracy: 31.3%\t***\n",
      "Epoch: 2/2\t Train step: 260/27699\t Batch loss: 4.039\t Batch perplexity: 56.751\t Batch accuracy: 29.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 280/27699\t Batch loss: 4.283\t Batch perplexity: 72.484\t Batch accuracy: 26.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 300/27699\t Batch loss: 4.242\t Batch perplexity: 69.577\t Batch accuracy: 26.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 320/27699\t Batch loss: 3.794\t Batch perplexity: 44.416\t Batch accuracy: 33.2%\t 0.80s/batch\n",
      "Epoch: 2/2\t Train step: 340/27699\t Batch loss: 4.033\t Batch perplexity: 56.422\t Batch accuracy: 31.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 360/27699\t Batch loss: 4.097\t Batch perplexity: 60.133\t Batch accuracy: 28.1%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 380/27699\t Batch loss: 4.551\t Batch perplexity: 94.687\t Batch accuracy: 24.8%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 400/27699\t Batch loss: 4.081\t Batch perplexity: 59.192\t Batch accuracy: 28.1%\t 0.83s/batch\n",
      "Epoch: 2/2\t Train step: 420/27699\t Batch loss: 4.021\t Batch perplexity: 55.758\t Batch accuracy: 31.9%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 440/27699\t Batch loss: 4.052\t Batch perplexity: 57.506\t Batch accuracy: 30.1%\t 0.77s/batch\n",
      "Epoch: 2/2\t Train step: 460/27699\t Batch loss: 4.146\t Batch perplexity: 63.191\t Batch accuracy: 29.4%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 480/27699\t Batch loss: 4.234\t Batch perplexity: 68.983\t Batch accuracy: 29.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 500/27699\t Batch loss: 3.968\t Batch perplexity: 52.854\t Batch accuracy: 28.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.183\t Batch perplexity: 65.558\t Batch accuracy: 29.7%\t***\n",
      "Epoch: 2/2\t Train step: 520/27699\t Batch loss: 4.058\t Batch perplexity: 57.848\t Batch accuracy: 29.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 540/27699\t Batch loss: 4.243\t Batch perplexity: 69.587\t Batch accuracy: 28.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 560/27699\t Batch loss: 4.206\t Batch perplexity: 67.069\t Batch accuracy: 26.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 580/27699\t Batch loss: 4.126\t Batch perplexity: 61.938\t Batch accuracy: 29.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 600/27699\t Batch loss: 4.195\t Batch perplexity: 66.377\t Batch accuracy: 25.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 620/27699\t Batch loss: 4.297\t Batch perplexity: 73.462\t Batch accuracy: 25.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 640/27699\t Batch loss: 4.170\t Batch perplexity: 64.687\t Batch accuracy: 29.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 660/27699\t Batch loss: 4.065\t Batch perplexity: 58.277\t Batch accuracy: 27.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 680/27699\t Batch loss: 4.160\t Batch perplexity: 64.056\t Batch accuracy: 24.8%\t 0.66s/batch\n",
      "Epoch: 2/2\t Train step: 700/27699\t Batch loss: 4.184\t Batch perplexity: 65.613\t Batch accuracy: 27.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 720/27699\t Batch loss: 4.034\t Batch perplexity: 56.466\t Batch accuracy: 29.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 740/27699\t Batch loss: 4.122\t Batch perplexity: 61.660\t Batch accuracy: 31.4%\t 0.67s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.994\t Batch perplexity: 54.247\t Batch accuracy: 29.7%\t***\n",
      "Epoch: 2/2\t Train step: 760/27699\t Batch loss: 4.168\t Batch perplexity: 64.618\t Batch accuracy: 24.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 780/27699\t Batch loss: 4.117\t Batch perplexity: 61.348\t Batch accuracy: 28.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 800/27699\t Batch loss: 4.278\t Batch perplexity: 72.132\t Batch accuracy: 27.7%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 820/27699\t Batch loss: 4.032\t Batch perplexity: 56.361\t Batch accuracy: 27.6%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 840/27699\t Batch loss: 4.252\t Batch perplexity: 70.238\t Batch accuracy: 27.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 860/27699\t Batch loss: 4.184\t Batch perplexity: 65.641\t Batch accuracy: 28.7%\t 0.80s/batch\n",
      "Epoch: 2/2\t Train step: 880/27699\t Batch loss: 4.216\t Batch perplexity: 67.777\t Batch accuracy: 30.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 900/27699\t Batch loss: 3.978\t Batch perplexity: 53.401\t Batch accuracy: 30.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 920/27699\t Batch loss: 4.082\t Batch perplexity: 59.245\t Batch accuracy: 30.3%\t 0.67s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2\t Train step: 940/27699\t Batch loss: 4.098\t Batch perplexity: 60.205\t Batch accuracy: 26.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 960/27699\t Batch loss: 4.328\t Batch perplexity: 75.805\t Batch accuracy: 26.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 980/27699\t Batch loss: 4.115\t Batch perplexity: 61.252\t Batch accuracy: 29.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 1000/27699\t Batch loss: 4.017\t Batch perplexity: 55.536\t Batch accuracy: 29.8%\t 0.70s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.934\t Batch perplexity: 51.121\t Batch accuracy: 29.9%\t***\n",
      "Epoch: 2/2\t Train step: 1020/27699\t Batch loss: 4.211\t Batch perplexity: 67.426\t Batch accuracy: 27.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 1040/27699\t Batch loss: 4.116\t Batch perplexity: 61.341\t Batch accuracy: 29.7%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 1060/27699\t Batch loss: 4.304\t Batch perplexity: 74.032\t Batch accuracy: 27.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 1080/27699\t Batch loss: 4.029\t Batch perplexity: 56.201\t Batch accuracy: 28.7%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 1100/27699\t Batch loss: 4.307\t Batch perplexity: 74.184\t Batch accuracy: 25.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 1120/27699\t Batch loss: 4.132\t Batch perplexity: 62.289\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 1140/27699\t Batch loss: 4.202\t Batch perplexity: 66.836\t Batch accuracy: 29.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 1160/27699\t Batch loss: 4.328\t Batch perplexity: 75.805\t Batch accuracy: 28.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 1180/27699\t Batch loss: 4.083\t Batch perplexity: 59.312\t Batch accuracy: 29.1%\t 0.76s/batch\n",
      "Epoch: 2/2\t Train step: 1200/27699\t Batch loss: 4.206\t Batch perplexity: 67.109\t Batch accuracy: 26.8%\t 0.74s/batch\n",
      "Epoch: 2/2\t Train step: 1220/27699\t Batch loss: 4.247\t Batch perplexity: 69.863\t Batch accuracy: 26.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 1240/27699\t Batch loss: 4.016\t Batch perplexity: 55.458\t Batch accuracy: 30.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.361\t Batch perplexity: 78.353\t Batch accuracy: 28.2%\t***\n",
      "Epoch: 2/2\t Train step: 1260/27699\t Batch loss: 4.148\t Batch perplexity: 63.327\t Batch accuracy: 26.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 1280/27699\t Batch loss: 4.064\t Batch perplexity: 58.183\t Batch accuracy: 27.8%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 1300/27699\t Batch loss: 4.218\t Batch perplexity: 67.899\t Batch accuracy: 27.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 1320/27699\t Batch loss: 4.238\t Batch perplexity: 69.260\t Batch accuracy: 28.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 1340/27699\t Batch loss: 4.281\t Batch perplexity: 72.307\t Batch accuracy: 27.1%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 1360/27699\t Batch loss: 4.218\t Batch perplexity: 67.909\t Batch accuracy: 29.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 1380/27699\t Batch loss: 4.159\t Batch perplexity: 63.999\t Batch accuracy: 28.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 1400/27699\t Batch loss: 4.002\t Batch perplexity: 54.729\t Batch accuracy: 27.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 1420/27699\t Batch loss: 4.094\t Batch perplexity: 59.949\t Batch accuracy: 26.3%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 1440/27699\t Batch loss: 4.209\t Batch perplexity: 67.293\t Batch accuracy: 26.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 1460/27699\t Batch loss: 4.108\t Batch perplexity: 60.807\t Batch accuracy: 28.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 1480/27699\t Batch loss: 4.151\t Batch perplexity: 63.487\t Batch accuracy: 26.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 1500/27699\t Batch loss: 4.007\t Batch perplexity: 54.988\t Batch accuracy: 28.3%\t 0.73s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.903\t Batch perplexity: 49.560\t Batch accuracy: 32.3%\t***\n",
      "Epoch: 2/2\t Train step: 1520/27699\t Batch loss: 4.396\t Batch perplexity: 81.089\t Batch accuracy: 25.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 1540/27699\t Batch loss: 4.251\t Batch perplexity: 70.153\t Batch accuracy: 26.2%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 1560/27699\t Batch loss: 4.069\t Batch perplexity: 58.518\t Batch accuracy: 28.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 1580/27699\t Batch loss: 4.222\t Batch perplexity: 68.184\t Batch accuracy: 25.4%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 1600/27699\t Batch loss: 4.051\t Batch perplexity: 57.432\t Batch accuracy: 28.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 1620/27699\t Batch loss: 4.091\t Batch perplexity: 59.823\t Batch accuracy: 28.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 1640/27699\t Batch loss: 4.136\t Batch perplexity: 62.576\t Batch accuracy: 29.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 1660/27699\t Batch loss: 4.189\t Batch perplexity: 65.984\t Batch accuracy: 28.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 1680/27699\t Batch loss: 3.968\t Batch perplexity: 52.899\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 1700/27699\t Batch loss: 4.249\t Batch perplexity: 70.027\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 1720/27699\t Batch loss: 4.057\t Batch perplexity: 57.778\t Batch accuracy: 29.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 1740/27699\t Batch loss: 4.033\t Batch perplexity: 56.413\t Batch accuracy: 28.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.108\t Batch perplexity: 60.799\t Batch accuracy: 29.7%\t***\n",
      "Epoch: 2/2\t Train step: 1760/27699\t Batch loss: 3.982\t Batch perplexity: 53.599\t Batch accuracy: 30.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 1780/27699\t Batch loss: 4.237\t Batch perplexity: 69.224\t Batch accuracy: 27.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 1800/27699\t Batch loss: 4.032\t Batch perplexity: 56.365\t Batch accuracy: 29.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 1820/27699\t Batch loss: 4.132\t Batch perplexity: 62.301\t Batch accuracy: 27.9%\t 0.75s/batch\n",
      "Epoch: 2/2\t Train step: 1840/27699\t Batch loss: 4.158\t Batch perplexity: 63.925\t Batch accuracy: 29.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 1860/27699\t Batch loss: 4.060\t Batch perplexity: 57.987\t Batch accuracy: 30.1%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 1880/27699\t Batch loss: 3.993\t Batch perplexity: 54.207\t Batch accuracy: 29.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 1900/27699\t Batch loss: 4.143\t Batch perplexity: 62.998\t Batch accuracy: 26.4%\t 0.73s/batch\n",
      "Epoch: 2/2\t Train step: 1920/27699\t Batch loss: 4.013\t Batch perplexity: 55.287\t Batch accuracy: 29.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 1940/27699\t Batch loss: 4.023\t Batch perplexity: 55.896\t Batch accuracy: 28.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 1960/27699\t Batch loss: 4.224\t Batch perplexity: 68.309\t Batch accuracy: 27.6%\t 0.72s/batch\n",
      "Epoch: 2/2\t Train step: 1980/27699\t Batch loss: 4.423\t Batch perplexity: 83.329\t Batch accuracy: 27.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 2000/27699\t Batch loss: 4.341\t Batch perplexity: 76.747\t Batch accuracy: 26.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.125\t Batch perplexity: 61.844\t Batch accuracy: 29.5%\t***\n",
      "Epoch: 2/2\t Train step: 2020/27699\t Batch loss: 4.234\t Batch perplexity: 68.975\t Batch accuracy: 26.6%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 2040/27699\t Batch loss: 4.298\t Batch perplexity: 73.537\t Batch accuracy: 25.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 2060/27699\t Batch loss: 4.104\t Batch perplexity: 60.594\t Batch accuracy: 29.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 2080/27699\t Batch loss: 4.134\t Batch perplexity: 62.421\t Batch accuracy: 29.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 2100/27699\t Batch loss: 4.372\t Batch perplexity: 79.181\t Batch accuracy: 25.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 2120/27699\t Batch loss: 4.314\t Batch perplexity: 74.765\t Batch accuracy: 26.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 2140/27699\t Batch loss: 4.152\t Batch perplexity: 63.582\t Batch accuracy: 28.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 2160/27699\t Batch loss: 4.109\t Batch perplexity: 60.886\t Batch accuracy: 27.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 2180/27699\t Batch loss: 3.936\t Batch perplexity: 51.200\t Batch accuracy: 30.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 2200/27699\t Batch loss: 4.105\t Batch perplexity: 60.633\t Batch accuracy: 26.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 2220/27699\t Batch loss: 4.011\t Batch perplexity: 55.195\t Batch accuracy: 30.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 2240/27699\t Batch loss: 4.144\t Batch perplexity: 63.071\t Batch accuracy: 27.0%\t 0.69s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.952\t Batch perplexity: 52.020\t Batch accuracy: 28.3%\t***\n",
      "Epoch: 2/2\t Train step: 2260/27699\t Batch loss: 4.250\t Batch perplexity: 70.109\t Batch accuracy: 27.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 2280/27699\t Batch loss: 4.044\t Batch perplexity: 57.076\t Batch accuracy: 28.6%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 2300/27699\t Batch loss: 4.286\t Batch perplexity: 72.692\t Batch accuracy: 25.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 2320/27699\t Batch loss: 4.152\t Batch perplexity: 63.562\t Batch accuracy: 29.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 2340/27699\t Batch loss: 4.251\t Batch perplexity: 70.153\t Batch accuracy: 27.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 2360/27699\t Batch loss: 4.164\t Batch perplexity: 64.332\t Batch accuracy: 25.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 2380/27699\t Batch loss: 4.270\t Batch perplexity: 71.539\t Batch accuracy: 28.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 2400/27699\t Batch loss: 3.989\t Batch perplexity: 53.998\t Batch accuracy: 28.9%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 2420/27699\t Batch loss: 4.322\t Batch perplexity: 75.341\t Batch accuracy: 27.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 2440/27699\t Batch loss: 4.331\t Batch perplexity: 76.049\t Batch accuracy: 26.1%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 2460/27699\t Batch loss: 4.153\t Batch perplexity: 63.645\t Batch accuracy: 25.3%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 2480/27699\t Batch loss: 4.124\t Batch perplexity: 61.823\t Batch accuracy: 28.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 2500/27699\t Batch loss: 4.180\t Batch perplexity: 65.393\t Batch accuracy: 29.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.247\t Batch perplexity: 69.869\t Batch accuracy: 28.4%\t***\n",
      "Epoch: 2/2\t Train step: 2520/27699\t Batch loss: 4.030\t Batch perplexity: 56.261\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 2540/27699\t Batch loss: 4.211\t Batch perplexity: 67.453\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 2560/27699\t Batch loss: 4.136\t Batch perplexity: 62.537\t Batch accuracy: 30.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 2580/27699\t Batch loss: 4.024\t Batch perplexity: 55.907\t Batch accuracy: 29.2%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 2600/27699\t Batch loss: 4.064\t Batch perplexity: 58.204\t Batch accuracy: 29.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 2620/27699\t Batch loss: 4.364\t Batch perplexity: 78.558\t Batch accuracy: 27.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 2640/27699\t Batch loss: 4.040\t Batch perplexity: 56.801\t Batch accuracy: 30.7%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 2660/27699\t Batch loss: 4.021\t Batch perplexity: 55.739\t Batch accuracy: 28.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 2680/27699\t Batch loss: 3.859\t Batch perplexity: 47.414\t Batch accuracy: 30.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 2700/27699\t Batch loss: 4.199\t Batch perplexity: 66.624\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 2720/27699\t Batch loss: 3.972\t Batch perplexity: 53.089\t Batch accuracy: 31.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 2740/27699\t Batch loss: 3.947\t Batch perplexity: 51.782\t Batch accuracy: 30.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.128\t Batch perplexity: 62.084\t Batch accuracy: 28.9%\t***\n",
      "Epoch: 2/2\t Train step: 2760/27699\t Batch loss: 4.088\t Batch perplexity: 59.604\t Batch accuracy: 28.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 2780/27699\t Batch loss: 4.259\t Batch perplexity: 70.758\t Batch accuracy: 27.5%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 2800/27699\t Batch loss: 4.121\t Batch perplexity: 61.632\t Batch accuracy: 28.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 2820/27699\t Batch loss: 4.038\t Batch perplexity: 56.725\t Batch accuracy: 31.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 2840/27699\t Batch loss: 4.127\t Batch perplexity: 61.971\t Batch accuracy: 28.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 2860/27699\t Batch loss: 4.105\t Batch perplexity: 60.648\t Batch accuracy: 30.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 2880/27699\t Batch loss: 4.286\t Batch perplexity: 72.680\t Batch accuracy: 27.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 2900/27699\t Batch loss: 3.929\t Batch perplexity: 50.838\t Batch accuracy: 31.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 2920/27699\t Batch loss: 4.142\t Batch perplexity: 62.942\t Batch accuracy: 26.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 2940/27699\t Batch loss: 4.084\t Batch perplexity: 59.406\t Batch accuracy: 29.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 2960/27699\t Batch loss: 4.032\t Batch perplexity: 56.362\t Batch accuracy: 27.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 2980/27699\t Batch loss: 4.294\t Batch perplexity: 73.234\t Batch accuracy: 26.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 3000/27699\t Batch loss: 4.330\t Batch perplexity: 75.920\t Batch accuracy: 26.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.142\t Batch perplexity: 62.939\t Batch accuracy: 29.7%\t***\n",
      "Epoch: 2/2\t Train step: 3020/27699\t Batch loss: 4.178\t Batch perplexity: 65.211\t Batch accuracy: 27.0%\t 0.66s/batch\n",
      "Epoch: 2/2\t Train step: 3040/27699\t Batch loss: 4.158\t Batch perplexity: 63.937\t Batch accuracy: 27.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 3060/27699\t Batch loss: 4.069\t Batch perplexity: 58.482\t Batch accuracy: 31.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 3080/27699\t Batch loss: 4.295\t Batch perplexity: 73.359\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 3100/27699\t Batch loss: 4.023\t Batch perplexity: 55.870\t Batch accuracy: 29.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 3120/27699\t Batch loss: 4.271\t Batch perplexity: 71.624\t Batch accuracy: 26.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 3140/27699\t Batch loss: 4.193\t Batch perplexity: 66.234\t Batch accuracy: 27.6%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 3160/27699\t Batch loss: 4.117\t Batch perplexity: 61.390\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 3180/27699\t Batch loss: 4.028\t Batch perplexity: 56.142\t Batch accuracy: 27.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 3200/27699\t Batch loss: 4.064\t Batch perplexity: 58.182\t Batch accuracy: 28.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 3220/27699\t Batch loss: 3.984\t Batch perplexity: 53.749\t Batch accuracy: 29.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 3240/27699\t Batch loss: 4.306\t Batch perplexity: 74.115\t Batch accuracy: 26.1%\t 0.67s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.119\t Batch perplexity: 61.520\t Batch accuracy: 27.9%\t***\n",
      "Epoch: 2/2\t Train step: 3260/27699\t Batch loss: 4.085\t Batch perplexity: 59.430\t Batch accuracy: 30.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 3280/27699\t Batch loss: 4.230\t Batch perplexity: 68.749\t Batch accuracy: 25.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 3300/27699\t Batch loss: 4.022\t Batch perplexity: 55.811\t Batch accuracy: 29.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 3320/27699\t Batch loss: 4.244\t Batch perplexity: 69.698\t Batch accuracy: 27.5%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 3340/27699\t Batch loss: 3.893\t Batch perplexity: 49.056\t Batch accuracy: 31.2%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 3360/27699\t Batch loss: 4.130\t Batch perplexity: 62.172\t Batch accuracy: 29.4%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 3380/27699\t Batch loss: 4.004\t Batch perplexity: 54.844\t Batch accuracy: 28.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 3400/27699\t Batch loss: 4.152\t Batch perplexity: 63.559\t Batch accuracy: 26.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 3420/27699\t Batch loss: 4.172\t Batch perplexity: 64.864\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 3440/27699\t Batch loss: 4.124\t Batch perplexity: 61.803\t Batch accuracy: 28.2%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 3460/27699\t Batch loss: 4.060\t Batch perplexity: 57.954\t Batch accuracy: 28.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 3480/27699\t Batch loss: 4.266\t Batch perplexity: 71.201\t Batch accuracy: 26.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 3500/27699\t Batch loss: 4.135\t Batch perplexity: 62.516\t Batch accuracy: 29.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.157\t Batch perplexity: 63.857\t Batch accuracy: 27.2%\t***\n",
      "Epoch: 2/2\t Train step: 3520/27699\t Batch loss: 4.082\t Batch perplexity: 59.281\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 3540/27699\t Batch loss: 3.916\t Batch perplexity: 50.210\t Batch accuracy: 30.9%\t 0.68s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2\t Train step: 3560/27699\t Batch loss: 4.211\t Batch perplexity: 67.411\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 3580/27699\t Batch loss: 4.217\t Batch perplexity: 67.813\t Batch accuracy: 26.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 3600/27699\t Batch loss: 4.234\t Batch perplexity: 68.979\t Batch accuracy: 27.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 3620/27699\t Batch loss: 4.207\t Batch perplexity: 67.137\t Batch accuracy: 27.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 3640/27699\t Batch loss: 3.993\t Batch perplexity: 54.220\t Batch accuracy: 29.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 3660/27699\t Batch loss: 4.055\t Batch perplexity: 57.709\t Batch accuracy: 26.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 3680/27699\t Batch loss: 4.089\t Batch perplexity: 59.672\t Batch accuracy: 27.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 3700/27699\t Batch loss: 4.143\t Batch perplexity: 63.011\t Batch accuracy: 28.6%\t 0.78s/batch\n",
      "Epoch: 2/2\t Train step: 3720/27699\t Batch loss: 4.136\t Batch perplexity: 62.543\t Batch accuracy: 26.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 3740/27699\t Batch loss: 4.265\t Batch perplexity: 71.187\t Batch accuracy: 28.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.006\t Batch perplexity: 54.948\t Batch accuracy: 28.9%\t***\n",
      "Epoch: 2/2\t Train step: 3760/27699\t Batch loss: 4.136\t Batch perplexity: 62.526\t Batch accuracy: 27.6%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 3780/27699\t Batch loss: 4.139\t Batch perplexity: 62.744\t Batch accuracy: 30.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 3800/27699\t Batch loss: 4.326\t Batch perplexity: 75.605\t Batch accuracy: 26.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 3820/27699\t Batch loss: 4.070\t Batch perplexity: 58.547\t Batch accuracy: 27.4%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 3840/27699\t Batch loss: 4.103\t Batch perplexity: 60.503\t Batch accuracy: 29.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 3860/27699\t Batch loss: 4.123\t Batch perplexity: 61.768\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 3880/27699\t Batch loss: 4.286\t Batch perplexity: 72.702\t Batch accuracy: 26.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 3900/27699\t Batch loss: 4.259\t Batch perplexity: 70.723\t Batch accuracy: 25.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 3920/27699\t Batch loss: 4.065\t Batch perplexity: 58.241\t Batch accuracy: 27.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 3940/27699\t Batch loss: 4.301\t Batch perplexity: 73.742\t Batch accuracy: 26.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 3960/27699\t Batch loss: 4.010\t Batch perplexity: 55.155\t Batch accuracy: 29.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 3980/27699\t Batch loss: 4.411\t Batch perplexity: 82.357\t Batch accuracy: 25.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 4000/27699\t Batch loss: 4.220\t Batch perplexity: 68.063\t Batch accuracy: 28.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.988\t Batch perplexity: 53.927\t Batch accuracy: 28.8%\t***\n",
      "Epoch: 2/2\t Train step: 4020/27699\t Batch loss: 4.245\t Batch perplexity: 69.730\t Batch accuracy: 27.6%\t 0.73s/batch\n",
      "Epoch: 2/2\t Train step: 4040/27699\t Batch loss: 4.067\t Batch perplexity: 58.370\t Batch accuracy: 28.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 4060/27699\t Batch loss: 4.184\t Batch perplexity: 65.619\t Batch accuracy: 26.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 4080/27699\t Batch loss: 3.934\t Batch perplexity: 51.109\t Batch accuracy: 30.4%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 4100/27699\t Batch loss: 4.178\t Batch perplexity: 65.205\t Batch accuracy: 28.6%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 4120/27699\t Batch loss: 3.995\t Batch perplexity: 54.334\t Batch accuracy: 28.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 4140/27699\t Batch loss: 4.186\t Batch perplexity: 65.728\t Batch accuracy: 28.8%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 4160/27699\t Batch loss: 4.157\t Batch perplexity: 63.862\t Batch accuracy: 28.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 4180/27699\t Batch loss: 4.367\t Batch perplexity: 78.769\t Batch accuracy: 27.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 4200/27699\t Batch loss: 4.399\t Batch perplexity: 81.339\t Batch accuracy: 23.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 4220/27699\t Batch loss: 3.849\t Batch perplexity: 46.930\t Batch accuracy: 30.5%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 4240/27699\t Batch loss: 4.112\t Batch perplexity: 61.064\t Batch accuracy: 26.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.986\t Batch perplexity: 53.835\t Batch accuracy: 31.6%\t***\n",
      "Epoch: 2/2\t Train step: 4260/27699\t Batch loss: 4.015\t Batch perplexity: 55.429\t Batch accuracy: 28.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 4280/27699\t Batch loss: 4.151\t Batch perplexity: 63.491\t Batch accuracy: 28.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 4300/27699\t Batch loss: 4.224\t Batch perplexity: 68.306\t Batch accuracy: 27.0%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 4320/27699\t Batch loss: 3.989\t Batch perplexity: 54.011\t Batch accuracy: 29.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 4340/27699\t Batch loss: 4.524\t Batch perplexity: 92.170\t Batch accuracy: 25.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 4360/27699\t Batch loss: 4.194\t Batch perplexity: 66.292\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 4380/27699\t Batch loss: 4.247\t Batch perplexity: 69.926\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 4400/27699\t Batch loss: 4.093\t Batch perplexity: 59.901\t Batch accuracy: 30.3%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 4420/27699\t Batch loss: 3.985\t Batch perplexity: 53.785\t Batch accuracy: 31.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 4440/27699\t Batch loss: 4.039\t Batch perplexity: 56.759\t Batch accuracy: 28.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 4460/27699\t Batch loss: 4.190\t Batch perplexity: 66.041\t Batch accuracy: 27.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 4480/27699\t Batch loss: 3.947\t Batch perplexity: 51.795\t Batch accuracy: 30.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 4500/27699\t Batch loss: 4.195\t Batch perplexity: 66.327\t Batch accuracy: 26.3%\t 0.70s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.942\t Batch perplexity: 51.525\t Batch accuracy: 30.4%\t***\n",
      "Epoch: 2/2\t Train step: 4520/27699\t Batch loss: 3.896\t Batch perplexity: 49.185\t Batch accuracy: 30.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 4540/27699\t Batch loss: 4.160\t Batch perplexity: 64.077\t Batch accuracy: 26.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 4560/27699\t Batch loss: 4.144\t Batch perplexity: 63.059\t Batch accuracy: 29.1%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 4580/27699\t Batch loss: 4.090\t Batch perplexity: 59.752\t Batch accuracy: 28.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 4600/27699\t Batch loss: 4.126\t Batch perplexity: 61.932\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 4620/27699\t Batch loss: 4.236\t Batch perplexity: 69.118\t Batch accuracy: 28.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 4640/27699\t Batch loss: 4.060\t Batch perplexity: 57.998\t Batch accuracy: 30.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 4660/27699\t Batch loss: 4.071\t Batch perplexity: 58.597\t Batch accuracy: 29.2%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 4680/27699\t Batch loss: 4.263\t Batch perplexity: 71.023\t Batch accuracy: 26.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 4700/27699\t Batch loss: 4.088\t Batch perplexity: 59.634\t Batch accuracy: 28.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 4720/27699\t Batch loss: 4.246\t Batch perplexity: 69.825\t Batch accuracy: 29.6%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 4740/27699\t Batch loss: 4.078\t Batch perplexity: 59.009\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.945\t Batch perplexity: 51.691\t Batch accuracy: 30.4%\t***\n",
      "Epoch: 2/2\t Train step: 4760/27699\t Batch loss: 4.258\t Batch perplexity: 70.677\t Batch accuracy: 27.4%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 4780/27699\t Batch loss: 4.264\t Batch perplexity: 71.105\t Batch accuracy: 28.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 4800/27699\t Batch loss: 4.164\t Batch perplexity: 64.347\t Batch accuracy: 27.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 4820/27699\t Batch loss: 4.089\t Batch perplexity: 59.671\t Batch accuracy: 29.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 4840/27699\t Batch loss: 4.078\t Batch perplexity: 59.026\t Batch accuracy: 29.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 4860/27699\t Batch loss: 3.958\t Batch perplexity: 52.357\t Batch accuracy: 29.7%\t 0.70s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2\t Train step: 4880/27699\t Batch loss: 4.166\t Batch perplexity: 64.449\t Batch accuracy: 26.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 4900/27699\t Batch loss: 4.240\t Batch perplexity: 69.431\t Batch accuracy: 27.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 4920/27699\t Batch loss: 4.237\t Batch perplexity: 69.229\t Batch accuracy: 29.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 4940/27699\t Batch loss: 4.268\t Batch perplexity: 71.348\t Batch accuracy: 25.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 4960/27699\t Batch loss: 4.096\t Batch perplexity: 60.070\t Batch accuracy: 28.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 4980/27699\t Batch loss: 4.250\t Batch perplexity: 70.139\t Batch accuracy: 25.7%\t 0.81s/batch\n",
      "Epoch: 2/2\t Train step: 5000/27699\t Batch loss: 4.072\t Batch perplexity: 58.672\t Batch accuracy: 29.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.044\t Batch perplexity: 57.073\t Batch accuracy: 30.1%\t***\n",
      "Epoch: 2/2\t Train step: 5020/27699\t Batch loss: 4.067\t Batch perplexity: 58.410\t Batch accuracy: 29.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 5040/27699\t Batch loss: 4.359\t Batch perplexity: 78.146\t Batch accuracy: 26.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 5060/27699\t Batch loss: 4.185\t Batch perplexity: 65.722\t Batch accuracy: 30.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 5080/27699\t Batch loss: 4.349\t Batch perplexity: 77.364\t Batch accuracy: 26.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 5100/27699\t Batch loss: 4.132\t Batch perplexity: 62.308\t Batch accuracy: 27.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 5120/27699\t Batch loss: 4.081\t Batch perplexity: 59.224\t Batch accuracy: 30.2%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 5140/27699\t Batch loss: 4.077\t Batch perplexity: 58.943\t Batch accuracy: 27.6%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 5160/27699\t Batch loss: 4.172\t Batch perplexity: 64.856\t Batch accuracy: 26.9%\t 0.74s/batch\n",
      "Epoch: 2/2\t Train step: 5180/27699\t Batch loss: 4.159\t Batch perplexity: 64.036\t Batch accuracy: 29.6%\t 0.72s/batch\n",
      "Epoch: 2/2\t Train step: 5200/27699\t Batch loss: 4.123\t Batch perplexity: 61.751\t Batch accuracy: 28.3%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 5220/27699\t Batch loss: 4.220\t Batch perplexity: 68.041\t Batch accuracy: 29.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 5240/27699\t Batch loss: 4.177\t Batch perplexity: 65.148\t Batch accuracy: 28.0%\t 0.70s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.922\t Batch perplexity: 50.522\t Batch accuracy: 31.5%\t***\n",
      "Epoch: 2/2\t Train step: 5260/27699\t Batch loss: 3.992\t Batch perplexity: 54.183\t Batch accuracy: 29.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 5280/27699\t Batch loss: 4.114\t Batch perplexity: 61.199\t Batch accuracy: 28.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 5300/27699\t Batch loss: 4.000\t Batch perplexity: 54.574\t Batch accuracy: 31.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 5320/27699\t Batch loss: 4.195\t Batch perplexity: 66.386\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 5340/27699\t Batch loss: 4.169\t Batch perplexity: 64.641\t Batch accuracy: 27.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 5360/27699\t Batch loss: 4.115\t Batch perplexity: 61.245\t Batch accuracy: 29.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 5380/27699\t Batch loss: 4.253\t Batch perplexity: 70.304\t Batch accuracy: 27.7%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 5400/27699\t Batch loss: 4.146\t Batch perplexity: 63.196\t Batch accuracy: 26.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 5420/27699\t Batch loss: 4.128\t Batch perplexity: 62.062\t Batch accuracy: 29.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 5440/27699\t Batch loss: 4.217\t Batch perplexity: 67.825\t Batch accuracy: 25.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 5460/27699\t Batch loss: 4.083\t Batch perplexity: 59.318\t Batch accuracy: 28.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 5480/27699\t Batch loss: 4.151\t Batch perplexity: 63.506\t Batch accuracy: 27.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 5500/27699\t Batch loss: 4.042\t Batch perplexity: 56.965\t Batch accuracy: 29.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.984\t Batch perplexity: 53.755\t Batch accuracy: 30.8%\t***\n",
      "Epoch: 2/2\t Train step: 5520/27699\t Batch loss: 4.031\t Batch perplexity: 56.309\t Batch accuracy: 27.7%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 5540/27699\t Batch loss: 4.103\t Batch perplexity: 60.518\t Batch accuracy: 28.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 5560/27699\t Batch loss: 4.150\t Batch perplexity: 63.407\t Batch accuracy: 27.6%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 5580/27699\t Batch loss: 4.002\t Batch perplexity: 54.709\t Batch accuracy: 31.9%\t 0.79s/batch\n",
      "Epoch: 2/2\t Train step: 5600/27699\t Batch loss: 4.029\t Batch perplexity: 56.198\t Batch accuracy: 30.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 5620/27699\t Batch loss: 4.188\t Batch perplexity: 65.858\t Batch accuracy: 26.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 5640/27699\t Batch loss: 4.431\t Batch perplexity: 84.035\t Batch accuracy: 25.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 5660/27699\t Batch loss: 4.054\t Batch perplexity: 57.619\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 5680/27699\t Batch loss: 4.351\t Batch perplexity: 77.535\t Batch accuracy: 29.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 5700/27699\t Batch loss: 3.889\t Batch perplexity: 48.860\t Batch accuracy: 31.3%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 5720/27699\t Batch loss: 3.982\t Batch perplexity: 53.631\t Batch accuracy: 30.0%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 5740/27699\t Batch loss: 4.195\t Batch perplexity: 66.333\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.948\t Batch perplexity: 51.834\t Batch accuracy: 29.0%\t***\n",
      "Epoch: 2/2\t Train step: 5760/27699\t Batch loss: 3.933\t Batch perplexity: 51.058\t Batch accuracy: 29.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 5780/27699\t Batch loss: 4.100\t Batch perplexity: 60.351\t Batch accuracy: 29.7%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 5800/27699\t Batch loss: 4.309\t Batch perplexity: 74.376\t Batch accuracy: 25.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 5820/27699\t Batch loss: 4.238\t Batch perplexity: 69.257\t Batch accuracy: 28.3%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 5840/27699\t Batch loss: 4.361\t Batch perplexity: 78.351\t Batch accuracy: 25.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 5860/27699\t Batch loss: 4.262\t Batch perplexity: 70.962\t Batch accuracy: 25.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 5880/27699\t Batch loss: 4.144\t Batch perplexity: 63.052\t Batch accuracy: 28.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 5900/27699\t Batch loss: 4.010\t Batch perplexity: 55.129\t Batch accuracy: 30.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 5920/27699\t Batch loss: 4.320\t Batch perplexity: 75.155\t Batch accuracy: 26.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 5940/27699\t Batch loss: 4.470\t Batch perplexity: 87.364\t Batch accuracy: 25.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 5960/27699\t Batch loss: 4.126\t Batch perplexity: 61.909\t Batch accuracy: 29.2%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 5980/27699\t Batch loss: 4.159\t Batch perplexity: 64.024\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 6000/27699\t Batch loss: 4.257\t Batch perplexity: 70.631\t Batch accuracy: 29.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.032\t Batch perplexity: 56.384\t Batch accuracy: 31.6%\t***\n",
      "Epoch: 2/2\t Train step: 6020/27699\t Batch loss: 4.023\t Batch perplexity: 55.862\t Batch accuracy: 30.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 6040/27699\t Batch loss: 4.169\t Batch perplexity: 64.677\t Batch accuracy: 28.3%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 6060/27699\t Batch loss: 3.976\t Batch perplexity: 53.318\t Batch accuracy: 29.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 6080/27699\t Batch loss: 4.284\t Batch perplexity: 72.551\t Batch accuracy: 26.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 6100/27699\t Batch loss: 4.320\t Batch perplexity: 75.165\t Batch accuracy: 25.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 6120/27699\t Batch loss: 4.054\t Batch perplexity: 57.609\t Batch accuracy: 32.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 6140/27699\t Batch loss: 4.092\t Batch perplexity: 59.843\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 6160/27699\t Batch loss: 4.142\t Batch perplexity: 62.954\t Batch accuracy: 26.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 6180/27699\t Batch loss: 4.232\t Batch perplexity: 68.823\t Batch accuracy: 30.5%\t 0.73s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2\t Train step: 6200/27699\t Batch loss: 3.967\t Batch perplexity: 52.813\t Batch accuracy: 30.4%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 6220/27699\t Batch loss: 4.147\t Batch perplexity: 63.257\t Batch accuracy: 27.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 6240/27699\t Batch loss: 4.115\t Batch perplexity: 61.259\t Batch accuracy: 28.2%\t 0.67s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.955\t Batch perplexity: 52.185\t Batch accuracy: 31.1%\t***\n",
      "Epoch: 2/2\t Train step: 6260/27699\t Batch loss: 4.179\t Batch perplexity: 65.305\t Batch accuracy: 29.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 6280/27699\t Batch loss: 4.417\t Batch perplexity: 82.807\t Batch accuracy: 24.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 6300/27699\t Batch loss: 4.110\t Batch perplexity: 60.960\t Batch accuracy: 28.4%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 6320/27699\t Batch loss: 4.116\t Batch perplexity: 61.322\t Batch accuracy: 28.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 6340/27699\t Batch loss: 4.125\t Batch perplexity: 61.845\t Batch accuracy: 29.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 6360/27699\t Batch loss: 4.339\t Batch perplexity: 76.643\t Batch accuracy: 24.0%\t 0.78s/batch\n",
      "Epoch: 2/2\t Train step: 6380/27699\t Batch loss: 4.278\t Batch perplexity: 72.069\t Batch accuracy: 25.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 6400/27699\t Batch loss: 4.176\t Batch perplexity: 65.095\t Batch accuracy: 27.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 6420/27699\t Batch loss: 4.096\t Batch perplexity: 60.100\t Batch accuracy: 27.0%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 6440/27699\t Batch loss: 3.997\t Batch perplexity: 54.455\t Batch accuracy: 30.8%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 6460/27699\t Batch loss: 4.277\t Batch perplexity: 72.056\t Batch accuracy: 29.8%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 6480/27699\t Batch loss: 4.276\t Batch perplexity: 71.969\t Batch accuracy: 26.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 6500/27699\t Batch loss: 4.153\t Batch perplexity: 63.617\t Batch accuracy: 29.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.955\t Batch perplexity: 52.194\t Batch accuracy: 29.5%\t***\n",
      "Epoch: 2/2\t Train step: 6520/27699\t Batch loss: 4.192\t Batch perplexity: 66.172\t Batch accuracy: 28.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 6540/27699\t Batch loss: 4.120\t Batch perplexity: 61.582\t Batch accuracy: 28.8%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 6560/27699\t Batch loss: 4.022\t Batch perplexity: 55.799\t Batch accuracy: 28.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 6580/27699\t Batch loss: 3.946\t Batch perplexity: 51.741\t Batch accuracy: 30.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 6600/27699\t Batch loss: 3.891\t Batch perplexity: 48.968\t Batch accuracy: 31.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 6620/27699\t Batch loss: 4.410\t Batch perplexity: 82.301\t Batch accuracy: 26.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 6640/27699\t Batch loss: 4.064\t Batch perplexity: 58.184\t Batch accuracy: 29.2%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 6660/27699\t Batch loss: 4.246\t Batch perplexity: 69.805\t Batch accuracy: 27.3%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 6680/27699\t Batch loss: 4.145\t Batch perplexity: 63.089\t Batch accuracy: 27.9%\t 0.77s/batch\n",
      "Epoch: 2/2\t Train step: 6700/27699\t Batch loss: 4.106\t Batch perplexity: 60.697\t Batch accuracy: 29.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 6720/27699\t Batch loss: 4.013\t Batch perplexity: 55.310\t Batch accuracy: 28.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 6740/27699\t Batch loss: 4.232\t Batch perplexity: 68.871\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.269\t Batch perplexity: 71.479\t Batch accuracy: 28.2%\t***\n",
      "Epoch: 2/2\t Train step: 6760/27699\t Batch loss: 4.280\t Batch perplexity: 72.242\t Batch accuracy: 25.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 6780/27699\t Batch loss: 4.242\t Batch perplexity: 69.525\t Batch accuracy: 29.2%\t 0.66s/batch\n",
      "Epoch: 2/2\t Train step: 6800/27699\t Batch loss: 4.045\t Batch perplexity: 57.088\t Batch accuracy: 29.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 6820/27699\t Batch loss: 4.152\t Batch perplexity: 63.564\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 6840/27699\t Batch loss: 4.105\t Batch perplexity: 60.616\t Batch accuracy: 27.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 6860/27699\t Batch loss: 3.979\t Batch perplexity: 53.447\t Batch accuracy: 30.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 6880/27699\t Batch loss: 4.110\t Batch perplexity: 60.941\t Batch accuracy: 31.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 6900/27699\t Batch loss: 4.367\t Batch perplexity: 78.833\t Batch accuracy: 27.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 6920/27699\t Batch loss: 4.249\t Batch perplexity: 70.053\t Batch accuracy: 26.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 6940/27699\t Batch loss: 3.840\t Batch perplexity: 46.543\t Batch accuracy: 30.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 6960/27699\t Batch loss: 4.062\t Batch perplexity: 58.113\t Batch accuracy: 29.8%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 6980/27699\t Batch loss: 4.203\t Batch perplexity: 66.878\t Batch accuracy: 26.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 7000/27699\t Batch loss: 4.103\t Batch perplexity: 60.492\t Batch accuracy: 29.3%\t 0.72s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.857\t Batch perplexity: 47.309\t Batch accuracy: 29.5%\t***\n",
      "Epoch: 2/2\t Train step: 7020/27699\t Batch loss: 4.237\t Batch perplexity: 69.198\t Batch accuracy: 25.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 7040/27699\t Batch loss: 4.427\t Batch perplexity: 83.718\t Batch accuracy: 26.9%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 7060/27699\t Batch loss: 3.993\t Batch perplexity: 54.241\t Batch accuracy: 28.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 7080/27699\t Batch loss: 4.258\t Batch perplexity: 70.644\t Batch accuracy: 30.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 7100/27699\t Batch loss: 4.185\t Batch perplexity: 65.676\t Batch accuracy: 27.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 7120/27699\t Batch loss: 4.130\t Batch perplexity: 62.175\t Batch accuracy: 27.8%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 7140/27699\t Batch loss: 4.260\t Batch perplexity: 70.782\t Batch accuracy: 26.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 7160/27699\t Batch loss: 4.316\t Batch perplexity: 74.882\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 7180/27699\t Batch loss: 4.164\t Batch perplexity: 64.323\t Batch accuracy: 26.8%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 7200/27699\t Batch loss: 3.957\t Batch perplexity: 52.320\t Batch accuracy: 31.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 7220/27699\t Batch loss: 4.281\t Batch perplexity: 72.296\t Batch accuracy: 25.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 7240/27699\t Batch loss: 4.407\t Batch perplexity: 82.061\t Batch accuracy: 26.3%\t 0.66s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.761\t Batch perplexity: 42.973\t Batch accuracy: 33.5%\t***\n",
      "Epoch: 2/2\t Train step: 7260/27699\t Batch loss: 4.225\t Batch perplexity: 68.380\t Batch accuracy: 28.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 7280/27699\t Batch loss: 4.280\t Batch perplexity: 72.242\t Batch accuracy: 26.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 7300/27699\t Batch loss: 4.111\t Batch perplexity: 61.019\t Batch accuracy: 29.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 7320/27699\t Batch loss: 3.998\t Batch perplexity: 54.513\t Batch accuracy: 29.2%\t 0.78s/batch\n",
      "Epoch: 2/2\t Train step: 7340/27699\t Batch loss: 4.139\t Batch perplexity: 62.711\t Batch accuracy: 27.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 7360/27699\t Batch loss: 4.287\t Batch perplexity: 72.730\t Batch accuracy: 26.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 7380/27699\t Batch loss: 4.000\t Batch perplexity: 54.618\t Batch accuracy: 28.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 7400/27699\t Batch loss: 4.140\t Batch perplexity: 62.825\t Batch accuracy: 24.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 7420/27699\t Batch loss: 4.046\t Batch perplexity: 57.168\t Batch accuracy: 25.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 7440/27699\t Batch loss: 4.190\t Batch perplexity: 65.995\t Batch accuracy: 27.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 7460/27699\t Batch loss: 3.981\t Batch perplexity: 53.576\t Batch accuracy: 29.3%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 7480/27699\t Batch loss: 4.083\t Batch perplexity: 59.294\t Batch accuracy: 29.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 7500/27699\t Batch loss: 3.982\t Batch perplexity: 53.600\t Batch accuracy: 29.5%\t 0.68s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.892\t Batch perplexity: 49.022\t Batch accuracy: 30.4%\t***\n",
      "Epoch: 2/2\t Train step: 7520/27699\t Batch loss: 4.252\t Batch perplexity: 70.228\t Batch accuracy: 27.6%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 7540/27699\t Batch loss: 4.252\t Batch perplexity: 70.226\t Batch accuracy: 26.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 7560/27699\t Batch loss: 4.077\t Batch perplexity: 58.956\t Batch accuracy: 27.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 7580/27699\t Batch loss: 4.180\t Batch perplexity: 65.376\t Batch accuracy: 27.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 7600/27699\t Batch loss: 4.129\t Batch perplexity: 62.090\t Batch accuracy: 30.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 7620/27699\t Batch loss: 4.230\t Batch perplexity: 68.734\t Batch accuracy: 28.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 7640/27699\t Batch loss: 4.123\t Batch perplexity: 61.761\t Batch accuracy: 28.1%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 7660/27699\t Batch loss: 4.136\t Batch perplexity: 62.558\t Batch accuracy: 26.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 7680/27699\t Batch loss: 4.053\t Batch perplexity: 57.593\t Batch accuracy: 29.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 7700/27699\t Batch loss: 4.267\t Batch perplexity: 71.329\t Batch accuracy: 27.1%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 7720/27699\t Batch loss: 4.183\t Batch perplexity: 65.570\t Batch accuracy: 28.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 7740/27699\t Batch loss: 4.154\t Batch perplexity: 63.696\t Batch accuracy: 27.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.037\t Batch perplexity: 56.632\t Batch accuracy: 28.1%\t***\n",
      "Epoch: 2/2\t Train step: 7760/27699\t Batch loss: 4.000\t Batch perplexity: 54.608\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 7780/27699\t Batch loss: 4.154\t Batch perplexity: 63.712\t Batch accuracy: 28.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 7800/27699\t Batch loss: 4.297\t Batch perplexity: 73.443\t Batch accuracy: 24.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 7820/27699\t Batch loss: 4.296\t Batch perplexity: 73.382\t Batch accuracy: 25.6%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 7840/27699\t Batch loss: 4.076\t Batch perplexity: 58.905\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 7860/27699\t Batch loss: 4.306\t Batch perplexity: 74.138\t Batch accuracy: 27.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 7880/27699\t Batch loss: 4.212\t Batch perplexity: 67.508\t Batch accuracy: 26.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 7900/27699\t Batch loss: 4.189\t Batch perplexity: 65.960\t Batch accuracy: 26.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 7920/27699\t Batch loss: 4.185\t Batch perplexity: 65.712\t Batch accuracy: 27.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 7940/27699\t Batch loss: 4.186\t Batch perplexity: 65.727\t Batch accuracy: 27.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 7960/27699\t Batch loss: 4.019\t Batch perplexity: 55.641\t Batch accuracy: 32.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 7980/27699\t Batch loss: 4.044\t Batch perplexity: 57.069\t Batch accuracy: 28.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 8000/27699\t Batch loss: 3.995\t Batch perplexity: 54.331\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.086\t Batch perplexity: 59.505\t Batch accuracy: 31.2%\t***\n",
      "Epoch: 2/2\t Train step: 8020/27699\t Batch loss: 4.058\t Batch perplexity: 57.886\t Batch accuracy: 30.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 8040/27699\t Batch loss: 4.141\t Batch perplexity: 62.839\t Batch accuracy: 26.2%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 8060/27699\t Batch loss: 4.152\t Batch perplexity: 63.592\t Batch accuracy: 28.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 8080/27699\t Batch loss: 4.303\t Batch perplexity: 73.899\t Batch accuracy: 27.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 8100/27699\t Batch loss: 4.110\t Batch perplexity: 60.929\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 8120/27699\t Batch loss: 4.237\t Batch perplexity: 69.227\t Batch accuracy: 26.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 8140/27699\t Batch loss: 4.143\t Batch perplexity: 63.019\t Batch accuracy: 28.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 8160/27699\t Batch loss: 4.190\t Batch perplexity: 66.018\t Batch accuracy: 28.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 8180/27699\t Batch loss: 4.137\t Batch perplexity: 62.634\t Batch accuracy: 28.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 8200/27699\t Batch loss: 4.164\t Batch perplexity: 64.327\t Batch accuracy: 28.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 8220/27699\t Batch loss: 4.032\t Batch perplexity: 56.357\t Batch accuracy: 28.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 8240/27699\t Batch loss: 4.114\t Batch perplexity: 61.166\t Batch accuracy: 28.5%\t 0.70s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.073\t Batch perplexity: 58.720\t Batch accuracy: 28.9%\t***\n",
      "Epoch: 2/2\t Train step: 8260/27699\t Batch loss: 4.089\t Batch perplexity: 59.710\t Batch accuracy: 30.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 8280/27699\t Batch loss: 4.316\t Batch perplexity: 74.863\t Batch accuracy: 26.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 8300/27699\t Batch loss: 4.223\t Batch perplexity: 68.257\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 8320/27699\t Batch loss: 4.210\t Batch perplexity: 67.349\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 8340/27699\t Batch loss: 4.329\t Batch perplexity: 75.838\t Batch accuracy: 26.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 8360/27699\t Batch loss: 3.926\t Batch perplexity: 50.722\t Batch accuracy: 28.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 8380/27699\t Batch loss: 4.108\t Batch perplexity: 60.810\t Batch accuracy: 28.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 8400/27699\t Batch loss: 4.200\t Batch perplexity: 66.683\t Batch accuracy: 26.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 8420/27699\t Batch loss: 4.157\t Batch perplexity: 63.907\t Batch accuracy: 29.5%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 8440/27699\t Batch loss: 4.158\t Batch perplexity: 63.914\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 8460/27699\t Batch loss: 4.213\t Batch perplexity: 67.583\t Batch accuracy: 29.1%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 8480/27699\t Batch loss: 4.184\t Batch perplexity: 65.629\t Batch accuracy: 27.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 8500/27699\t Batch loss: 4.426\t Batch perplexity: 83.574\t Batch accuracy: 28.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.091\t Batch perplexity: 59.784\t Batch accuracy: 28.5%\t***\n",
      "Epoch: 2/2\t Train step: 8520/27699\t Batch loss: 4.125\t Batch perplexity: 61.842\t Batch accuracy: 28.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 8540/27699\t Batch loss: 3.986\t Batch perplexity: 53.845\t Batch accuracy: 30.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 8560/27699\t Batch loss: 4.114\t Batch perplexity: 61.216\t Batch accuracy: 28.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 8580/27699\t Batch loss: 4.206\t Batch perplexity: 67.066\t Batch accuracy: 28.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 8600/27699\t Batch loss: 4.179\t Batch perplexity: 65.296\t Batch accuracy: 27.9%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 8620/27699\t Batch loss: 4.218\t Batch perplexity: 67.894\t Batch accuracy: 27.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 8640/27699\t Batch loss: 4.247\t Batch perplexity: 69.909\t Batch accuracy: 27.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 8660/27699\t Batch loss: 4.233\t Batch perplexity: 68.947\t Batch accuracy: 28.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 8680/27699\t Batch loss: 4.390\t Batch perplexity: 80.678\t Batch accuracy: 25.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 8700/27699\t Batch loss: 4.223\t Batch perplexity: 68.211\t Batch accuracy: 27.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 8720/27699\t Batch loss: 4.047\t Batch perplexity: 57.227\t Batch accuracy: 29.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 8740/27699\t Batch loss: 4.167\t Batch perplexity: 64.496\t Batch accuracy: 27.3%\t 0.73s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.121\t Batch perplexity: 61.610\t Batch accuracy: 28.5%\t***\n",
      "Epoch: 2/2\t Train step: 8760/27699\t Batch loss: 4.242\t Batch perplexity: 69.530\t Batch accuracy: 29.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 8780/27699\t Batch loss: 4.151\t Batch perplexity: 63.484\t Batch accuracy: 28.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 8800/27699\t Batch loss: 3.858\t Batch perplexity: 47.356\t Batch accuracy: 30.1%\t 0.71s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2\t Train step: 8820/27699\t Batch loss: 4.022\t Batch perplexity: 55.794\t Batch accuracy: 28.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 8840/27699\t Batch loss: 4.336\t Batch perplexity: 76.435\t Batch accuracy: 25.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 8860/27699\t Batch loss: 4.239\t Batch perplexity: 69.310\t Batch accuracy: 27.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 8880/27699\t Batch loss: 4.205\t Batch perplexity: 67.011\t Batch accuracy: 27.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 8900/27699\t Batch loss: 4.092\t Batch perplexity: 59.887\t Batch accuracy: 26.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 8920/27699\t Batch loss: 4.140\t Batch perplexity: 62.813\t Batch accuracy: 27.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 8940/27699\t Batch loss: 4.083\t Batch perplexity: 59.318\t Batch accuracy: 28.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 8960/27699\t Batch loss: 4.174\t Batch perplexity: 64.971\t Batch accuracy: 29.2%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 8980/27699\t Batch loss: 3.996\t Batch perplexity: 54.373\t Batch accuracy: 27.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 9000/27699\t Batch loss: 4.352\t Batch perplexity: 77.604\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.878\t Batch perplexity: 48.330\t Batch accuracy: 31.3%\t***\n",
      "Epoch: 2/2\t Train step: 9020/27699\t Batch loss: 4.246\t Batch perplexity: 69.822\t Batch accuracy: 26.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 9040/27699\t Batch loss: 4.162\t Batch perplexity: 64.189\t Batch accuracy: 28.5%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 9060/27699\t Batch loss: 4.290\t Batch perplexity: 72.937\t Batch accuracy: 27.6%\t 0.74s/batch\n",
      "Epoch: 2/2\t Train step: 9080/27699\t Batch loss: 4.133\t Batch perplexity: 62.349\t Batch accuracy: 27.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 9100/27699\t Batch loss: 4.185\t Batch perplexity: 65.672\t Batch accuracy: 29.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 9120/27699\t Batch loss: 4.177\t Batch perplexity: 65.158\t Batch accuracy: 26.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 9140/27699\t Batch loss: 4.227\t Batch perplexity: 68.540\t Batch accuracy: 28.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 9160/27699\t Batch loss: 4.160\t Batch perplexity: 64.080\t Batch accuracy: 26.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 9180/27699\t Batch loss: 4.104\t Batch perplexity: 60.569\t Batch accuracy: 29.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 9200/27699\t Batch loss: 4.132\t Batch perplexity: 62.315\t Batch accuracy: 29.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 9220/27699\t Batch loss: 4.262\t Batch perplexity: 70.978\t Batch accuracy: 26.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 9240/27699\t Batch loss: 4.136\t Batch perplexity: 62.524\t Batch accuracy: 29.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.991\t Batch perplexity: 54.101\t Batch accuracy: 29.4%\t***\n",
      "Epoch: 2/2\t Train step: 9260/27699\t Batch loss: 4.087\t Batch perplexity: 59.547\t Batch accuracy: 27.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 9280/27699\t Batch loss: 4.209\t Batch perplexity: 67.279\t Batch accuracy: 26.4%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 9300/27699\t Batch loss: 4.230\t Batch perplexity: 68.735\t Batch accuracy: 27.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 9320/27699\t Batch loss: 4.412\t Batch perplexity: 82.459\t Batch accuracy: 27.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 9340/27699\t Batch loss: 4.181\t Batch perplexity: 65.428\t Batch accuracy: 28.2%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 9360/27699\t Batch loss: 4.069\t Batch perplexity: 58.496\t Batch accuracy: 29.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 9380/27699\t Batch loss: 4.198\t Batch perplexity: 66.576\t Batch accuracy: 26.8%\t 0.76s/batch\n",
      "Epoch: 2/2\t Train step: 9400/27699\t Batch loss: 4.165\t Batch perplexity: 64.385\t Batch accuracy: 30.2%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 9420/27699\t Batch loss: 4.115\t Batch perplexity: 61.272\t Batch accuracy: 29.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 9440/27699\t Batch loss: 4.241\t Batch perplexity: 69.505\t Batch accuracy: 27.6%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 9460/27699\t Batch loss: 4.497\t Batch perplexity: 89.758\t Batch accuracy: 26.1%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 9480/27699\t Batch loss: 4.194\t Batch perplexity: 66.316\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 9500/27699\t Batch loss: 4.146\t Batch perplexity: 63.204\t Batch accuracy: 28.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.774\t Batch perplexity: 43.571\t Batch accuracy: 30.9%\t***\n",
      "Epoch: 2/2\t Train step: 9520/27699\t Batch loss: 4.055\t Batch perplexity: 57.685\t Batch accuracy: 28.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 9540/27699\t Batch loss: 3.857\t Batch perplexity: 47.336\t Batch accuracy: 31.3%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 9560/27699\t Batch loss: 4.284\t Batch perplexity: 72.561\t Batch accuracy: 26.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 9580/27699\t Batch loss: 4.055\t Batch perplexity: 57.676\t Batch accuracy: 28.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 9600/27699\t Batch loss: 4.134\t Batch perplexity: 62.434\t Batch accuracy: 25.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 9620/27699\t Batch loss: 4.042\t Batch perplexity: 56.953\t Batch accuracy: 29.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 9640/27699\t Batch loss: 4.393\t Batch perplexity: 80.872\t Batch accuracy: 26.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 9660/27699\t Batch loss: 4.098\t Batch perplexity: 60.239\t Batch accuracy: 26.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 9680/27699\t Batch loss: 4.217\t Batch perplexity: 67.829\t Batch accuracy: 28.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 9700/27699\t Batch loss: 4.289\t Batch perplexity: 72.928\t Batch accuracy: 26.7%\t 0.75s/batch\n",
      "Epoch: 2/2\t Train step: 9720/27699\t Batch loss: 4.448\t Batch perplexity: 85.442\t Batch accuracy: 26.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 9740/27699\t Batch loss: 4.215\t Batch perplexity: 67.671\t Batch accuracy: 29.5%\t 0.70s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.007\t Batch perplexity: 54.989\t Batch accuracy: 28.8%\t***\n",
      "Epoch: 2/2\t Train step: 9760/27699\t Batch loss: 4.176\t Batch perplexity: 65.076\t Batch accuracy: 28.0%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 9780/27699\t Batch loss: 4.181\t Batch perplexity: 65.458\t Batch accuracy: 28.2%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 9800/27699\t Batch loss: 4.042\t Batch perplexity: 56.930\t Batch accuracy: 27.5%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 9820/27699\t Batch loss: 4.118\t Batch perplexity: 61.427\t Batch accuracy: 26.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 9840/27699\t Batch loss: 4.010\t Batch perplexity: 55.139\t Batch accuracy: 29.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 9860/27699\t Batch loss: 4.008\t Batch perplexity: 55.051\t Batch accuracy: 29.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 9880/27699\t Batch loss: 4.244\t Batch perplexity: 69.666\t Batch accuracy: 29.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 9900/27699\t Batch loss: 4.026\t Batch perplexity: 56.015\t Batch accuracy: 28.3%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 9920/27699\t Batch loss: 4.078\t Batch perplexity: 59.011\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 9940/27699\t Batch loss: 4.287\t Batch perplexity: 72.753\t Batch accuracy: 26.4%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 9960/27699\t Batch loss: 4.114\t Batch perplexity: 61.219\t Batch accuracy: 29.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 9980/27699\t Batch loss: 3.934\t Batch perplexity: 51.105\t Batch accuracy: 31.6%\t 0.73s/batch\n",
      "Epoch: 2/2\t Train step: 10000/27699\t Batch loss: 3.989\t Batch perplexity: 53.981\t Batch accuracy: 27.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.017\t Batch perplexity: 55.561\t Batch accuracy: 27.9%\t***\n",
      "Epoch: 2/2\t Train step: 10020/27699\t Batch loss: 4.136\t Batch perplexity: 62.542\t Batch accuracy: 28.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 10040/27699\t Batch loss: 4.186\t Batch perplexity: 65.730\t Batch accuracy: 28.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 10060/27699\t Batch loss: 4.258\t Batch perplexity: 70.699\t Batch accuracy: 27.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 10080/27699\t Batch loss: 4.183\t Batch perplexity: 65.536\t Batch accuracy: 26.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 10100/27699\t Batch loss: 4.021\t Batch perplexity: 55.757\t Batch accuracy: 30.6%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 10120/27699\t Batch loss: 4.085\t Batch perplexity: 59.417\t Batch accuracy: 29.3%\t 0.67s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2\t Train step: 10140/27699\t Batch loss: 3.958\t Batch perplexity: 52.368\t Batch accuracy: 31.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 10160/27699\t Batch loss: 3.938\t Batch perplexity: 51.318\t Batch accuracy: 29.7%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 10180/27699\t Batch loss: 4.159\t Batch perplexity: 63.979\t Batch accuracy: 28.4%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 10200/27699\t Batch loss: 4.212\t Batch perplexity: 67.524\t Batch accuracy: 26.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 10220/27699\t Batch loss: 4.285\t Batch perplexity: 72.601\t Batch accuracy: 28.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 10240/27699\t Batch loss: 4.149\t Batch perplexity: 63.359\t Batch accuracy: 27.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.032\t Batch perplexity: 56.346\t Batch accuracy: 29.9%\t***\n",
      "Epoch: 2/2\t Train step: 10260/27699\t Batch loss: 4.093\t Batch perplexity: 59.932\t Batch accuracy: 30.6%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 10280/27699\t Batch loss: 4.249\t Batch perplexity: 70.038\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 10300/27699\t Batch loss: 4.135\t Batch perplexity: 62.498\t Batch accuracy: 28.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 10320/27699\t Batch loss: 3.917\t Batch perplexity: 50.271\t Batch accuracy: 30.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 10340/27699\t Batch loss: 4.294\t Batch perplexity: 73.245\t Batch accuracy: 25.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 10360/27699\t Batch loss: 4.032\t Batch perplexity: 56.379\t Batch accuracy: 29.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 10380/27699\t Batch loss: 3.977\t Batch perplexity: 53.331\t Batch accuracy: 28.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 10400/27699\t Batch loss: 4.113\t Batch perplexity: 61.131\t Batch accuracy: 31.2%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 10420/27699\t Batch loss: 4.399\t Batch perplexity: 81.391\t Batch accuracy: 25.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 10440/27699\t Batch loss: 3.898\t Batch perplexity: 49.316\t Batch accuracy: 29.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 10460/27699\t Batch loss: 4.191\t Batch perplexity: 66.100\t Batch accuracy: 28.3%\t 0.76s/batch\n",
      "Epoch: 2/2\t Train step: 10480/27699\t Batch loss: 4.052\t Batch perplexity: 57.519\t Batch accuracy: 28.9%\t 0.79s/batch\n",
      "Epoch: 2/2\t Train step: 10500/27699\t Batch loss: 4.375\t Batch perplexity: 79.450\t Batch accuracy: 27.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.897\t Batch perplexity: 49.241\t Batch accuracy: 30.3%\t***\n",
      "Epoch: 2/2\t Train step: 10520/27699\t Batch loss: 4.120\t Batch perplexity: 61.549\t Batch accuracy: 28.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 10540/27699\t Batch loss: 4.166\t Batch perplexity: 64.451\t Batch accuracy: 29.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 10560/27699\t Batch loss: 4.244\t Batch perplexity: 69.703\t Batch accuracy: 29.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 10580/27699\t Batch loss: 4.024\t Batch perplexity: 55.926\t Batch accuracy: 31.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 10600/27699\t Batch loss: 4.044\t Batch perplexity: 57.073\t Batch accuracy: 28.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 10620/27699\t Batch loss: 4.038\t Batch perplexity: 56.695\t Batch accuracy: 26.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 10640/27699\t Batch loss: 4.349\t Batch perplexity: 77.380\t Batch accuracy: 26.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 10660/27699\t Batch loss: 4.132\t Batch perplexity: 62.287\t Batch accuracy: 29.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 10680/27699\t Batch loss: 4.238\t Batch perplexity: 69.301\t Batch accuracy: 28.1%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 10700/27699\t Batch loss: 4.031\t Batch perplexity: 56.341\t Batch accuracy: 28.6%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 10720/27699\t Batch loss: 4.265\t Batch perplexity: 71.159\t Batch accuracy: 27.4%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 10740/27699\t Batch loss: 4.337\t Batch perplexity: 76.481\t Batch accuracy: 26.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.169\t Batch perplexity: 64.626\t Batch accuracy: 29.1%\t***\n",
      "Epoch: 2/2\t Train step: 10760/27699\t Batch loss: 4.234\t Batch perplexity: 68.983\t Batch accuracy: 27.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 10780/27699\t Batch loss: 4.076\t Batch perplexity: 58.884\t Batch accuracy: 28.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 10800/27699\t Batch loss: 4.016\t Batch perplexity: 55.501\t Batch accuracy: 27.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 10820/27699\t Batch loss: 4.136\t Batch perplexity: 62.536\t Batch accuracy: 27.8%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 10840/27699\t Batch loss: 4.233\t Batch perplexity: 68.930\t Batch accuracy: 28.0%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 10860/27699\t Batch loss: 4.203\t Batch perplexity: 66.890\t Batch accuracy: 30.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 10880/27699\t Batch loss: 4.058\t Batch perplexity: 57.862\t Batch accuracy: 31.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 10900/27699\t Batch loss: 4.122\t Batch perplexity: 61.698\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 10920/27699\t Batch loss: 4.177\t Batch perplexity: 65.159\t Batch accuracy: 26.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 10940/27699\t Batch loss: 4.095\t Batch perplexity: 60.027\t Batch accuracy: 28.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 10960/27699\t Batch loss: 4.054\t Batch perplexity: 57.642\t Batch accuracy: 27.3%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 10980/27699\t Batch loss: 4.062\t Batch perplexity: 58.117\t Batch accuracy: 28.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 11000/27699\t Batch loss: 4.261\t Batch perplexity: 70.876\t Batch accuracy: 28.1%\t 0.67s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.013\t Batch perplexity: 55.287\t Batch accuracy: 31.5%\t***\n",
      "Epoch: 2/2\t Train step: 11020/27699\t Batch loss: 4.210\t Batch perplexity: 67.324\t Batch accuracy: 29.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 11040/27699\t Batch loss: 3.887\t Batch perplexity: 48.767\t Batch accuracy: 32.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 11060/27699\t Batch loss: 4.228\t Batch perplexity: 68.583\t Batch accuracy: 25.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 11080/27699\t Batch loss: 4.257\t Batch perplexity: 70.623\t Batch accuracy: 28.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 11100/27699\t Batch loss: 4.238\t Batch perplexity: 69.238\t Batch accuracy: 27.7%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 11120/27699\t Batch loss: 4.037\t Batch perplexity: 56.641\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 11140/27699\t Batch loss: 4.117\t Batch perplexity: 61.360\t Batch accuracy: 28.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 11160/27699\t Batch loss: 4.114\t Batch perplexity: 61.187\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 11180/27699\t Batch loss: 4.080\t Batch perplexity: 59.143\t Batch accuracy: 28.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 11200/27699\t Batch loss: 4.049\t Batch perplexity: 57.341\t Batch accuracy: 28.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 11220/27699\t Batch loss: 4.000\t Batch perplexity: 54.623\t Batch accuracy: 31.2%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 11240/27699\t Batch loss: 4.205\t Batch perplexity: 67.009\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.846\t Batch perplexity: 46.825\t Batch accuracy: 30.0%\t***\n",
      "Epoch: 2/2\t Train step: 11260/27699\t Batch loss: 4.378\t Batch perplexity: 79.687\t Batch accuracy: 24.1%\t 0.74s/batch\n",
      "Epoch: 2/2\t Train step: 11280/27699\t Batch loss: 4.084\t Batch perplexity: 59.365\t Batch accuracy: 28.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 11300/27699\t Batch loss: 4.182\t Batch perplexity: 65.476\t Batch accuracy: 26.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 11320/27699\t Batch loss: 4.169\t Batch perplexity: 64.637\t Batch accuracy: 28.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 11340/27699\t Batch loss: 4.147\t Batch perplexity: 63.265\t Batch accuracy: 29.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 11360/27699\t Batch loss: 4.131\t Batch perplexity: 62.239\t Batch accuracy: 28.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 11380/27699\t Batch loss: 4.096\t Batch perplexity: 60.095\t Batch accuracy: 27.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 11400/27699\t Batch loss: 4.093\t Batch perplexity: 59.911\t Batch accuracy: 29.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 11420/27699\t Batch loss: 4.167\t Batch perplexity: 64.494\t Batch accuracy: 27.5%\t 0.69s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2\t Train step: 11440/27699\t Batch loss: 4.037\t Batch perplexity: 56.637\t Batch accuracy: 27.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 11460/27699\t Batch loss: 4.136\t Batch perplexity: 62.570\t Batch accuracy: 28.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 11480/27699\t Batch loss: 3.984\t Batch perplexity: 53.742\t Batch accuracy: 31.7%\t 0.66s/batch\n",
      "Epoch: 2/2\t Train step: 11500/27699\t Batch loss: 4.022\t Batch perplexity: 55.821\t Batch accuracy: 28.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.045\t Batch perplexity: 57.123\t Batch accuracy: 27.1%\t***\n",
      "Epoch: 2/2\t Train step: 11520/27699\t Batch loss: 4.240\t Batch perplexity: 69.431\t Batch accuracy: 26.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 11540/27699\t Batch loss: 4.019\t Batch perplexity: 55.628\t Batch accuracy: 29.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 11560/27699\t Batch loss: 4.064\t Batch perplexity: 58.226\t Batch accuracy: 28.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 11580/27699\t Batch loss: 4.072\t Batch perplexity: 58.663\t Batch accuracy: 29.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 11600/27699\t Batch loss: 4.140\t Batch perplexity: 62.782\t Batch accuracy: 27.7%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 11620/27699\t Batch loss: 3.898\t Batch perplexity: 49.324\t Batch accuracy: 29.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 11640/27699\t Batch loss: 4.237\t Batch perplexity: 69.220\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 11660/27699\t Batch loss: 4.099\t Batch perplexity: 60.253\t Batch accuracy: 29.2%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 11680/27699\t Batch loss: 4.032\t Batch perplexity: 56.378\t Batch accuracy: 26.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 11700/27699\t Batch loss: 4.142\t Batch perplexity: 62.948\t Batch accuracy: 28.1%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 11720/27699\t Batch loss: 4.161\t Batch perplexity: 64.104\t Batch accuracy: 27.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 11740/27699\t Batch loss: 4.271\t Batch perplexity: 71.584\t Batch accuracy: 28.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.947\t Batch perplexity: 51.771\t Batch accuracy: 30.2%\t***\n",
      "Epoch: 2/2\t Train step: 11760/27699\t Batch loss: 4.404\t Batch perplexity: 81.798\t Batch accuracy: 25.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 11780/27699\t Batch loss: 4.109\t Batch perplexity: 60.913\t Batch accuracy: 28.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 11800/27699\t Batch loss: 4.118\t Batch perplexity: 61.452\t Batch accuracy: 29.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 11820/27699\t Batch loss: 4.039\t Batch perplexity: 56.778\t Batch accuracy: 28.3%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 11840/27699\t Batch loss: 4.179\t Batch perplexity: 65.330\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 11860/27699\t Batch loss: 4.248\t Batch perplexity: 69.976\t Batch accuracy: 27.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 11880/27699\t Batch loss: 3.966\t Batch perplexity: 52.754\t Batch accuracy: 29.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 11900/27699\t Batch loss: 4.137\t Batch perplexity: 62.584\t Batch accuracy: 27.6%\t 0.66s/batch\n",
      "Epoch: 2/2\t Train step: 11920/27699\t Batch loss: 4.162\t Batch perplexity: 64.169\t Batch accuracy: 26.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 11940/27699\t Batch loss: 4.205\t Batch perplexity: 67.005\t Batch accuracy: 27.1%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 11960/27699\t Batch loss: 4.023\t Batch perplexity: 55.888\t Batch accuracy: 28.6%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 11980/27699\t Batch loss: 3.894\t Batch perplexity: 49.102\t Batch accuracy: 29.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 12000/27699\t Batch loss: 4.130\t Batch perplexity: 62.182\t Batch accuracy: 28.6%\t 0.66s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.909\t Batch perplexity: 49.840\t Batch accuracy: 32.0%\t***\n",
      "Epoch: 2/2\t Train step: 12020/27699\t Batch loss: 3.955\t Batch perplexity: 52.188\t Batch accuracy: 30.7%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 12040/27699\t Batch loss: 4.304\t Batch perplexity: 74.015\t Batch accuracy: 28.3%\t 0.73s/batch\n",
      "Epoch: 2/2\t Train step: 12060/27699\t Batch loss: 4.028\t Batch perplexity: 56.152\t Batch accuracy: 26.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 12080/27699\t Batch loss: 4.149\t Batch perplexity: 63.380\t Batch accuracy: 27.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 12100/27699\t Batch loss: 3.960\t Batch perplexity: 52.470\t Batch accuracy: 29.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 12120/27699\t Batch loss: 4.310\t Batch perplexity: 74.429\t Batch accuracy: 26.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 12140/27699\t Batch loss: 3.851\t Batch perplexity: 47.050\t Batch accuracy: 31.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 12160/27699\t Batch loss: 4.123\t Batch perplexity: 61.751\t Batch accuracy: 28.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 12180/27699\t Batch loss: 4.213\t Batch perplexity: 67.539\t Batch accuracy: 27.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 12200/27699\t Batch loss: 4.182\t Batch perplexity: 65.477\t Batch accuracy: 28.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 12220/27699\t Batch loss: 4.278\t Batch perplexity: 72.079\t Batch accuracy: 27.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 12240/27699\t Batch loss: 4.081\t Batch perplexity: 59.207\t Batch accuracy: 28.3%\t 0.67s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.973\t Batch perplexity: 53.123\t Batch accuracy: 29.2%\t***\n",
      "Epoch: 2/2\t Train step: 12260/27699\t Batch loss: 4.044\t Batch perplexity: 57.057\t Batch accuracy: 30.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 12280/27699\t Batch loss: 4.190\t Batch perplexity: 66.001\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 12300/27699\t Batch loss: 4.216\t Batch perplexity: 67.748\t Batch accuracy: 26.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 12320/27699\t Batch loss: 4.269\t Batch perplexity: 71.484\t Batch accuracy: 26.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 12340/27699\t Batch loss: 4.208\t Batch perplexity: 67.230\t Batch accuracy: 29.5%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 12360/27699\t Batch loss: 4.173\t Batch perplexity: 64.939\t Batch accuracy: 28.1%\t 0.78s/batch\n",
      "Epoch: 2/2\t Train step: 12380/27699\t Batch loss: 3.969\t Batch perplexity: 52.954\t Batch accuracy: 28.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 12400/27699\t Batch loss: 4.453\t Batch perplexity: 85.904\t Batch accuracy: 26.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 12420/27699\t Batch loss: 4.032\t Batch perplexity: 56.372\t Batch accuracy: 29.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 12440/27699\t Batch loss: 3.819\t Batch perplexity: 45.558\t Batch accuracy: 31.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 12460/27699\t Batch loss: 4.218\t Batch perplexity: 67.928\t Batch accuracy: 26.8%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 12480/27699\t Batch loss: 3.959\t Batch perplexity: 52.403\t Batch accuracy: 28.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 12500/27699\t Batch loss: 4.253\t Batch perplexity: 70.319\t Batch accuracy: 29.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.914\t Batch perplexity: 50.086\t Batch accuracy: 29.7%\t***\n",
      "Epoch: 2/2\t Train step: 12520/27699\t Batch loss: 4.326\t Batch perplexity: 75.659\t Batch accuracy: 27.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 12540/27699\t Batch loss: 4.168\t Batch perplexity: 64.558\t Batch accuracy: 30.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 12560/27699\t Batch loss: 3.871\t Batch perplexity: 47.969\t Batch accuracy: 29.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 12580/27699\t Batch loss: 4.085\t Batch perplexity: 59.426\t Batch accuracy: 28.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 12600/27699\t Batch loss: 4.172\t Batch perplexity: 64.837\t Batch accuracy: 27.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 12620/27699\t Batch loss: 4.250\t Batch perplexity: 70.125\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 12640/27699\t Batch loss: 4.188\t Batch perplexity: 65.883\t Batch accuracy: 30.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 12660/27699\t Batch loss: 4.068\t Batch perplexity: 58.422\t Batch accuracy: 28.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 12680/27699\t Batch loss: 4.190\t Batch perplexity: 66.043\t Batch accuracy: 27.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 12700/27699\t Batch loss: 4.195\t Batch perplexity: 66.336\t Batch accuracy: 28.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 12720/27699\t Batch loss: 3.935\t Batch perplexity: 51.150\t Batch accuracy: 29.9%\t 0.68s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2\t Train step: 12740/27699\t Batch loss: 4.005\t Batch perplexity: 54.889\t Batch accuracy: 27.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.918\t Batch perplexity: 50.286\t Batch accuracy: 31.9%\t***\n",
      "Epoch: 2/2\t Train step: 12760/27699\t Batch loss: 4.068\t Batch perplexity: 58.465\t Batch accuracy: 29.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 12780/27699\t Batch loss: 4.093\t Batch perplexity: 59.908\t Batch accuracy: 29.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 12800/27699\t Batch loss: 4.127\t Batch perplexity: 62.022\t Batch accuracy: 29.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 12820/27699\t Batch loss: 4.084\t Batch perplexity: 59.390\t Batch accuracy: 27.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 12840/27699\t Batch loss: 4.247\t Batch perplexity: 69.910\t Batch accuracy: 28.1%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 12860/27699\t Batch loss: 4.062\t Batch perplexity: 58.094\t Batch accuracy: 28.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 12880/27699\t Batch loss: 4.075\t Batch perplexity: 58.872\t Batch accuracy: 28.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 12900/27699\t Batch loss: 4.095\t Batch perplexity: 60.057\t Batch accuracy: 26.8%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 12920/27699\t Batch loss: 4.007\t Batch perplexity: 54.987\t Batch accuracy: 30.8%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 12940/27699\t Batch loss: 3.951\t Batch perplexity: 52.000\t Batch accuracy: 28.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 12960/27699\t Batch loss: 4.072\t Batch perplexity: 58.651\t Batch accuracy: 28.9%\t 0.74s/batch\n",
      "Epoch: 2/2\t Train step: 12980/27699\t Batch loss: 4.221\t Batch perplexity: 68.129\t Batch accuracy: 28.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 13000/27699\t Batch loss: 4.010\t Batch perplexity: 55.160\t Batch accuracy: 29.5%\t 0.77s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.983\t Batch perplexity: 53.674\t Batch accuracy: 31.4%\t***\n",
      "Epoch: 2/2\t Train step: 13020/27699\t Batch loss: 4.376\t Batch perplexity: 79.514\t Batch accuracy: 26.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 13040/27699\t Batch loss: 4.331\t Batch perplexity: 75.986\t Batch accuracy: 25.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 13060/27699\t Batch loss: 3.968\t Batch perplexity: 52.854\t Batch accuracy: 29.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 13080/27699\t Batch loss: 4.185\t Batch perplexity: 65.699\t Batch accuracy: 28.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 13100/27699\t Batch loss: 4.220\t Batch perplexity: 68.013\t Batch accuracy: 27.6%\t 0.66s/batch\n",
      "Epoch: 2/2\t Train step: 13120/27699\t Batch loss: 4.075\t Batch perplexity: 58.841\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 13140/27699\t Batch loss: 3.978\t Batch perplexity: 53.399\t Batch accuracy: 30.2%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 13160/27699\t Batch loss: 4.070\t Batch perplexity: 58.529\t Batch accuracy: 27.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 13180/27699\t Batch loss: 4.105\t Batch perplexity: 60.640\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 13200/27699\t Batch loss: 4.018\t Batch perplexity: 55.603\t Batch accuracy: 29.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 13220/27699\t Batch loss: 4.144\t Batch perplexity: 63.032\t Batch accuracy: 27.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 13240/27699\t Batch loss: 4.355\t Batch perplexity: 77.872\t Batch accuracy: 25.2%\t 0.67s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.081\t Batch perplexity: 59.212\t Batch accuracy: 29.0%\t***\n",
      "Epoch: 2/2\t Train step: 13260/27699\t Batch loss: 4.106\t Batch perplexity: 60.711\t Batch accuracy: 28.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 13280/27699\t Batch loss: 3.949\t Batch perplexity: 51.909\t Batch accuracy: 29.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 13300/27699\t Batch loss: 4.124\t Batch perplexity: 61.785\t Batch accuracy: 29.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 13320/27699\t Batch loss: 4.090\t Batch perplexity: 59.717\t Batch accuracy: 28.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 13340/27699\t Batch loss: 4.329\t Batch perplexity: 75.846\t Batch accuracy: 26.1%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 13360/27699\t Batch loss: 4.119\t Batch perplexity: 61.502\t Batch accuracy: 26.1%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 13380/27699\t Batch loss: 4.321\t Batch perplexity: 75.255\t Batch accuracy: 25.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 13400/27699\t Batch loss: 4.361\t Batch perplexity: 78.331\t Batch accuracy: 28.1%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 13420/27699\t Batch loss: 4.211\t Batch perplexity: 67.396\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 13440/27699\t Batch loss: 4.045\t Batch perplexity: 57.104\t Batch accuracy: 29.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 13460/27699\t Batch loss: 4.124\t Batch perplexity: 61.793\t Batch accuracy: 29.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 13480/27699\t Batch loss: 4.105\t Batch perplexity: 60.635\t Batch accuracy: 30.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 13500/27699\t Batch loss: 4.062\t Batch perplexity: 58.071\t Batch accuracy: 29.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.001\t Batch perplexity: 54.660\t Batch accuracy: 31.8%\t***\n",
      "Epoch: 2/2\t Train step: 13520/27699\t Batch loss: 4.266\t Batch perplexity: 71.237\t Batch accuracy: 27.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 13540/27699\t Batch loss: 4.152\t Batch perplexity: 63.555\t Batch accuracy: 28.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 13560/27699\t Batch loss: 4.101\t Batch perplexity: 60.373\t Batch accuracy: 28.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 13580/27699\t Batch loss: 4.219\t Batch perplexity: 67.950\t Batch accuracy: 30.1%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 13600/27699\t Batch loss: 4.113\t Batch perplexity: 61.130\t Batch accuracy: 28.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 13620/27699\t Batch loss: 4.215\t Batch perplexity: 67.696\t Batch accuracy: 27.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 13640/27699\t Batch loss: 4.268\t Batch perplexity: 71.392\t Batch accuracy: 28.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 13660/27699\t Batch loss: 4.040\t Batch perplexity: 56.848\t Batch accuracy: 29.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 13680/27699\t Batch loss: 3.929\t Batch perplexity: 50.874\t Batch accuracy: 27.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 13700/27699\t Batch loss: 4.137\t Batch perplexity: 62.618\t Batch accuracy: 28.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 13720/27699\t Batch loss: 4.140\t Batch perplexity: 62.781\t Batch accuracy: 29.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 13740/27699\t Batch loss: 4.013\t Batch perplexity: 55.296\t Batch accuracy: 29.1%\t 0.67s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.103\t Batch perplexity: 60.524\t Batch accuracy: 29.2%\t***\n",
      "Epoch: 2/2\t Train step: 13760/27699\t Batch loss: 4.024\t Batch perplexity: 55.908\t Batch accuracy: 29.3%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 13780/27699\t Batch loss: 4.183\t Batch perplexity: 65.585\t Batch accuracy: 26.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 13800/27699\t Batch loss: 4.137\t Batch perplexity: 62.611\t Batch accuracy: 27.3%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 13820/27699\t Batch loss: 4.000\t Batch perplexity: 54.575\t Batch accuracy: 30.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 13840/27699\t Batch loss: 4.062\t Batch perplexity: 58.077\t Batch accuracy: 27.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 13860/27699\t Batch loss: 4.149\t Batch perplexity: 63.354\t Batch accuracy: 26.5%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 13880/27699\t Batch loss: 4.193\t Batch perplexity: 66.244\t Batch accuracy: 28.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 13900/27699\t Batch loss: 4.009\t Batch perplexity: 55.065\t Batch accuracy: 28.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 13920/27699\t Batch loss: 4.128\t Batch perplexity: 62.075\t Batch accuracy: 27.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 13940/27699\t Batch loss: 4.241\t Batch perplexity: 69.482\t Batch accuracy: 28.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 13960/27699\t Batch loss: 3.883\t Batch perplexity: 48.546\t Batch accuracy: 30.9%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 13980/27699\t Batch loss: 4.061\t Batch perplexity: 58.052\t Batch accuracy: 29.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 14000/27699\t Batch loss: 4.098\t Batch perplexity: 60.245\t Batch accuracy: 29.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.830\t Batch perplexity: 46.063\t Batch accuracy: 29.2%\t***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2\t Train step: 14020/27699\t Batch loss: 4.234\t Batch perplexity: 69.022\t Batch accuracy: 28.1%\t 0.73s/batch\n",
      "Epoch: 2/2\t Train step: 14040/27699\t Batch loss: 4.140\t Batch perplexity: 62.774\t Batch accuracy: 30.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 14060/27699\t Batch loss: 4.070\t Batch perplexity: 58.573\t Batch accuracy: 29.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 14080/27699\t Batch loss: 4.159\t Batch perplexity: 63.986\t Batch accuracy: 29.8%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 14100/27699\t Batch loss: 4.064\t Batch perplexity: 58.199\t Batch accuracy: 28.1%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 14120/27699\t Batch loss: 3.968\t Batch perplexity: 52.859\t Batch accuracy: 30.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 14140/27699\t Batch loss: 4.297\t Batch perplexity: 73.485\t Batch accuracy: 27.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 14160/27699\t Batch loss: 4.059\t Batch perplexity: 57.890\t Batch accuracy: 32.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 14180/27699\t Batch loss: 3.954\t Batch perplexity: 52.127\t Batch accuracy: 28.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 14200/27699\t Batch loss: 4.112\t Batch perplexity: 61.048\t Batch accuracy: 28.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 14220/27699\t Batch loss: 4.116\t Batch perplexity: 61.331\t Batch accuracy: 28.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 14240/27699\t Batch loss: 4.120\t Batch perplexity: 61.553\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.934\t Batch perplexity: 51.097\t Batch accuracy: 30.5%\t***\n",
      "Epoch: 2/2\t Train step: 14260/27699\t Batch loss: 4.132\t Batch perplexity: 62.320\t Batch accuracy: 27.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 14280/27699\t Batch loss: 4.263\t Batch perplexity: 71.041\t Batch accuracy: 26.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 14300/27699\t Batch loss: 4.134\t Batch perplexity: 62.433\t Batch accuracy: 29.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 14320/27699\t Batch loss: 4.096\t Batch perplexity: 60.129\t Batch accuracy: 29.1%\t 0.76s/batch\n",
      "Epoch: 2/2\t Train step: 14340/27699\t Batch loss: 4.291\t Batch perplexity: 73.066\t Batch accuracy: 27.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 14360/27699\t Batch loss: 4.169\t Batch perplexity: 64.653\t Batch accuracy: 26.4%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 14380/27699\t Batch loss: 4.102\t Batch perplexity: 60.440\t Batch accuracy: 29.6%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 14400/27699\t Batch loss: 4.092\t Batch perplexity: 59.870\t Batch accuracy: 29.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 14420/27699\t Batch loss: 4.062\t Batch perplexity: 58.089\t Batch accuracy: 27.7%\t 0.78s/batch\n",
      "Epoch: 2/2\t Train step: 14440/27699\t Batch loss: 4.102\t Batch perplexity: 60.486\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 14460/27699\t Batch loss: 4.025\t Batch perplexity: 55.986\t Batch accuracy: 31.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 14480/27699\t Batch loss: 4.257\t Batch perplexity: 70.595\t Batch accuracy: 26.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 14500/27699\t Batch loss: 4.038\t Batch perplexity: 56.707\t Batch accuracy: 27.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.923\t Batch perplexity: 50.530\t Batch accuracy: 30.5%\t***\n",
      "Epoch: 2/2\t Train step: 14520/27699\t Batch loss: 4.198\t Batch perplexity: 66.552\t Batch accuracy: 28.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 14540/27699\t Batch loss: 4.071\t Batch perplexity: 58.624\t Batch accuracy: 27.4%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 14560/27699\t Batch loss: 4.222\t Batch perplexity: 68.194\t Batch accuracy: 28.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 14580/27699\t Batch loss: 4.200\t Batch perplexity: 66.685\t Batch accuracy: 27.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 14600/27699\t Batch loss: 4.036\t Batch perplexity: 56.612\t Batch accuracy: 28.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 14620/27699\t Batch loss: 4.131\t Batch perplexity: 62.228\t Batch accuracy: 28.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 14640/27699\t Batch loss: 4.102\t Batch perplexity: 60.462\t Batch accuracy: 27.1%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 14660/27699\t Batch loss: 4.109\t Batch perplexity: 60.860\t Batch accuracy: 28.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 14680/27699\t Batch loss: 4.083\t Batch perplexity: 59.346\t Batch accuracy: 28.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 14700/27699\t Batch loss: 4.280\t Batch perplexity: 72.254\t Batch accuracy: 27.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 14720/27699\t Batch loss: 4.186\t Batch perplexity: 65.737\t Batch accuracy: 28.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 14740/27699\t Batch loss: 4.453\t Batch perplexity: 85.886\t Batch accuracy: 24.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.828\t Batch perplexity: 45.972\t Batch accuracy: 33.9%\t***\n",
      "Epoch: 2/2\t Train step: 14760/27699\t Batch loss: 3.990\t Batch perplexity: 54.048\t Batch accuracy: 30.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 14780/27699\t Batch loss: 4.029\t Batch perplexity: 56.219\t Batch accuracy: 29.0%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 14800/27699\t Batch loss: 3.991\t Batch perplexity: 54.119\t Batch accuracy: 30.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 14820/27699\t Batch loss: 4.092\t Batch perplexity: 59.850\t Batch accuracy: 29.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 14840/27699\t Batch loss: 4.083\t Batch perplexity: 59.348\t Batch accuracy: 30.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 14860/27699\t Batch loss: 4.040\t Batch perplexity: 56.798\t Batch accuracy: 28.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 14880/27699\t Batch loss: 4.175\t Batch perplexity: 65.043\t Batch accuracy: 29.2%\t 0.76s/batch\n",
      "Epoch: 2/2\t Train step: 14900/27699\t Batch loss: 4.150\t Batch perplexity: 63.459\t Batch accuracy: 30.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 14920/27699\t Batch loss: 4.318\t Batch perplexity: 75.028\t Batch accuracy: 27.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 14940/27699\t Batch loss: 4.411\t Batch perplexity: 82.319\t Batch accuracy: 27.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 14960/27699\t Batch loss: 4.058\t Batch perplexity: 57.857\t Batch accuracy: 30.3%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 14980/27699\t Batch loss: 4.216\t Batch perplexity: 67.773\t Batch accuracy: 27.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 15000/27699\t Batch loss: 3.899\t Batch perplexity: 49.329\t Batch accuracy: 28.2%\t 0.75s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.991\t Batch perplexity: 54.132\t Batch accuracy: 28.1%\t***\n",
      "Epoch: 2/2\t Train step: 15020/27699\t Batch loss: 4.234\t Batch perplexity: 68.982\t Batch accuracy: 27.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 15040/27699\t Batch loss: 4.020\t Batch perplexity: 55.707\t Batch accuracy: 28.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 15060/27699\t Batch loss: 4.166\t Batch perplexity: 64.443\t Batch accuracy: 25.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 15080/27699\t Batch loss: 4.226\t Batch perplexity: 68.415\t Batch accuracy: 27.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 15100/27699\t Batch loss: 4.073\t Batch perplexity: 58.733\t Batch accuracy: 30.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 15120/27699\t Batch loss: 4.259\t Batch perplexity: 70.708\t Batch accuracy: 29.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 15140/27699\t Batch loss: 4.048\t Batch perplexity: 57.288\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 15160/27699\t Batch loss: 3.953\t Batch perplexity: 52.086\t Batch accuracy: 28.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 15180/27699\t Batch loss: 4.030\t Batch perplexity: 56.241\t Batch accuracy: 30.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 15200/27699\t Batch loss: 3.951\t Batch perplexity: 51.988\t Batch accuracy: 30.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 15220/27699\t Batch loss: 4.074\t Batch perplexity: 58.772\t Batch accuracy: 26.2%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 15240/27699\t Batch loss: 3.947\t Batch perplexity: 51.771\t Batch accuracy: 30.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.935\t Batch perplexity: 51.188\t Batch accuracy: 31.3%\t***\n",
      "Epoch: 2/2\t Train step: 15260/27699\t Batch loss: 4.077\t Batch perplexity: 58.949\t Batch accuracy: 30.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 15280/27699\t Batch loss: 4.129\t Batch perplexity: 62.122\t Batch accuracy: 29.1%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 15300/27699\t Batch loss: 4.116\t Batch perplexity: 61.315\t Batch accuracy: 29.7%\t 0.67s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2\t Train step: 15320/27699\t Batch loss: 4.348\t Batch perplexity: 77.337\t Batch accuracy: 26.1%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 15340/27699\t Batch loss: 3.979\t Batch perplexity: 53.446\t Batch accuracy: 31.2%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 15360/27699\t Batch loss: 4.374\t Batch perplexity: 79.374\t Batch accuracy: 25.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 15380/27699\t Batch loss: 4.138\t Batch perplexity: 62.692\t Batch accuracy: 26.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 15400/27699\t Batch loss: 4.135\t Batch perplexity: 62.502\t Batch accuracy: 28.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 15420/27699\t Batch loss: 4.222\t Batch perplexity: 68.196\t Batch accuracy: 26.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 15440/27699\t Batch loss: 4.136\t Batch perplexity: 62.529\t Batch accuracy: 27.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 15460/27699\t Batch loss: 4.228\t Batch perplexity: 68.601\t Batch accuracy: 25.5%\t 0.74s/batch\n",
      "Epoch: 2/2\t Train step: 15480/27699\t Batch loss: 3.856\t Batch perplexity: 47.279\t Batch accuracy: 32.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 15500/27699\t Batch loss: 4.074\t Batch perplexity: 58.770\t Batch accuracy: 29.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.979\t Batch perplexity: 53.445\t Batch accuracy: 27.9%\t***\n",
      "Epoch: 2/2\t Train step: 15520/27699\t Batch loss: 4.029\t Batch perplexity: 56.211\t Batch accuracy: 29.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 15540/27699\t Batch loss: 4.006\t Batch perplexity: 54.907\t Batch accuracy: 29.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 15560/27699\t Batch loss: 4.196\t Batch perplexity: 66.392\t Batch accuracy: 26.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 15580/27699\t Batch loss: 4.089\t Batch perplexity: 59.677\t Batch accuracy: 26.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 15600/27699\t Batch loss: 4.052\t Batch perplexity: 57.522\t Batch accuracy: 31.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 15620/27699\t Batch loss: 4.329\t Batch perplexity: 75.878\t Batch accuracy: 27.9%\t 0.66s/batch\n",
      "Epoch: 2/2\t Train step: 15640/27699\t Batch loss: 4.338\t Batch perplexity: 76.588\t Batch accuracy: 25.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 15660/27699\t Batch loss: 4.259\t Batch perplexity: 70.713\t Batch accuracy: 27.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 15680/27699\t Batch loss: 4.208\t Batch perplexity: 67.254\t Batch accuracy: 28.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 15700/27699\t Batch loss: 4.198\t Batch perplexity: 66.541\t Batch accuracy: 25.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 15720/27699\t Batch loss: 4.362\t Batch perplexity: 78.405\t Batch accuracy: 27.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 15740/27699\t Batch loss: 4.192\t Batch perplexity: 66.177\t Batch accuracy: 27.7%\t 0.71s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.021\t Batch perplexity: 55.762\t Batch accuracy: 28.2%\t***\n",
      "Epoch: 2/2\t Train step: 15760/27699\t Batch loss: 3.762\t Batch perplexity: 43.041\t Batch accuracy: 31.5%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 15780/27699\t Batch loss: 4.150\t Batch perplexity: 63.425\t Batch accuracy: 28.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 15800/27699\t Batch loss: 4.058\t Batch perplexity: 57.834\t Batch accuracy: 29.5%\t 0.73s/batch\n",
      "Epoch: 2/2\t Train step: 15820/27699\t Batch loss: 4.101\t Batch perplexity: 60.390\t Batch accuracy: 28.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 15840/27699\t Batch loss: 4.333\t Batch perplexity: 76.184\t Batch accuracy: 23.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 15860/27699\t Batch loss: 4.087\t Batch perplexity: 59.570\t Batch accuracy: 27.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 15880/27699\t Batch loss: 4.194\t Batch perplexity: 66.318\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 15900/27699\t Batch loss: 4.107\t Batch perplexity: 60.787\t Batch accuracy: 27.8%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 15920/27699\t Batch loss: 4.172\t Batch perplexity: 64.821\t Batch accuracy: 27.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 15940/27699\t Batch loss: 4.192\t Batch perplexity: 66.152\t Batch accuracy: 29.9%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 15960/27699\t Batch loss: 4.164\t Batch perplexity: 64.359\t Batch accuracy: 27.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 15980/27699\t Batch loss: 4.204\t Batch perplexity: 66.934\t Batch accuracy: 25.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 16000/27699\t Batch loss: 3.874\t Batch perplexity: 48.132\t Batch accuracy: 30.2%\t 0.67s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.988\t Batch perplexity: 53.962\t Batch accuracy: 28.4%\t***\n",
      "Epoch: 2/2\t Train step: 16020/27699\t Batch loss: 4.044\t Batch perplexity: 57.030\t Batch accuracy: 29.4%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 16040/27699\t Batch loss: 3.984\t Batch perplexity: 53.756\t Batch accuracy: 28.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 16060/27699\t Batch loss: 3.997\t Batch perplexity: 54.430\t Batch accuracy: 29.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 16080/27699\t Batch loss: 3.921\t Batch perplexity: 50.427\t Batch accuracy: 31.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 16100/27699\t Batch loss: 4.150\t Batch perplexity: 63.413\t Batch accuracy: 29.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 16120/27699\t Batch loss: 4.074\t Batch perplexity: 58.785\t Batch accuracy: 30.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 16140/27699\t Batch loss: 3.974\t Batch perplexity: 53.175\t Batch accuracy: 29.7%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 16160/27699\t Batch loss: 3.990\t Batch perplexity: 54.066\t Batch accuracy: 32.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 16180/27699\t Batch loss: 4.226\t Batch perplexity: 68.412\t Batch accuracy: 28.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 16200/27699\t Batch loss: 4.253\t Batch perplexity: 70.290\t Batch accuracy: 26.3%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 16220/27699\t Batch loss: 4.138\t Batch perplexity: 62.669\t Batch accuracy: 26.8%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 16240/27699\t Batch loss: 4.208\t Batch perplexity: 67.214\t Batch accuracy: 28.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.064\t Batch perplexity: 58.199\t Batch accuracy: 30.9%\t***\n",
      "Epoch: 2/2\t Train step: 16260/27699\t Batch loss: 4.159\t Batch perplexity: 64.000\t Batch accuracy: 27.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 16280/27699\t Batch loss: 4.042\t Batch perplexity: 56.941\t Batch accuracy: 29.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 16300/27699\t Batch loss: 4.200\t Batch perplexity: 66.688\t Batch accuracy: 29.8%\t 0.74s/batch\n",
      "Epoch: 2/2\t Train step: 16320/27699\t Batch loss: 4.162\t Batch perplexity: 64.214\t Batch accuracy: 27.9%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 16340/27699\t Batch loss: 4.004\t Batch perplexity: 54.835\t Batch accuracy: 29.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 16360/27699\t Batch loss: 3.805\t Batch perplexity: 44.907\t Batch accuracy: 30.4%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 16380/27699\t Batch loss: 4.235\t Batch perplexity: 69.067\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 16400/27699\t Batch loss: 4.079\t Batch perplexity: 59.085\t Batch accuracy: 29.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 16420/27699\t Batch loss: 4.435\t Batch perplexity: 84.394\t Batch accuracy: 25.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 16440/27699\t Batch loss: 4.070\t Batch perplexity: 58.551\t Batch accuracy: 28.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 16460/27699\t Batch loss: 4.031\t Batch perplexity: 56.310\t Batch accuracy: 28.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 16480/27699\t Batch loss: 3.984\t Batch perplexity: 53.713\t Batch accuracy: 29.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 16500/27699\t Batch loss: 3.912\t Batch perplexity: 49.975\t Batch accuracy: 29.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.991\t Batch perplexity: 54.092\t Batch accuracy: 29.2%\t***\n",
      "Epoch: 2/2\t Train step: 16520/27699\t Batch loss: 4.191\t Batch perplexity: 66.092\t Batch accuracy: 28.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 16540/27699\t Batch loss: 4.161\t Batch perplexity: 64.104\t Batch accuracy: 25.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 16560/27699\t Batch loss: 4.155\t Batch perplexity: 63.766\t Batch accuracy: 27.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 16580/27699\t Batch loss: 4.273\t Batch perplexity: 71.741\t Batch accuracy: 26.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 16600/27699\t Batch loss: 4.002\t Batch perplexity: 54.708\t Batch accuracy: 27.7%\t 0.68s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2\t Train step: 16620/27699\t Batch loss: 4.189\t Batch perplexity: 65.968\t Batch accuracy: 29.1%\t 0.72s/batch\n",
      "Epoch: 2/2\t Train step: 16640/27699\t Batch loss: 3.907\t Batch perplexity: 49.727\t Batch accuracy: 27.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 16660/27699\t Batch loss: 4.329\t Batch perplexity: 75.852\t Batch accuracy: 27.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 16680/27699\t Batch loss: 4.130\t Batch perplexity: 62.176\t Batch accuracy: 28.3%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 16700/27699\t Batch loss: 4.186\t Batch perplexity: 65.743\t Batch accuracy: 28.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 16720/27699\t Batch loss: 4.305\t Batch perplexity: 74.079\t Batch accuracy: 26.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 16740/27699\t Batch loss: 4.168\t Batch perplexity: 64.554\t Batch accuracy: 28.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.933\t Batch perplexity: 51.046\t Batch accuracy: 30.8%\t***\n",
      "Epoch: 2/2\t Train step: 16760/27699\t Batch loss: 4.042\t Batch perplexity: 56.916\t Batch accuracy: 30.1%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 16780/27699\t Batch loss: 4.073\t Batch perplexity: 58.750\t Batch accuracy: 26.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 16800/27699\t Batch loss: 4.330\t Batch perplexity: 75.919\t Batch accuracy: 27.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 16820/27699\t Batch loss: 4.186\t Batch perplexity: 65.731\t Batch accuracy: 26.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 16840/27699\t Batch loss: 4.046\t Batch perplexity: 57.159\t Batch accuracy: 30.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 16860/27699\t Batch loss: 4.153\t Batch perplexity: 63.632\t Batch accuracy: 27.8%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 16880/27699\t Batch loss: 4.125\t Batch perplexity: 61.874\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 16900/27699\t Batch loss: 4.183\t Batch perplexity: 65.579\t Batch accuracy: 28.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 16920/27699\t Batch loss: 4.391\t Batch perplexity: 80.704\t Batch accuracy: 27.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 16940/27699\t Batch loss: 4.086\t Batch perplexity: 59.510\t Batch accuracy: 27.4%\t 0.79s/batch\n",
      "Epoch: 2/2\t Train step: 16960/27699\t Batch loss: 4.139\t Batch perplexity: 62.742\t Batch accuracy: 26.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 16980/27699\t Batch loss: 4.068\t Batch perplexity: 58.445\t Batch accuracy: 28.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 17000/27699\t Batch loss: 4.166\t Batch perplexity: 64.472\t Batch accuracy: 28.2%\t 0.67s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.174\t Batch perplexity: 64.998\t Batch accuracy: 27.9%\t***\n",
      "Epoch: 2/2\t Train step: 17020/27699\t Batch loss: 4.100\t Batch perplexity: 60.358\t Batch accuracy: 29.3%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 17040/27699\t Batch loss: 4.115\t Batch perplexity: 61.246\t Batch accuracy: 30.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 17060/27699\t Batch loss: 4.195\t Batch perplexity: 66.323\t Batch accuracy: 26.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 17080/27699\t Batch loss: 4.162\t Batch perplexity: 64.191\t Batch accuracy: 26.8%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 17100/27699\t Batch loss: 4.048\t Batch perplexity: 57.301\t Batch accuracy: 28.0%\t 0.73s/batch\n",
      "Epoch: 2/2\t Train step: 17120/27699\t Batch loss: 4.162\t Batch perplexity: 64.182\t Batch accuracy: 27.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 17140/27699\t Batch loss: 4.141\t Batch perplexity: 62.884\t Batch accuracy: 28.2%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 17160/27699\t Batch loss: 4.102\t Batch perplexity: 60.469\t Batch accuracy: 27.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 17180/27699\t Batch loss: 4.207\t Batch perplexity: 67.180\t Batch accuracy: 26.7%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 17200/27699\t Batch loss: 4.025\t Batch perplexity: 55.959\t Batch accuracy: 29.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 17220/27699\t Batch loss: 4.269\t Batch perplexity: 71.482\t Batch accuracy: 26.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 17240/27699\t Batch loss: 4.332\t Batch perplexity: 76.102\t Batch accuracy: 26.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.912\t Batch perplexity: 50.020\t Batch accuracy: 31.3%\t***\n",
      "Epoch: 2/2\t Train step: 17260/27699\t Batch loss: 4.249\t Batch perplexity: 70.025\t Batch accuracy: 27.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 17280/27699\t Batch loss: 4.124\t Batch perplexity: 61.825\t Batch accuracy: 29.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 17300/27699\t Batch loss: 4.157\t Batch perplexity: 63.856\t Batch accuracy: 30.1%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 17320/27699\t Batch loss: 4.306\t Batch perplexity: 74.140\t Batch accuracy: 28.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 17340/27699\t Batch loss: 4.114\t Batch perplexity: 61.175\t Batch accuracy: 29.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 17360/27699\t Batch loss: 3.859\t Batch perplexity: 47.434\t Batch accuracy: 27.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 17380/27699\t Batch loss: 4.336\t Batch perplexity: 76.426\t Batch accuracy: 26.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 17400/27699\t Batch loss: 4.084\t Batch perplexity: 59.372\t Batch accuracy: 29.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 17420/27699\t Batch loss: 4.329\t Batch perplexity: 75.834\t Batch accuracy: 25.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 17440/27699\t Batch loss: 4.016\t Batch perplexity: 55.483\t Batch accuracy: 29.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 17460/27699\t Batch loss: 4.249\t Batch perplexity: 70.052\t Batch accuracy: 25.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 17480/27699\t Batch loss: 4.155\t Batch perplexity: 63.765\t Batch accuracy: 30.2%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 17500/27699\t Batch loss: 4.206\t Batch perplexity: 67.121\t Batch accuracy: 29.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.986\t Batch perplexity: 53.850\t Batch accuracy: 29.5%\t***\n",
      "Epoch: 2/2\t Train step: 17520/27699\t Batch loss: 4.141\t Batch perplexity: 62.835\t Batch accuracy: 27.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 17540/27699\t Batch loss: 4.271\t Batch perplexity: 71.592\t Batch accuracy: 29.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 17560/27699\t Batch loss: 4.152\t Batch perplexity: 63.567\t Batch accuracy: 29.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 17580/27699\t Batch loss: 4.137\t Batch perplexity: 62.609\t Batch accuracy: 28.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 17600/27699\t Batch loss: 4.264\t Batch perplexity: 71.085\t Batch accuracy: 27.8%\t 0.76s/batch\n",
      "Epoch: 2/2\t Train step: 17620/27699\t Batch loss: 3.956\t Batch perplexity: 52.234\t Batch accuracy: 30.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 17640/27699\t Batch loss: 4.084\t Batch perplexity: 59.365\t Batch accuracy: 29.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 17660/27699\t Batch loss: 3.933\t Batch perplexity: 51.080\t Batch accuracy: 28.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 17680/27699\t Batch loss: 4.088\t Batch perplexity: 59.610\t Batch accuracy: 29.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 17700/27699\t Batch loss: 4.105\t Batch perplexity: 60.631\t Batch accuracy: 27.3%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 17720/27699\t Batch loss: 4.363\t Batch perplexity: 78.490\t Batch accuracy: 28.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 17740/27699\t Batch loss: 4.176\t Batch perplexity: 65.105\t Batch accuracy: 28.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.033\t Batch perplexity: 56.444\t Batch accuracy: 31.5%\t***\n",
      "Epoch: 2/2\t Train step: 17760/27699\t Batch loss: 4.217\t Batch perplexity: 67.816\t Batch accuracy: 28.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 17780/27699\t Batch loss: 4.221\t Batch perplexity: 68.076\t Batch accuracy: 26.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 17800/27699\t Batch loss: 4.149\t Batch perplexity: 63.348\t Batch accuracy: 28.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 17820/27699\t Batch loss: 4.077\t Batch perplexity: 58.991\t Batch accuracy: 28.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 17840/27699\t Batch loss: 4.074\t Batch perplexity: 58.799\t Batch accuracy: 29.3%\t 0.66s/batch\n",
      "Epoch: 2/2\t Train step: 17860/27699\t Batch loss: 3.932\t Batch perplexity: 51.006\t Batch accuracy: 29.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 17880/27699\t Batch loss: 3.974\t Batch perplexity: 53.223\t Batch accuracy: 28.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 17900/27699\t Batch loss: 3.968\t Batch perplexity: 52.864\t Batch accuracy: 30.7%\t 0.73s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2\t Train step: 17920/27699\t Batch loss: 4.051\t Batch perplexity: 57.475\t Batch accuracy: 30.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 17940/27699\t Batch loss: 4.200\t Batch perplexity: 66.678\t Batch accuracy: 28.1%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 17960/27699\t Batch loss: 3.925\t Batch perplexity: 50.674\t Batch accuracy: 29.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 17980/27699\t Batch loss: 4.161\t Batch perplexity: 64.123\t Batch accuracy: 28.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 18000/27699\t Batch loss: 4.145\t Batch perplexity: 63.108\t Batch accuracy: 28.0%\t 0.71s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.996\t Batch perplexity: 54.405\t Batch accuracy: 29.6%\t***\n",
      "Epoch: 2/2\t Train step: 18020/27699\t Batch loss: 4.080\t Batch perplexity: 59.163\t Batch accuracy: 28.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 18040/27699\t Batch loss: 3.948\t Batch perplexity: 51.851\t Batch accuracy: 31.6%\t 0.75s/batch\n",
      "Epoch: 2/2\t Train step: 18060/27699\t Batch loss: 4.155\t Batch perplexity: 63.754\t Batch accuracy: 30.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 18080/27699\t Batch loss: 4.112\t Batch perplexity: 61.048\t Batch accuracy: 29.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 18100/27699\t Batch loss: 4.302\t Batch perplexity: 73.870\t Batch accuracy: 29.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 18120/27699\t Batch loss: 4.111\t Batch perplexity: 61.009\t Batch accuracy: 27.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 18140/27699\t Batch loss: 4.205\t Batch perplexity: 67.002\t Batch accuracy: 28.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 18160/27699\t Batch loss: 4.023\t Batch perplexity: 55.892\t Batch accuracy: 28.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 18180/27699\t Batch loss: 4.109\t Batch perplexity: 60.911\t Batch accuracy: 27.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 18200/27699\t Batch loss: 4.216\t Batch perplexity: 67.789\t Batch accuracy: 27.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 18220/27699\t Batch loss: 4.220\t Batch perplexity: 68.027\t Batch accuracy: 29.7%\t 0.66s/batch\n",
      "Epoch: 2/2\t Train step: 18240/27699\t Batch loss: 4.198\t Batch perplexity: 66.564\t Batch accuracy: 28.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.002\t Batch perplexity: 54.701\t Batch accuracy: 27.0%\t***\n",
      "Epoch: 2/2\t Train step: 18260/27699\t Batch loss: 4.133\t Batch perplexity: 62.374\t Batch accuracy: 27.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 18280/27699\t Batch loss: 4.031\t Batch perplexity: 56.328\t Batch accuracy: 30.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 18300/27699\t Batch loss: 4.093\t Batch perplexity: 59.904\t Batch accuracy: 30.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 18320/27699\t Batch loss: 4.064\t Batch perplexity: 58.193\t Batch accuracy: 29.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 18340/27699\t Batch loss: 4.066\t Batch perplexity: 58.320\t Batch accuracy: 29.2%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 18360/27699\t Batch loss: 4.262\t Batch perplexity: 70.933\t Batch accuracy: 26.8%\t 0.73s/batch\n",
      "Epoch: 2/2\t Train step: 18380/27699\t Batch loss: 4.144\t Batch perplexity: 63.074\t Batch accuracy: 29.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 18400/27699\t Batch loss: 4.201\t Batch perplexity: 66.730\t Batch accuracy: 26.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 18420/27699\t Batch loss: 4.231\t Batch perplexity: 68.774\t Batch accuracy: 25.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 18440/27699\t Batch loss: 4.035\t Batch perplexity: 56.564\t Batch accuracy: 29.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 18460/27699\t Batch loss: 4.022\t Batch perplexity: 55.790\t Batch accuracy: 29.1%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 18480/27699\t Batch loss: 4.151\t Batch perplexity: 63.482\t Batch accuracy: 28.1%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 18500/27699\t Batch loss: 4.019\t Batch perplexity: 55.667\t Batch accuracy: 29.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.045\t Batch perplexity: 57.115\t Batch accuracy: 32.2%\t***\n",
      "Epoch: 2/2\t Train step: 18520/27699\t Batch loss: 4.336\t Batch perplexity: 76.411\t Batch accuracy: 26.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 18540/27699\t Batch loss: 3.999\t Batch perplexity: 54.569\t Batch accuracy: 29.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 18560/27699\t Batch loss: 4.003\t Batch perplexity: 54.787\t Batch accuracy: 30.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 18580/27699\t Batch loss: 4.267\t Batch perplexity: 71.307\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 18600/27699\t Batch loss: 4.089\t Batch perplexity: 59.682\t Batch accuracy: 28.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 18620/27699\t Batch loss: 4.327\t Batch perplexity: 75.745\t Batch accuracy: 26.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 18640/27699\t Batch loss: 4.067\t Batch perplexity: 58.398\t Batch accuracy: 28.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 18660/27699\t Batch loss: 4.071\t Batch perplexity: 58.632\t Batch accuracy: 30.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 18680/27699\t Batch loss: 4.155\t Batch perplexity: 63.772\t Batch accuracy: 30.4%\t 0.82s/batch\n",
      "Epoch: 2/2\t Train step: 18700/27699\t Batch loss: 3.982\t Batch perplexity: 53.647\t Batch accuracy: 28.1%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 18720/27699\t Batch loss: 4.213\t Batch perplexity: 67.588\t Batch accuracy: 28.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 18740/27699\t Batch loss: 3.932\t Batch perplexity: 51.017\t Batch accuracy: 30.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.002\t Batch perplexity: 54.708\t Batch accuracy: 31.0%\t***\n",
      "Epoch: 2/2\t Train step: 18760/27699\t Batch loss: 4.251\t Batch perplexity: 70.190\t Batch accuracy: 29.1%\t 0.72s/batch\n",
      "Epoch: 2/2\t Train step: 18780/27699\t Batch loss: 4.153\t Batch perplexity: 63.620\t Batch accuracy: 29.8%\t 0.72s/batch\n",
      "Epoch: 2/2\t Train step: 18800/27699\t Batch loss: 4.131\t Batch perplexity: 62.257\t Batch accuracy: 29.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 18820/27699\t Batch loss: 4.134\t Batch perplexity: 62.400\t Batch accuracy: 28.0%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 18840/27699\t Batch loss: 4.029\t Batch perplexity: 56.210\t Batch accuracy: 28.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 18860/27699\t Batch loss: 4.015\t Batch perplexity: 55.421\t Batch accuracy: 27.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 18880/27699\t Batch loss: 3.997\t Batch perplexity: 54.424\t Batch accuracy: 31.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 18900/27699\t Batch loss: 4.166\t Batch perplexity: 64.435\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 18920/27699\t Batch loss: 4.185\t Batch perplexity: 65.700\t Batch accuracy: 26.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 18940/27699\t Batch loss: 4.121\t Batch perplexity: 61.637\t Batch accuracy: 30.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 18960/27699\t Batch loss: 4.317\t Batch perplexity: 74.961\t Batch accuracy: 27.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 18980/27699\t Batch loss: 4.129\t Batch perplexity: 62.144\t Batch accuracy: 26.7%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 19000/27699\t Batch loss: 4.234\t Batch perplexity: 68.964\t Batch accuracy: 26.5%\t 0.84s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.961\t Batch perplexity: 52.495\t Batch accuracy: 27.4%\t***\n",
      "Epoch: 2/2\t Train step: 19020/27699\t Batch loss: 4.149\t Batch perplexity: 63.363\t Batch accuracy: 28.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 19040/27699\t Batch loss: 4.151\t Batch perplexity: 63.484\t Batch accuracy: 28.4%\t 0.73s/batch\n",
      "Epoch: 2/2\t Train step: 19060/27699\t Batch loss: 4.040\t Batch perplexity: 56.832\t Batch accuracy: 30.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 19080/27699\t Batch loss: 4.220\t Batch perplexity: 68.033\t Batch accuracy: 26.8%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 19100/27699\t Batch loss: 4.132\t Batch perplexity: 62.285\t Batch accuracy: 27.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 19120/27699\t Batch loss: 4.113\t Batch perplexity: 61.120\t Batch accuracy: 29.0%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 19140/27699\t Batch loss: 4.179\t Batch perplexity: 65.292\t Batch accuracy: 26.5%\t 0.72s/batch\n",
      "Epoch: 2/2\t Train step: 19160/27699\t Batch loss: 4.363\t Batch perplexity: 78.502\t Batch accuracy: 24.5%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 19180/27699\t Batch loss: 4.003\t Batch perplexity: 54.767\t Batch accuracy: 29.7%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 19200/27699\t Batch loss: 4.140\t Batch perplexity: 62.802\t Batch accuracy: 28.4%\t 0.71s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2\t Train step: 19220/27699\t Batch loss: 4.295\t Batch perplexity: 73.369\t Batch accuracy: 26.4%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 19240/27699\t Batch loss: 4.039\t Batch perplexity: 56.766\t Batch accuracy: 28.1%\t 0.79s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.081\t Batch perplexity: 59.206\t Batch accuracy: 28.6%\t***\n",
      "Epoch: 2/2\t Train step: 19260/27699\t Batch loss: 4.012\t Batch perplexity: 55.245\t Batch accuracy: 29.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 19280/27699\t Batch loss: 4.196\t Batch perplexity: 66.401\t Batch accuracy: 26.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 19300/27699\t Batch loss: 4.003\t Batch perplexity: 54.750\t Batch accuracy: 30.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 19320/27699\t Batch loss: 4.277\t Batch perplexity: 72.031\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 19340/27699\t Batch loss: 4.131\t Batch perplexity: 62.219\t Batch accuracy: 28.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 19360/27699\t Batch loss: 4.065\t Batch perplexity: 58.281\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 19380/27699\t Batch loss: 4.262\t Batch perplexity: 70.934\t Batch accuracy: 27.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 19400/27699\t Batch loss: 4.002\t Batch perplexity: 54.700\t Batch accuracy: 30.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 19420/27699\t Batch loss: 4.315\t Batch perplexity: 74.803\t Batch accuracy: 28.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 19440/27699\t Batch loss: 4.029\t Batch perplexity: 56.193\t Batch accuracy: 27.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 19460/27699\t Batch loss: 4.145\t Batch perplexity: 63.112\t Batch accuracy: 29.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 19480/27699\t Batch loss: 4.161\t Batch perplexity: 64.158\t Batch accuracy: 26.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 19500/27699\t Batch loss: 4.224\t Batch perplexity: 68.274\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.016\t Batch perplexity: 55.477\t Batch accuracy: 30.5%\t***\n",
      "Epoch: 2/2\t Train step: 19520/27699\t Batch loss: 4.128\t Batch perplexity: 62.030\t Batch accuracy: 28.4%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 19540/27699\t Batch loss: 4.280\t Batch perplexity: 72.260\t Batch accuracy: 31.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 19560/27699\t Batch loss: 4.316\t Batch perplexity: 74.899\t Batch accuracy: 26.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 19580/27699\t Batch loss: 4.087\t Batch perplexity: 59.587\t Batch accuracy: 28.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 19600/27699\t Batch loss: 4.213\t Batch perplexity: 67.566\t Batch accuracy: 26.9%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 19620/27699\t Batch loss: 4.257\t Batch perplexity: 70.605\t Batch accuracy: 26.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 19640/27699\t Batch loss: 4.202\t Batch perplexity: 66.808\t Batch accuracy: 27.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 19660/27699\t Batch loss: 3.851\t Batch perplexity: 47.026\t Batch accuracy: 32.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 19680/27699\t Batch loss: 4.464\t Batch perplexity: 86.832\t Batch accuracy: 23.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 19700/27699\t Batch loss: 4.084\t Batch perplexity: 59.365\t Batch accuracy: 25.6%\t 0.66s/batch\n",
      "Epoch: 2/2\t Train step: 19720/27699\t Batch loss: 4.229\t Batch perplexity: 68.682\t Batch accuracy: 25.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 19740/27699\t Batch loss: 4.144\t Batch perplexity: 63.071\t Batch accuracy: 27.7%\t 0.73s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.953\t Batch perplexity: 52.067\t Batch accuracy: 29.8%\t***\n",
      "Epoch: 2/2\t Train step: 19760/27699\t Batch loss: 4.148\t Batch perplexity: 63.284\t Batch accuracy: 25.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 19780/27699\t Batch loss: 4.033\t Batch perplexity: 56.441\t Batch accuracy: 29.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 19800/27699\t Batch loss: 4.124\t Batch perplexity: 61.794\t Batch accuracy: 28.8%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 19820/27699\t Batch loss: 4.114\t Batch perplexity: 61.217\t Batch accuracy: 29.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 19840/27699\t Batch loss: 4.197\t Batch perplexity: 66.482\t Batch accuracy: 29.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 19860/27699\t Batch loss: 4.092\t Batch perplexity: 59.888\t Batch accuracy: 29.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 19880/27699\t Batch loss: 4.258\t Batch perplexity: 70.671\t Batch accuracy: 27.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 19900/27699\t Batch loss: 4.308\t Batch perplexity: 74.304\t Batch accuracy: 27.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 19920/27699\t Batch loss: 4.112\t Batch perplexity: 61.039\t Batch accuracy: 29.0%\t 0.74s/batch\n",
      "Epoch: 2/2\t Train step: 19940/27699\t Batch loss: 4.246\t Batch perplexity: 69.812\t Batch accuracy: 28.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 19960/27699\t Batch loss: 4.250\t Batch perplexity: 70.084\t Batch accuracy: 28.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 19980/27699\t Batch loss: 4.275\t Batch perplexity: 71.879\t Batch accuracy: 27.1%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 20000/27699\t Batch loss: 4.283\t Batch perplexity: 72.460\t Batch accuracy: 26.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.933\t Batch perplexity: 51.044\t Batch accuracy: 29.8%\t***\n",
      "Epoch: 2/2\t Train step: 20020/27699\t Batch loss: 3.886\t Batch perplexity: 48.706\t Batch accuracy: 28.3%\t 0.73s/batch\n",
      "Epoch: 2/2\t Train step: 20040/27699\t Batch loss: 4.122\t Batch perplexity: 61.711\t Batch accuracy: 28.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 20060/27699\t Batch loss: 4.223\t Batch perplexity: 68.229\t Batch accuracy: 25.2%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 20080/27699\t Batch loss: 4.076\t Batch perplexity: 58.910\t Batch accuracy: 28.7%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 20100/27699\t Batch loss: 4.243\t Batch perplexity: 69.651\t Batch accuracy: 28.3%\t 0.73s/batch\n",
      "Epoch: 2/2\t Train step: 20120/27699\t Batch loss: 3.918\t Batch perplexity: 50.302\t Batch accuracy: 28.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 20140/27699\t Batch loss: 4.094\t Batch perplexity: 59.986\t Batch accuracy: 26.4%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 20160/27699\t Batch loss: 4.070\t Batch perplexity: 58.578\t Batch accuracy: 28.9%\t 0.82s/batch\n",
      "Epoch: 2/2\t Train step: 20180/27699\t Batch loss: 4.069\t Batch perplexity: 58.493\t Batch accuracy: 29.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 20200/27699\t Batch loss: 4.293\t Batch perplexity: 73.210\t Batch accuracy: 27.4%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 20220/27699\t Batch loss: 3.837\t Batch perplexity: 46.403\t Batch accuracy: 32.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 20240/27699\t Batch loss: 4.221\t Batch perplexity: 68.120\t Batch accuracy: 28.2%\t 0.70s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.702\t Batch perplexity: 40.546\t Batch accuracy: 30.6%\t***\n",
      "Epoch: 2/2\t Train step: 20260/27699\t Batch loss: 3.904\t Batch perplexity: 49.587\t Batch accuracy: 28.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 20280/27699\t Batch loss: 4.266\t Batch perplexity: 71.262\t Batch accuracy: 26.8%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 20300/27699\t Batch loss: 4.012\t Batch perplexity: 55.275\t Batch accuracy: 31.5%\t 0.82s/batch\n",
      "Epoch: 2/2\t Train step: 20320/27699\t Batch loss: 4.183\t Batch perplexity: 65.545\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 20340/27699\t Batch loss: 4.321\t Batch perplexity: 75.256\t Batch accuracy: 27.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 20360/27699\t Batch loss: 4.100\t Batch perplexity: 60.320\t Batch accuracy: 29.3%\t 0.73s/batch\n",
      "Epoch: 2/2\t Train step: 20380/27699\t Batch loss: 4.082\t Batch perplexity: 59.290\t Batch accuracy: 29.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 20400/27699\t Batch loss: 4.270\t Batch perplexity: 71.522\t Batch accuracy: 27.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 20420/27699\t Batch loss: 4.262\t Batch perplexity: 70.947\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 20440/27699\t Batch loss: 4.239\t Batch perplexity: 69.327\t Batch accuracy: 26.5%\t 0.79s/batch\n",
      "Epoch: 2/2\t Train step: 20460/27699\t Batch loss: 4.103\t Batch perplexity: 60.493\t Batch accuracy: 30.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 20480/27699\t Batch loss: 4.216\t Batch perplexity: 67.736\t Batch accuracy: 26.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 20500/27699\t Batch loss: 4.090\t Batch perplexity: 59.721\t Batch accuracy: 27.6%\t 0.68s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.902\t Batch perplexity: 49.484\t Batch accuracy: 32.1%\t***\n",
      "Epoch: 2/2\t Train step: 20520/27699\t Batch loss: 4.217\t Batch perplexity: 67.823\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 20540/27699\t Batch loss: 4.157\t Batch perplexity: 63.877\t Batch accuracy: 29.3%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 20560/27699\t Batch loss: 3.907\t Batch perplexity: 49.756\t Batch accuracy: 29.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 20580/27699\t Batch loss: 4.071\t Batch perplexity: 58.595\t Batch accuracy: 29.7%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 20600/27699\t Batch loss: 4.200\t Batch perplexity: 66.701\t Batch accuracy: 27.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 20620/27699\t Batch loss: 4.090\t Batch perplexity: 59.745\t Batch accuracy: 27.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 20640/27699\t Batch loss: 4.098\t Batch perplexity: 60.236\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 20660/27699\t Batch loss: 4.020\t Batch perplexity: 55.693\t Batch accuracy: 30.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 20680/27699\t Batch loss: 4.132\t Batch perplexity: 62.310\t Batch accuracy: 27.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 20700/27699\t Batch loss: 4.001\t Batch perplexity: 54.649\t Batch accuracy: 28.5%\t 0.73s/batch\n",
      "Epoch: 2/2\t Train step: 20720/27699\t Batch loss: 4.123\t Batch perplexity: 61.722\t Batch accuracy: 31.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 20740/27699\t Batch loss: 4.248\t Batch perplexity: 69.933\t Batch accuracy: 28.7%\t 0.82s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.145\t Batch perplexity: 63.097\t Batch accuracy: 27.5%\t***\n",
      "Epoch: 2/2\t Train step: 20760/27699\t Batch loss: 4.184\t Batch perplexity: 65.630\t Batch accuracy: 28.1%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 20780/27699\t Batch loss: 4.005\t Batch perplexity: 54.896\t Batch accuracy: 28.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 20800/27699\t Batch loss: 3.980\t Batch perplexity: 53.536\t Batch accuracy: 30.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 20820/27699\t Batch loss: 4.142\t Batch perplexity: 62.930\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 20840/27699\t Batch loss: 4.093\t Batch perplexity: 59.927\t Batch accuracy: 28.0%\t 0.72s/batch\n",
      "Epoch: 2/2\t Train step: 20860/27699\t Batch loss: 4.309\t Batch perplexity: 74.354\t Batch accuracy: 26.2%\t 0.77s/batch\n",
      "Epoch: 2/2\t Train step: 20880/27699\t Batch loss: 4.102\t Batch perplexity: 60.440\t Batch accuracy: 28.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 20900/27699\t Batch loss: 4.130\t Batch perplexity: 62.152\t Batch accuracy: 29.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 20920/27699\t Batch loss: 4.232\t Batch perplexity: 68.846\t Batch accuracy: 27.4%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 20940/27699\t Batch loss: 4.227\t Batch perplexity: 68.511\t Batch accuracy: 29.2%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 20960/27699\t Batch loss: 3.968\t Batch perplexity: 52.896\t Batch accuracy: 28.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 20980/27699\t Batch loss: 4.017\t Batch perplexity: 55.524\t Batch accuracy: 28.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 21000/27699\t Batch loss: 4.204\t Batch perplexity: 66.950\t Batch accuracy: 27.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.063\t Batch perplexity: 58.167\t Batch accuracy: 28.5%\t***\n",
      "Epoch: 2/2\t Train step: 21020/27699\t Batch loss: 4.196\t Batch perplexity: 66.434\t Batch accuracy: 27.2%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 21040/27699\t Batch loss: 4.149\t Batch perplexity: 63.373\t Batch accuracy: 28.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 21060/27699\t Batch loss: 4.222\t Batch perplexity: 68.194\t Batch accuracy: 27.1%\t 0.74s/batch\n",
      "Epoch: 2/2\t Train step: 21080/27699\t Batch loss: 4.158\t Batch perplexity: 63.931\t Batch accuracy: 26.3%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 21100/27699\t Batch loss: 4.064\t Batch perplexity: 58.207\t Batch accuracy: 28.5%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 21120/27699\t Batch loss: 4.065\t Batch perplexity: 58.290\t Batch accuracy: 29.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 21140/27699\t Batch loss: 4.096\t Batch perplexity: 60.080\t Batch accuracy: 30.2%\t 0.84s/batch\n",
      "Epoch: 2/2\t Train step: 21160/27699\t Batch loss: 4.394\t Batch perplexity: 80.951\t Batch accuracy: 24.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 21180/27699\t Batch loss: 4.231\t Batch perplexity: 68.761\t Batch accuracy: 27.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 21200/27699\t Batch loss: 3.744\t Batch perplexity: 42.255\t Batch accuracy: 30.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 21220/27699\t Batch loss: 4.179\t Batch perplexity: 65.273\t Batch accuracy: 28.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 21240/27699\t Batch loss: 3.970\t Batch perplexity: 52.965\t Batch accuracy: 29.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.138\t Batch perplexity: 62.663\t Batch accuracy: 30.6%\t***\n",
      "Epoch: 2/2\t Train step: 21260/27699\t Batch loss: 3.936\t Batch perplexity: 51.237\t Batch accuracy: 31.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 21280/27699\t Batch loss: 4.068\t Batch perplexity: 58.415\t Batch accuracy: 27.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 21300/27699\t Batch loss: 4.180\t Batch perplexity: 65.346\t Batch accuracy: 27.0%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 21320/27699\t Batch loss: 4.010\t Batch perplexity: 55.151\t Batch accuracy: 31.1%\t 0.72s/batch\n",
      "Epoch: 2/2\t Train step: 21340/27699\t Batch loss: 4.034\t Batch perplexity: 56.494\t Batch accuracy: 29.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 21360/27699\t Batch loss: 4.109\t Batch perplexity: 60.887\t Batch accuracy: 28.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 21380/27699\t Batch loss: 4.111\t Batch perplexity: 61.004\t Batch accuracy: 27.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 21400/27699\t Batch loss: 3.972\t Batch perplexity: 53.090\t Batch accuracy: 29.8%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 21420/27699\t Batch loss: 4.137\t Batch perplexity: 62.598\t Batch accuracy: 30.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 21440/27699\t Batch loss: 3.970\t Batch perplexity: 52.999\t Batch accuracy: 28.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 21460/27699\t Batch loss: 3.973\t Batch perplexity: 53.122\t Batch accuracy: 29.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 21480/27699\t Batch loss: 4.249\t Batch perplexity: 70.018\t Batch accuracy: 24.7%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 21500/27699\t Batch loss: 4.116\t Batch perplexity: 61.321\t Batch accuracy: 26.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.769\t Batch perplexity: 43.356\t Batch accuracy: 32.7%\t***\n",
      "Epoch: 2/2\t Train step: 21520/27699\t Batch loss: 4.178\t Batch perplexity: 65.255\t Batch accuracy: 28.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 21540/27699\t Batch loss: 4.115\t Batch perplexity: 61.233\t Batch accuracy: 28.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 21560/27699\t Batch loss: 4.197\t Batch perplexity: 66.514\t Batch accuracy: 29.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 21580/27699\t Batch loss: 4.217\t Batch perplexity: 67.836\t Batch accuracy: 28.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 21600/27699\t Batch loss: 4.149\t Batch perplexity: 63.353\t Batch accuracy: 28.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 21620/27699\t Batch loss: 4.174\t Batch perplexity: 64.996\t Batch accuracy: 28.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 21640/27699\t Batch loss: 3.924\t Batch perplexity: 50.621\t Batch accuracy: 26.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 21660/27699\t Batch loss: 4.156\t Batch perplexity: 63.810\t Batch accuracy: 28.6%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 21680/27699\t Batch loss: 4.241\t Batch perplexity: 69.461\t Batch accuracy: 27.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 21700/27699\t Batch loss: 4.211\t Batch perplexity: 67.438\t Batch accuracy: 28.2%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 21720/27699\t Batch loss: 4.246\t Batch perplexity: 69.834\t Batch accuracy: 26.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 21740/27699\t Batch loss: 4.049\t Batch perplexity: 57.334\t Batch accuracy: 29.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.925\t Batch perplexity: 50.646\t Batch accuracy: 29.9%\t***\n",
      "Epoch: 2/2\t Train step: 21760/27699\t Batch loss: 4.112\t Batch perplexity: 61.096\t Batch accuracy: 27.5%\t 0.81s/batch\n",
      "Epoch: 2/2\t Train step: 21780/27699\t Batch loss: 4.013\t Batch perplexity: 55.325\t Batch accuracy: 30.7%\t 0.86s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2\t Train step: 21800/27699\t Batch loss: 4.119\t Batch perplexity: 61.473\t Batch accuracy: 26.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 21820/27699\t Batch loss: 4.022\t Batch perplexity: 55.835\t Batch accuracy: 27.8%\t 0.66s/batch\n",
      "Epoch: 2/2\t Train step: 21840/27699\t Batch loss: 3.855\t Batch perplexity: 47.244\t Batch accuracy: 27.6%\t 0.77s/batch\n",
      "Epoch: 2/2\t Train step: 21860/27699\t Batch loss: 3.957\t Batch perplexity: 52.293\t Batch accuracy: 29.9%\t 0.74s/batch\n",
      "Epoch: 2/2\t Train step: 21880/27699\t Batch loss: 3.986\t Batch perplexity: 53.851\t Batch accuracy: 27.2%\t 0.77s/batch\n",
      "Epoch: 2/2\t Train step: 21900/27699\t Batch loss: 4.287\t Batch perplexity: 72.717\t Batch accuracy: 26.6%\t 0.81s/batch\n",
      "Epoch: 2/2\t Train step: 21920/27699\t Batch loss: 4.085\t Batch perplexity: 59.468\t Batch accuracy: 29.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 21940/27699\t Batch loss: 3.970\t Batch perplexity: 53.003\t Batch accuracy: 30.9%\t 0.72s/batch\n",
      "Epoch: 2/2\t Train step: 21960/27699\t Batch loss: 4.033\t Batch perplexity: 56.419\t Batch accuracy: 29.8%\t 0.72s/batch\n",
      "Epoch: 2/2\t Train step: 21980/27699\t Batch loss: 3.923\t Batch perplexity: 50.576\t Batch accuracy: 31.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 22000/27699\t Batch loss: 4.112\t Batch perplexity: 61.064\t Batch accuracy: 28.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.199\t Batch perplexity: 66.640\t Batch accuracy: 27.4%\t***\n",
      "Epoch: 2/2\t Train step: 22020/27699\t Batch loss: 4.149\t Batch perplexity: 63.344\t Batch accuracy: 26.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 22040/27699\t Batch loss: 3.989\t Batch perplexity: 53.990\t Batch accuracy: 30.8%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 22060/27699\t Batch loss: 4.110\t Batch perplexity: 60.964\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 22080/27699\t Batch loss: 3.898\t Batch perplexity: 49.294\t Batch accuracy: 26.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 22100/27699\t Batch loss: 4.254\t Batch perplexity: 70.421\t Batch accuracy: 30.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 22120/27699\t Batch loss: 4.009\t Batch perplexity: 55.079\t Batch accuracy: 28.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 22140/27699\t Batch loss: 4.173\t Batch perplexity: 64.880\t Batch accuracy: 29.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 22160/27699\t Batch loss: 4.356\t Batch perplexity: 77.943\t Batch accuracy: 28.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 22180/27699\t Batch loss: 3.925\t Batch perplexity: 50.666\t Batch accuracy: 31.1%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 22200/27699\t Batch loss: 4.192\t Batch perplexity: 66.180\t Batch accuracy: 27.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 22220/27699\t Batch loss: 4.359\t Batch perplexity: 78.215\t Batch accuracy: 28.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 22240/27699\t Batch loss: 4.036\t Batch perplexity: 56.586\t Batch accuracy: 29.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.104\t Batch perplexity: 60.597\t Batch accuracy: 29.8%\t***\n",
      "Epoch: 2/2\t Train step: 22260/27699\t Batch loss: 4.210\t Batch perplexity: 67.331\t Batch accuracy: 27.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 22280/27699\t Batch loss: 4.099\t Batch perplexity: 60.291\t Batch accuracy: 27.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 22300/27699\t Batch loss: 4.159\t Batch perplexity: 63.995\t Batch accuracy: 27.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 22320/27699\t Batch loss: 4.037\t Batch perplexity: 56.641\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 22340/27699\t Batch loss: 3.998\t Batch perplexity: 54.485\t Batch accuracy: 30.2%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 22360/27699\t Batch loss: 3.915\t Batch perplexity: 50.132\t Batch accuracy: 30.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 22380/27699\t Batch loss: 4.000\t Batch perplexity: 54.594\t Batch accuracy: 29.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 22400/27699\t Batch loss: 4.112\t Batch perplexity: 61.061\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 22420/27699\t Batch loss: 4.169\t Batch perplexity: 64.668\t Batch accuracy: 28.1%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 22440/27699\t Batch loss: 4.242\t Batch perplexity: 69.529\t Batch accuracy: 27.4%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 22460/27699\t Batch loss: 4.310\t Batch perplexity: 74.438\t Batch accuracy: 28.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 22480/27699\t Batch loss: 4.038\t Batch perplexity: 56.702\t Batch accuracy: 29.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 22500/27699\t Batch loss: 4.030\t Batch perplexity: 56.267\t Batch accuracy: 27.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.961\t Batch perplexity: 52.523\t Batch accuracy: 27.7%\t***\n",
      "Epoch: 2/2\t Train step: 22520/27699\t Batch loss: 4.291\t Batch perplexity: 73.025\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 22540/27699\t Batch loss: 4.198\t Batch perplexity: 66.566\t Batch accuracy: 26.5%\t 0.78s/batch\n",
      "Epoch: 2/2\t Train step: 22560/27699\t Batch loss: 3.967\t Batch perplexity: 52.830\t Batch accuracy: 30.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 22580/27699\t Batch loss: 4.164\t Batch perplexity: 64.300\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 22600/27699\t Batch loss: 4.109\t Batch perplexity: 60.892\t Batch accuracy: 28.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 22620/27699\t Batch loss: 4.182\t Batch perplexity: 65.496\t Batch accuracy: 28.3%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 22640/27699\t Batch loss: 4.126\t Batch perplexity: 61.918\t Batch accuracy: 30.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 22660/27699\t Batch loss: 4.188\t Batch perplexity: 65.922\t Batch accuracy: 28.3%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 22680/27699\t Batch loss: 4.146\t Batch perplexity: 63.181\t Batch accuracy: 27.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 22700/27699\t Batch loss: 4.128\t Batch perplexity: 62.043\t Batch accuracy: 28.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 22720/27699\t Batch loss: 4.256\t Batch perplexity: 70.560\t Batch accuracy: 27.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 22740/27699\t Batch loss: 4.250\t Batch perplexity: 70.074\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.002\t Batch perplexity: 54.715\t Batch accuracy: 28.7%\t***\n",
      "Epoch: 2/2\t Train step: 22760/27699\t Batch loss: 3.979\t Batch perplexity: 53.488\t Batch accuracy: 27.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 22780/27699\t Batch loss: 3.991\t Batch perplexity: 54.109\t Batch accuracy: 28.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 22800/27699\t Batch loss: 4.200\t Batch perplexity: 66.718\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 22820/27699\t Batch loss: 4.203\t Batch perplexity: 66.911\t Batch accuracy: 27.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 22840/27699\t Batch loss: 4.005\t Batch perplexity: 54.867\t Batch accuracy: 28.5%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 22860/27699\t Batch loss: 4.159\t Batch perplexity: 64.005\t Batch accuracy: 29.8%\t 0.77s/batch\n",
      "Epoch: 2/2\t Train step: 22880/27699\t Batch loss: 4.259\t Batch perplexity: 70.753\t Batch accuracy: 27.4%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 22900/27699\t Batch loss: 4.154\t Batch perplexity: 63.705\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 22920/27699\t Batch loss: 4.145\t Batch perplexity: 63.101\t Batch accuracy: 28.7%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 22940/27699\t Batch loss: 4.284\t Batch perplexity: 72.520\t Batch accuracy: 27.2%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 22960/27699\t Batch loss: 4.143\t Batch perplexity: 62.975\t Batch accuracy: 27.7%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 22980/27699\t Batch loss: 4.170\t Batch perplexity: 64.716\t Batch accuracy: 26.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 23000/27699\t Batch loss: 4.108\t Batch perplexity: 60.843\t Batch accuracy: 29.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.079\t Batch perplexity: 59.061\t Batch accuracy: 30.2%\t***\n",
      "Epoch: 2/2\t Train step: 23020/27699\t Batch loss: 4.245\t Batch perplexity: 69.754\t Batch accuracy: 29.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 23040/27699\t Batch loss: 4.109\t Batch perplexity: 60.912\t Batch accuracy: 28.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 23060/27699\t Batch loss: 4.116\t Batch perplexity: 61.331\t Batch accuracy: 29.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 23080/27699\t Batch loss: 4.337\t Batch perplexity: 76.442\t Batch accuracy: 26.8%\t 0.68s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2\t Train step: 23100/27699\t Batch loss: 4.095\t Batch perplexity: 60.014\t Batch accuracy: 29.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 23120/27699\t Batch loss: 4.054\t Batch perplexity: 57.631\t Batch accuracy: 29.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 23140/27699\t Batch loss: 4.132\t Batch perplexity: 62.330\t Batch accuracy: 28.4%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 23160/27699\t Batch loss: 3.931\t Batch perplexity: 50.940\t Batch accuracy: 33.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 23180/27699\t Batch loss: 4.362\t Batch perplexity: 78.440\t Batch accuracy: 26.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 23200/27699\t Batch loss: 4.198\t Batch perplexity: 66.544\t Batch accuracy: 27.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 23220/27699\t Batch loss: 4.000\t Batch perplexity: 54.607\t Batch accuracy: 30.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 23240/27699\t Batch loss: 4.111\t Batch perplexity: 61.033\t Batch accuracy: 28.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.169\t Batch perplexity: 64.672\t Batch accuracy: 27.8%\t***\n",
      "Epoch: 2/2\t Train step: 23260/27699\t Batch loss: 4.360\t Batch perplexity: 78.242\t Batch accuracy: 26.1%\t 0.74s/batch\n",
      "Epoch: 2/2\t Train step: 23280/27699\t Batch loss: 3.903\t Batch perplexity: 49.541\t Batch accuracy: 31.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 23300/27699\t Batch loss: 4.354\t Batch perplexity: 77.775\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 23320/27699\t Batch loss: 4.261\t Batch perplexity: 70.869\t Batch accuracy: 27.6%\t 0.75s/batch\n",
      "Epoch: 2/2\t Train step: 23340/27699\t Batch loss: 4.090\t Batch perplexity: 59.715\t Batch accuracy: 30.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 23360/27699\t Batch loss: 4.261\t Batch perplexity: 70.873\t Batch accuracy: 26.7%\t 0.77s/batch\n",
      "Epoch: 2/2\t Train step: 23380/27699\t Batch loss: 3.963\t Batch perplexity: 52.625\t Batch accuracy: 30.1%\t 0.73s/batch\n",
      "Epoch: 2/2\t Train step: 23400/27699\t Batch loss: 4.103\t Batch perplexity: 60.492\t Batch accuracy: 27.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 23420/27699\t Batch loss: 4.220\t Batch perplexity: 68.053\t Batch accuracy: 28.7%\t 0.79s/batch\n",
      "Epoch: 2/2\t Train step: 23440/27699\t Batch loss: 4.188\t Batch perplexity: 65.907\t Batch accuracy: 28.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 23460/27699\t Batch loss: 3.948\t Batch perplexity: 51.850\t Batch accuracy: 28.9%\t 0.72s/batch\n",
      "Epoch: 2/2\t Train step: 23480/27699\t Batch loss: 4.197\t Batch perplexity: 66.512\t Batch accuracy: 27.2%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 23500/27699\t Batch loss: 4.082\t Batch perplexity: 59.270\t Batch accuracy: 28.7%\t 1.01s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.902\t Batch perplexity: 49.522\t Batch accuracy: 30.1%\t***\n",
      "Epoch: 2/2\t Train step: 23520/27699\t Batch loss: 4.148\t Batch perplexity: 63.324\t Batch accuracy: 30.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 23540/27699\t Batch loss: 4.188\t Batch perplexity: 65.872\t Batch accuracy: 27.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 23560/27699\t Batch loss: 4.199\t Batch perplexity: 66.593\t Batch accuracy: 27.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 23580/27699\t Batch loss: 3.860\t Batch perplexity: 47.470\t Batch accuracy: 31.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 23600/27699\t Batch loss: 4.310\t Batch perplexity: 74.447\t Batch accuracy: 26.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 23620/27699\t Batch loss: 4.004\t Batch perplexity: 54.817\t Batch accuracy: 30.7%\t 0.73s/batch\n",
      "Epoch: 2/2\t Train step: 23640/27699\t Batch loss: 4.165\t Batch perplexity: 64.403\t Batch accuracy: 26.6%\t 0.78s/batch\n",
      "Epoch: 2/2\t Train step: 23660/27699\t Batch loss: 3.878\t Batch perplexity: 48.348\t Batch accuracy: 30.1%\t 0.73s/batch\n",
      "Epoch: 2/2\t Train step: 23680/27699\t Batch loss: 4.233\t Batch perplexity: 68.900\t Batch accuracy: 24.8%\t 0.72s/batch\n",
      "Epoch: 2/2\t Train step: 23700/27699\t Batch loss: 4.080\t Batch perplexity: 59.120\t Batch accuracy: 28.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 23720/27699\t Batch loss: 4.103\t Batch perplexity: 60.522\t Batch accuracy: 28.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 23740/27699\t Batch loss: 3.975\t Batch perplexity: 53.259\t Batch accuracy: 28.8%\t 0.72s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.084\t Batch perplexity: 59.400\t Batch accuracy: 29.5%\t***\n",
      "Epoch: 2/2\t Train step: 23760/27699\t Batch loss: 4.204\t Batch perplexity: 66.967\t Batch accuracy: 28.2%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 23780/27699\t Batch loss: 4.000\t Batch perplexity: 54.607\t Batch accuracy: 31.0%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 23800/27699\t Batch loss: 4.093\t Batch perplexity: 59.907\t Batch accuracy: 28.5%\t 0.76s/batch\n",
      "Epoch: 2/2\t Train step: 23820/27699\t Batch loss: 4.188\t Batch perplexity: 65.874\t Batch accuracy: 29.3%\t 0.76s/batch\n",
      "Epoch: 2/2\t Train step: 23840/27699\t Batch loss: 4.220\t Batch perplexity: 68.051\t Batch accuracy: 27.9%\t 0.78s/batch\n",
      "Epoch: 2/2\t Train step: 23860/27699\t Batch loss: 4.079\t Batch perplexity: 59.100\t Batch accuracy: 28.1%\t 0.74s/batch\n",
      "Epoch: 2/2\t Train step: 23880/27699\t Batch loss: 4.159\t Batch perplexity: 63.998\t Batch accuracy: 27.5%\t 0.75s/batch\n",
      "Epoch: 2/2\t Train step: 23900/27699\t Batch loss: 4.026\t Batch perplexity: 56.054\t Batch accuracy: 29.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 23920/27699\t Batch loss: 4.067\t Batch perplexity: 58.370\t Batch accuracy: 27.0%\t 0.72s/batch\n",
      "Epoch: 2/2\t Train step: 23940/27699\t Batch loss: 4.199\t Batch perplexity: 66.649\t Batch accuracy: 28.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 23960/27699\t Batch loss: 4.109\t Batch perplexity: 60.885\t Batch accuracy: 28.6%\t 0.83s/batch\n",
      "Epoch: 2/2\t Train step: 23980/27699\t Batch loss: 4.239\t Batch perplexity: 69.331\t Batch accuracy: 27.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 24000/27699\t Batch loss: 3.919\t Batch perplexity: 50.350\t Batch accuracy: 29.6%\t 0.70s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.659\t Batch perplexity: 38.807\t Batch accuracy: 33.9%\t***\n",
      "Epoch: 2/2\t Train step: 24020/27699\t Batch loss: 4.153\t Batch perplexity: 63.611\t Batch accuracy: 28.0%\t 0.72s/batch\n",
      "Epoch: 2/2\t Train step: 24040/27699\t Batch loss: 3.847\t Batch perplexity: 46.857\t Batch accuracy: 30.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 24060/27699\t Batch loss: 4.048\t Batch perplexity: 57.275\t Batch accuracy: 29.1%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 24080/27699\t Batch loss: 4.088\t Batch perplexity: 59.596\t Batch accuracy: 27.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 24100/27699\t Batch loss: 4.223\t Batch perplexity: 68.232\t Batch accuracy: 28.7%\t 0.76s/batch\n",
      "Epoch: 2/2\t Train step: 24120/27699\t Batch loss: 3.918\t Batch perplexity: 50.295\t Batch accuracy: 31.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 24140/27699\t Batch loss: 4.224\t Batch perplexity: 68.312\t Batch accuracy: 28.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 24160/27699\t Batch loss: 4.114\t Batch perplexity: 61.183\t Batch accuracy: 27.1%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 24180/27699\t Batch loss: 4.176\t Batch perplexity: 65.133\t Batch accuracy: 27.6%\t 0.73s/batch\n",
      "Epoch: 2/2\t Train step: 24200/27699\t Batch loss: 4.028\t Batch perplexity: 56.141\t Batch accuracy: 29.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 24220/27699\t Batch loss: 4.042\t Batch perplexity: 56.938\t Batch accuracy: 30.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 24240/27699\t Batch loss: 3.996\t Batch perplexity: 54.376\t Batch accuracy: 29.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.943\t Batch perplexity: 51.568\t Batch accuracy: 30.0%\t***\n",
      "Epoch: 2/2\t Train step: 24260/27699\t Batch loss: 4.172\t Batch perplexity: 64.850\t Batch accuracy: 30.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 24280/27699\t Batch loss: 4.173\t Batch perplexity: 64.882\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 24300/27699\t Batch loss: 3.944\t Batch perplexity: 51.603\t Batch accuracy: 29.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 24320/27699\t Batch loss: 4.074\t Batch perplexity: 58.817\t Batch accuracy: 31.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 24340/27699\t Batch loss: 4.126\t Batch perplexity: 61.900\t Batch accuracy: 29.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 24360/27699\t Batch loss: 4.201\t Batch perplexity: 66.730\t Batch accuracy: 28.3%\t 0.72s/batch\n",
      "Epoch: 2/2\t Train step: 24380/27699\t Batch loss: 4.007\t Batch perplexity: 54.998\t Batch accuracy: 27.2%\t 0.72s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2\t Train step: 24400/27699\t Batch loss: 3.979\t Batch perplexity: 53.480\t Batch accuracy: 31.2%\t 0.74s/batch\n",
      "Epoch: 2/2\t Train step: 24420/27699\t Batch loss: 4.139\t Batch perplexity: 62.737\t Batch accuracy: 29.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 24440/27699\t Batch loss: 4.231\t Batch perplexity: 68.766\t Batch accuracy: 29.2%\t 0.74s/batch\n",
      "Epoch: 2/2\t Train step: 24460/27699\t Batch loss: 3.991\t Batch perplexity: 54.088\t Batch accuracy: 29.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 24480/27699\t Batch loss: 4.118\t Batch perplexity: 61.458\t Batch accuracy: 29.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 24500/27699\t Batch loss: 3.931\t Batch perplexity: 50.966\t Batch accuracy: 29.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.076\t Batch perplexity: 58.894\t Batch accuracy: 27.9%\t***\n",
      "Epoch: 2/2\t Train step: 24520/27699\t Batch loss: 4.147\t Batch perplexity: 63.261\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 24540/27699\t Batch loss: 4.143\t Batch perplexity: 62.972\t Batch accuracy: 28.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 24560/27699\t Batch loss: 4.040\t Batch perplexity: 56.818\t Batch accuracy: 28.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 24580/27699\t Batch loss: 4.143\t Batch perplexity: 62.989\t Batch accuracy: 26.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 24600/27699\t Batch loss: 4.002\t Batch perplexity: 54.700\t Batch accuracy: 30.7%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 24620/27699\t Batch loss: 4.187\t Batch perplexity: 65.841\t Batch accuracy: 28.1%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 24640/27699\t Batch loss: 4.154\t Batch perplexity: 63.717\t Batch accuracy: 29.1%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 24660/27699\t Batch loss: 4.025\t Batch perplexity: 55.987\t Batch accuracy: 31.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 24680/27699\t Batch loss: 3.912\t Batch perplexity: 50.010\t Batch accuracy: 31.2%\t 0.84s/batch\n",
      "Epoch: 2/2\t Train step: 24700/27699\t Batch loss: 4.251\t Batch perplexity: 70.195\t Batch accuracy: 27.7%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 24720/27699\t Batch loss: 4.222\t Batch perplexity: 68.137\t Batch accuracy: 28.8%\t 0.74s/batch\n",
      "Epoch: 2/2\t Train step: 24740/27699\t Batch loss: 4.025\t Batch perplexity: 55.953\t Batch accuracy: 28.6%\t 0.79s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.907\t Batch perplexity: 49.764\t Batch accuracy: 31.1%\t***\n",
      "Epoch: 2/2\t Train step: 24760/27699\t Batch loss: 4.053\t Batch perplexity: 57.583\t Batch accuracy: 30.6%\t 0.74s/batch\n",
      "Epoch: 2/2\t Train step: 24780/27699\t Batch loss: 4.271\t Batch perplexity: 71.591\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 24800/27699\t Batch loss: 4.131\t Batch perplexity: 62.209\t Batch accuracy: 29.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 24820/27699\t Batch loss: 4.091\t Batch perplexity: 59.784\t Batch accuracy: 25.9%\t 0.85s/batch\n",
      "Epoch: 2/2\t Train step: 24840/27699\t Batch loss: 4.176\t Batch perplexity: 65.084\t Batch accuracy: 28.0%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 24860/27699\t Batch loss: 4.026\t Batch perplexity: 56.049\t Batch accuracy: 28.5%\t 0.72s/batch\n",
      "Epoch: 2/2\t Train step: 24880/27699\t Batch loss: 4.299\t Batch perplexity: 73.601\t Batch accuracy: 29.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 24900/27699\t Batch loss: 3.949\t Batch perplexity: 51.881\t Batch accuracy: 29.1%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 24920/27699\t Batch loss: 4.265\t Batch perplexity: 71.163\t Batch accuracy: 26.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 24940/27699\t Batch loss: 4.169\t Batch perplexity: 64.653\t Batch accuracy: 27.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 24960/27699\t Batch loss: 3.917\t Batch perplexity: 50.237\t Batch accuracy: 31.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 24980/27699\t Batch loss: 4.155\t Batch perplexity: 63.753\t Batch accuracy: 28.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 25000/27699\t Batch loss: 4.089\t Batch perplexity: 59.662\t Batch accuracy: 30.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.954\t Batch perplexity: 52.134\t Batch accuracy: 28.0%\t***\n",
      "Epoch: 2/2\t Train step: 25020/27699\t Batch loss: 4.046\t Batch perplexity: 57.194\t Batch accuracy: 26.5%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 25040/27699\t Batch loss: 4.091\t Batch perplexity: 59.780\t Batch accuracy: 28.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 25060/27699\t Batch loss: 3.960\t Batch perplexity: 52.439\t Batch accuracy: 30.1%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 25080/27699\t Batch loss: 3.949\t Batch perplexity: 51.890\t Batch accuracy: 27.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 25100/27699\t Batch loss: 4.134\t Batch perplexity: 62.420\t Batch accuracy: 27.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 25120/27699\t Batch loss: 4.063\t Batch perplexity: 58.147\t Batch accuracy: 27.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 25140/27699\t Batch loss: 4.026\t Batch perplexity: 56.064\t Batch accuracy: 30.2%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 25160/27699\t Batch loss: 4.248\t Batch perplexity: 69.970\t Batch accuracy: 26.3%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 25180/27699\t Batch loss: 4.010\t Batch perplexity: 55.158\t Batch accuracy: 30.1%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 25200/27699\t Batch loss: 4.073\t Batch perplexity: 58.739\t Batch accuracy: 28.3%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 25220/27699\t Batch loss: 4.125\t Batch perplexity: 61.864\t Batch accuracy: 27.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 25240/27699\t Batch loss: 4.239\t Batch perplexity: 69.340\t Batch accuracy: 28.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.860\t Batch perplexity: 47.481\t Batch accuracy: 32.6%\t***\n",
      "Epoch: 2/2\t Train step: 25260/27699\t Batch loss: 4.110\t Batch perplexity: 60.959\t Batch accuracy: 28.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 25280/27699\t Batch loss: 4.179\t Batch perplexity: 65.314\t Batch accuracy: 27.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 25300/27699\t Batch loss: 4.109\t Batch perplexity: 60.861\t Batch accuracy: 30.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 25320/27699\t Batch loss: 4.129\t Batch perplexity: 62.114\t Batch accuracy: 26.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 25340/27699\t Batch loss: 4.133\t Batch perplexity: 62.386\t Batch accuracy: 27.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 25360/27699\t Batch loss: 4.020\t Batch perplexity: 55.717\t Batch accuracy: 29.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 25380/27699\t Batch loss: 4.014\t Batch perplexity: 55.367\t Batch accuracy: 29.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 25400/27699\t Batch loss: 4.102\t Batch perplexity: 60.462\t Batch accuracy: 28.3%\t 0.73s/batch\n",
      "Epoch: 2/2\t Train step: 25420/27699\t Batch loss: 3.950\t Batch perplexity: 51.937\t Batch accuracy: 29.9%\t 0.81s/batch\n",
      "Epoch: 2/2\t Train step: 25440/27699\t Batch loss: 4.228\t Batch perplexity: 68.569\t Batch accuracy: 28.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 25460/27699\t Batch loss: 4.158\t Batch perplexity: 63.954\t Batch accuracy: 30.1%\t 0.73s/batch\n",
      "Epoch: 2/2\t Train step: 25480/27699\t Batch loss: 4.063\t Batch perplexity: 58.156\t Batch accuracy: 27.8%\t 0.73s/batch\n",
      "Epoch: 2/2\t Train step: 25500/27699\t Batch loss: 4.047\t Batch perplexity: 57.211\t Batch accuracy: 27.5%\t 0.75s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.002\t Batch perplexity: 54.701\t Batch accuracy: 29.7%\t***\n",
      "Epoch: 2/2\t Train step: 25520/27699\t Batch loss: 3.923\t Batch perplexity: 50.547\t Batch accuracy: 29.4%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 25540/27699\t Batch loss: 4.081\t Batch perplexity: 59.225\t Batch accuracy: 28.3%\t 0.79s/batch\n",
      "Epoch: 2/2\t Train step: 25560/27699\t Batch loss: 4.045\t Batch perplexity: 57.106\t Batch accuracy: 29.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 25580/27699\t Batch loss: 3.809\t Batch perplexity: 45.085\t Batch accuracy: 31.1%\t 0.75s/batch\n",
      "Epoch: 2/2\t Train step: 25600/27699\t Batch loss: 4.153\t Batch perplexity: 63.621\t Batch accuracy: 25.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 25620/27699\t Batch loss: 4.121\t Batch perplexity: 61.640\t Batch accuracy: 29.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 25640/27699\t Batch loss: 3.963\t Batch perplexity: 52.601\t Batch accuracy: 30.7%\t 0.72s/batch\n",
      "Epoch: 2/2\t Train step: 25660/27699\t Batch loss: 4.076\t Batch perplexity: 58.933\t Batch accuracy: 29.3%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 25680/27699\t Batch loss: 3.960\t Batch perplexity: 52.467\t Batch accuracy: 29.1%\t 0.68s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2\t Train step: 25700/27699\t Batch loss: 4.082\t Batch perplexity: 59.286\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 25720/27699\t Batch loss: 3.992\t Batch perplexity: 54.148\t Batch accuracy: 27.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 25740/27699\t Batch loss: 3.896\t Batch perplexity: 49.183\t Batch accuracy: 30.4%\t 0.67s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.235\t Batch perplexity: 69.082\t Batch accuracy: 28.6%\t***\n",
      "Epoch: 2/2\t Train step: 25760/27699\t Batch loss: 4.038\t Batch perplexity: 56.700\t Batch accuracy: 30.5%\t 0.72s/batch\n",
      "Epoch: 2/2\t Train step: 25780/27699\t Batch loss: 4.061\t Batch perplexity: 58.004\t Batch accuracy: 28.7%\t 0.75s/batch\n",
      "Epoch: 2/2\t Train step: 25800/27699\t Batch loss: 4.040\t Batch perplexity: 56.829\t Batch accuracy: 29.8%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 25820/27699\t Batch loss: 4.005\t Batch perplexity: 54.852\t Batch accuracy: 29.5%\t 0.82s/batch\n",
      "Epoch: 2/2\t Train step: 25840/27699\t Batch loss: 4.266\t Batch perplexity: 71.224\t Batch accuracy: 26.0%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 25860/27699\t Batch loss: 4.258\t Batch perplexity: 70.634\t Batch accuracy: 27.1%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 25880/27699\t Batch loss: 3.806\t Batch perplexity: 44.972\t Batch accuracy: 32.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 25900/27699\t Batch loss: 4.272\t Batch perplexity: 71.646\t Batch accuracy: 26.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 25920/27699\t Batch loss: 4.072\t Batch perplexity: 58.654\t Batch accuracy: 26.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 25940/27699\t Batch loss: 4.162\t Batch perplexity: 64.186\t Batch accuracy: 26.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 25960/27699\t Batch loss: 4.309\t Batch perplexity: 74.382\t Batch accuracy: 26.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 25980/27699\t Batch loss: 4.063\t Batch perplexity: 58.127\t Batch accuracy: 31.1%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 26000/27699\t Batch loss: 4.000\t Batch perplexity: 54.616\t Batch accuracy: 30.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.880\t Batch perplexity: 48.439\t Batch accuracy: 29.9%\t***\n",
      "Epoch: 2/2\t Train step: 26020/27699\t Batch loss: 4.041\t Batch perplexity: 56.893\t Batch accuracy: 28.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 26040/27699\t Batch loss: 4.368\t Batch perplexity: 78.875\t Batch accuracy: 27.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 26060/27699\t Batch loss: 4.138\t Batch perplexity: 62.687\t Batch accuracy: 27.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 26080/27699\t Batch loss: 4.168\t Batch perplexity: 64.600\t Batch accuracy: 29.5%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 26100/27699\t Batch loss: 4.232\t Batch perplexity: 68.877\t Batch accuracy: 28.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 26120/27699\t Batch loss: 4.161\t Batch perplexity: 64.137\t Batch accuracy: 27.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 26140/27699\t Batch loss: 4.015\t Batch perplexity: 55.443\t Batch accuracy: 28.0%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 26160/27699\t Batch loss: 3.975\t Batch perplexity: 53.241\t Batch accuracy: 30.7%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 26180/27699\t Batch loss: 4.193\t Batch perplexity: 66.222\t Batch accuracy: 28.9%\t 0.75s/batch\n",
      "Epoch: 2/2\t Train step: 26200/27699\t Batch loss: 4.151\t Batch perplexity: 63.523\t Batch accuracy: 29.6%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 26220/27699\t Batch loss: 4.170\t Batch perplexity: 64.710\t Batch accuracy: 29.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 26240/27699\t Batch loss: 4.308\t Batch perplexity: 74.297\t Batch accuracy: 28.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.979\t Batch perplexity: 53.447\t Batch accuracy: 30.1%\t***\n",
      "Epoch: 2/2\t Train step: 26260/27699\t Batch loss: 4.132\t Batch perplexity: 62.294\t Batch accuracy: 28.8%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 26280/27699\t Batch loss: 4.154\t Batch perplexity: 63.707\t Batch accuracy: 28.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 26300/27699\t Batch loss: 3.976\t Batch perplexity: 53.295\t Batch accuracy: 28.8%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 26320/27699\t Batch loss: 4.063\t Batch perplexity: 58.146\t Batch accuracy: 27.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 26340/27699\t Batch loss: 4.287\t Batch perplexity: 72.714\t Batch accuracy: 26.6%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 26360/27699\t Batch loss: 4.042\t Batch perplexity: 56.947\t Batch accuracy: 27.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 26380/27699\t Batch loss: 4.263\t Batch perplexity: 71.001\t Batch accuracy: 26.1%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 26400/27699\t Batch loss: 4.213\t Batch perplexity: 67.537\t Batch accuracy: 26.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 26420/27699\t Batch loss: 4.080\t Batch perplexity: 59.144\t Batch accuracy: 28.3%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 26440/27699\t Batch loss: 4.142\t Batch perplexity: 62.952\t Batch accuracy: 26.9%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 26460/27699\t Batch loss: 4.125\t Batch perplexity: 61.887\t Batch accuracy: 28.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 26480/27699\t Batch loss: 4.030\t Batch perplexity: 56.241\t Batch accuracy: 29.5%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 26500/27699\t Batch loss: 4.176\t Batch perplexity: 65.084\t Batch accuracy: 28.1%\t 0.73s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.928\t Batch perplexity: 50.803\t Batch accuracy: 29.8%\t***\n",
      "Epoch: 2/2\t Train step: 26520/27699\t Batch loss: 4.092\t Batch perplexity: 59.833\t Batch accuracy: 27.7%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 26540/27699\t Batch loss: 4.192\t Batch perplexity: 66.164\t Batch accuracy: 27.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 26560/27699\t Batch loss: 4.117\t Batch perplexity: 61.375\t Batch accuracy: 29.8%\t 0.72s/batch\n",
      "Epoch: 2/2\t Train step: 26580/27699\t Batch loss: 4.203\t Batch perplexity: 66.916\t Batch accuracy: 28.0%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 26600/27699\t Batch loss: 3.931\t Batch perplexity: 50.972\t Batch accuracy: 29.8%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 26620/27699\t Batch loss: 4.101\t Batch perplexity: 60.388\t Batch accuracy: 28.1%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 26640/27699\t Batch loss: 3.966\t Batch perplexity: 52.763\t Batch accuracy: 31.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 26660/27699\t Batch loss: 4.246\t Batch perplexity: 69.815\t Batch accuracy: 27.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 26680/27699\t Batch loss: 3.992\t Batch perplexity: 54.148\t Batch accuracy: 27.0%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 26700/27699\t Batch loss: 3.854\t Batch perplexity: 47.170\t Batch accuracy: 31.4%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 26720/27699\t Batch loss: 3.889\t Batch perplexity: 48.839\t Batch accuracy: 29.9%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 26740/27699\t Batch loss: 4.276\t Batch perplexity: 71.924\t Batch accuracy: 27.4%\t 0.76s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.924\t Batch perplexity: 50.595\t Batch accuracy: 28.5%\t***\n",
      "Epoch: 2/2\t Train step: 26760/27699\t Batch loss: 4.102\t Batch perplexity: 60.491\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 26780/27699\t Batch loss: 4.083\t Batch perplexity: 59.324\t Batch accuracy: 27.9%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 26800/27699\t Batch loss: 3.984\t Batch perplexity: 53.752\t Batch accuracy: 29.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 26820/27699\t Batch loss: 4.188\t Batch perplexity: 65.893\t Batch accuracy: 27.0%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 26840/27699\t Batch loss: 4.173\t Batch perplexity: 64.888\t Batch accuracy: 26.3%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 26860/27699\t Batch loss: 4.060\t Batch perplexity: 57.999\t Batch accuracy: 28.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 26880/27699\t Batch loss: 4.226\t Batch perplexity: 68.461\t Batch accuracy: 26.3%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 26900/27699\t Batch loss: 4.085\t Batch perplexity: 59.468\t Batch accuracy: 28.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 26920/27699\t Batch loss: 4.130\t Batch perplexity: 62.208\t Batch accuracy: 29.2%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 26940/27699\t Batch loss: 4.063\t Batch perplexity: 58.124\t Batch accuracy: 28.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 26960/27699\t Batch loss: 4.181\t Batch perplexity: 65.460\t Batch accuracy: 29.1%\t 0.72s/batch\n",
      "Epoch: 2/2\t Train step: 26980/27699\t Batch loss: 4.106\t Batch perplexity: 60.703\t Batch accuracy: 28.4%\t 0.68s/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2\t Train step: 27000/27699\t Batch loss: 3.991\t Batch perplexity: 54.083\t Batch accuracy: 30.1%\t 0.67s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 4.008\t Batch perplexity: 55.015\t Batch accuracy: 28.8%\t***\n",
      "Epoch: 2/2\t Train step: 27020/27699\t Batch loss: 4.206\t Batch perplexity: 67.061\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 27040/27699\t Batch loss: 3.952\t Batch perplexity: 52.054\t Batch accuracy: 27.1%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 27060/27699\t Batch loss: 4.211\t Batch perplexity: 67.409\t Batch accuracy: 28.5%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 27080/27699\t Batch loss: 4.164\t Batch perplexity: 64.332\t Batch accuracy: 29.6%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 27100/27699\t Batch loss: 4.166\t Batch perplexity: 64.432\t Batch accuracy: 29.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 27120/27699\t Batch loss: 4.212\t Batch perplexity: 67.499\t Batch accuracy: 28.5%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 27140/27699\t Batch loss: 3.731\t Batch perplexity: 41.703\t Batch accuracy: 31.0%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 27160/27699\t Batch loss: 4.105\t Batch perplexity: 60.613\t Batch accuracy: 30.0%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 27180/27699\t Batch loss: 4.100\t Batch perplexity: 60.317\t Batch accuracy: 27.5%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 27200/27699\t Batch loss: 4.023\t Batch perplexity: 55.861\t Batch accuracy: 27.8%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 27220/27699\t Batch loss: 3.959\t Batch perplexity: 52.417\t Batch accuracy: 32.2%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 27240/27699\t Batch loss: 4.138\t Batch perplexity: 62.670\t Batch accuracy: 29.5%\t 0.71s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.973\t Batch perplexity: 53.139\t Batch accuracy: 31.0%\t***\n",
      "Epoch: 2/2\t Train step: 27260/27699\t Batch loss: 4.016\t Batch perplexity: 55.484\t Batch accuracy: 27.6%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 27280/27699\t Batch loss: 4.209\t Batch perplexity: 67.322\t Batch accuracy: 24.7%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 27300/27699\t Batch loss: 4.046\t Batch perplexity: 57.168\t Batch accuracy: 30.3%\t 0.70s/batch\n",
      "Epoch: 2/2\t Train step: 27320/27699\t Batch loss: 4.297\t Batch perplexity: 73.457\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 27340/27699\t Batch loss: 3.894\t Batch perplexity: 49.117\t Batch accuracy: 31.3%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 27360/27699\t Batch loss: 3.985\t Batch perplexity: 53.799\t Batch accuracy: 30.1%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 27380/27699\t Batch loss: 4.110\t Batch perplexity: 60.917\t Batch accuracy: 27.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 27400/27699\t Batch loss: 4.066\t Batch perplexity: 58.327\t Batch accuracy: 27.9%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 27420/27699\t Batch loss: 4.119\t Batch perplexity: 61.478\t Batch accuracy: 27.0%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 27440/27699\t Batch loss: 4.118\t Batch perplexity: 61.418\t Batch accuracy: 29.7%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 27460/27699\t Batch loss: 4.112\t Batch perplexity: 61.077\t Batch accuracy: 29.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 27480/27699\t Batch loss: 4.151\t Batch perplexity: 63.497\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 27500/27699\t Batch loss: 4.152\t Batch perplexity: 63.549\t Batch accuracy: 28.3%\t 0.67s/batch\n",
      "Epoch: 2/2\t * Validation batch *\t Batch loss: 3.703\t Batch perplexity: 40.552\t Batch accuracy: 33.4%\t***\n",
      "Epoch: 2/2\t Train step: 27520/27699\t Batch loss: 4.091\t Batch perplexity: 59.771\t Batch accuracy: 28.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 27540/27699\t Batch loss: 4.351\t Batch perplexity: 77.547\t Batch accuracy: 27.4%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 27560/27699\t Batch loss: 4.013\t Batch perplexity: 55.327\t Batch accuracy: 28.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 27580/27699\t Batch loss: 3.993\t Batch perplexity: 54.221\t Batch accuracy: 29.2%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 27600/27699\t Batch loss: 4.230\t Batch perplexity: 68.736\t Batch accuracy: 27.8%\t 0.71s/batch\n",
      "Epoch: 2/2\t Train step: 27620/27699\t Batch loss: 4.291\t Batch perplexity: 73.074\t Batch accuracy: 28.3%\t 0.67s/batch\n",
      "Epoch: 2/2\t Train step: 27640/27699\t Batch loss: 4.068\t Batch perplexity: 58.428\t Batch accuracy: 29.3%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 27660/27699\t Batch loss: 4.095\t Batch perplexity: 60.048\t Batch accuracy: 27.5%\t 0.69s/batch\n",
      "Epoch: 2/2\t Train step: 27680/27699\t Batch loss: 4.145\t Batch perplexity: 63.102\t Batch accuracy: 27.7%\t 0.68s/batch\n",
      "Epoch: 2/2\t Train step: 27700/27699\t Batch loss: 4.205\t Batch perplexity: 67.000\t Batch accuracy: 29.3%\t 0.70s/batch\n",
      "*** Experiment C: Training complete:\t2 epoch(s) with 27700 batches each\n",
      "*** Training time:\t 13:02:47.88\n",
      "INFO:tensorflow:Restoring parameters from ./C-checkpoints\\run-18-04-17-09-16-16_i1_s512.ckpt\n",
      "Evaluating sentence 50/9847\n",
      "Evaluating sentence 100/9847\n",
      "Evaluating sentence 150/9847\n",
      "Evaluating sentence 200/9847\n",
      "Evaluating sentence 250/9847\n",
      "Evaluating sentence 300/9847\n",
      "Evaluating sentence 350/9847\n",
      "Evaluating sentence 400/9847\n",
      "Evaluating sentence 450/9847\n",
      "Evaluating sentence 500/9847\n",
      "Evaluating sentence 550/9847\n",
      "Evaluating sentence 600/9847\n",
      "Evaluating sentence 650/9847\n",
      "Evaluating sentence 700/9847\n",
      "Evaluating sentence 750/9847\n",
      "Evaluating sentence 800/9847\n",
      "Evaluating sentence 850/9847\n",
      "Evaluating sentence 900/9847\n",
      "Evaluating sentence 950/9847\n",
      "Evaluating sentence 1000/9847\n",
      "Evaluating sentence 1050/9847\n",
      "Evaluating sentence 1100/9847\n",
      "Evaluating sentence 1150/9847\n",
      "Evaluating sentence 1200/9847\n",
      "Evaluating sentence 1250/9847\n",
      "Evaluating sentence 1300/9847\n",
      "Evaluating sentence 1350/9847\n",
      "Evaluating sentence 1400/9847\n",
      "Evaluating sentence 1450/9847\n",
      "Evaluating sentence 1500/9847\n",
      "Evaluating sentence 1550/9847\n",
      "Evaluating sentence 1600/9847\n",
      "Evaluating sentence 1650/9847\n",
      "Evaluating sentence 1700/9847\n",
      "Evaluating sentence 1750/9847\n",
      "Evaluating sentence 1800/9847\n",
      "Evaluating sentence 1850/9847\n",
      "Evaluating sentence 1900/9847\n",
      "Evaluating sentence 1950/9847\n",
      "Evaluating sentence 2000/9847\n",
      "Evaluating sentence 2050/9847\n",
      "Evaluating sentence 2100/9847\n",
      "Evaluating sentence 2150/9847\n",
      "Evaluating sentence 2200/9847\n",
      "Evaluating sentence 2250/9847\n",
      "Evaluating sentence 2300/9847\n",
      "Evaluating sentence 2350/9847\n",
      "Evaluating sentence 2400/9847\n",
      "Evaluating sentence 2450/9847\n",
      "Evaluating sentence 2500/9847\n",
      "Evaluating sentence 2550/9847\n",
      "Evaluating sentence 2600/9847\n",
      "Evaluating sentence 2650/9847\n",
      "Evaluating sentence 2700/9847\n",
      "Evaluating sentence 2750/9847\n",
      "Evaluating sentence 2800/9847\n",
      "Evaluating sentence 2850/9847\n",
      "Evaluating sentence 2900/9847\n",
      "Evaluating sentence 2950/9847\n",
      "Evaluating sentence 3000/9847\n",
      "Evaluating sentence 3050/9847\n",
      "Evaluating sentence 3100/9847\n",
      "Evaluating sentence 3150/9847\n",
      "Evaluating sentence 3200/9847\n",
      "Evaluating sentence 3250/9847\n",
      "Evaluating sentence 3300/9847\n",
      "Evaluating sentence 3350/9847\n",
      "Evaluating sentence 3400/9847\n",
      "Evaluating sentence 3450/9847\n",
      "Evaluating sentence 3500/9847\n",
      "Evaluating sentence 3550/9847\n",
      "Evaluating sentence 3600/9847\n",
      "Evaluating sentence 3650/9847\n",
      "Evaluating sentence 3700/9847\n",
      "Evaluating sentence 3750/9847\n",
      "Evaluating sentence 3800/9847\n",
      "Evaluating sentence 3850/9847\n",
      "Evaluating sentence 3900/9847\n",
      "Evaluating sentence 3950/9847\n",
      "Evaluating sentence 4000/9847\n",
      "Evaluating sentence 4050/9847\n",
      "Evaluating sentence 4100/9847\n",
      "Evaluating sentence 4150/9847\n",
      "Evaluating sentence 4200/9847\n",
      "Evaluating sentence 4250/9847\n",
      "Evaluating sentence 4300/9847\n",
      "Evaluating sentence 4350/9847\n",
      "Evaluating sentence 4400/9847\n",
      "Evaluating sentence 4450/9847\n",
      "Evaluating sentence 4500/9847\n",
      "Evaluating sentence 4550/9847\n",
      "Evaluating sentence 4600/9847\n",
      "Evaluating sentence 4650/9847\n",
      "Evaluating sentence 4700/9847\n",
      "Evaluating sentence 4750/9847\n",
      "Evaluating sentence 4800/9847\n",
      "Evaluating sentence 4850/9847\n",
      "Evaluating sentence 4900/9847\n",
      "Evaluating sentence 4950/9847\n",
      "Evaluating sentence 5000/9847\n",
      "Evaluating sentence 5050/9847\n",
      "Evaluating sentence 5100/9847\n",
      "Evaluating sentence 5150/9847\n",
      "Evaluating sentence 5200/9847\n",
      "Evaluating sentence 5250/9847\n",
      "Evaluating sentence 5300/9847\n",
      "Evaluating sentence 5350/9847\n",
      "Evaluating sentence 5400/9847\n",
      "Evaluating sentence 5450/9847\n",
      "Evaluating sentence 5500/9847\n",
      "Evaluating sentence 5550/9847\n",
      "Evaluating sentence 5600/9847\n",
      "Evaluating sentence 5650/9847\n",
      "Evaluating sentence 5700/9847\n",
      "Evaluating sentence 5750/9847\n",
      "Evaluating sentence 5800/9847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sentence 5850/9847\n",
      "Evaluating sentence 5900/9847\n",
      "Evaluating sentence 5950/9847\n",
      "Evaluating sentence 6000/9847\n",
      "Evaluating sentence 6050/9847\n",
      "Evaluating sentence 6100/9847\n",
      "Evaluating sentence 6150/9847\n",
      "Evaluating sentence 6200/9847\n",
      "Evaluating sentence 6250/9847\n",
      "Evaluating sentence 6300/9847\n",
      "Evaluating sentence 6350/9847\n",
      "Evaluating sentence 6400/9847\n",
      "Evaluating sentence 6450/9847\n",
      "Evaluating sentence 6500/9847\n",
      "Evaluating sentence 6550/9847\n",
      "Evaluating sentence 6600/9847\n",
      "Evaluating sentence 6650/9847\n",
      "Evaluating sentence 6700/9847\n",
      "Evaluating sentence 6750/9847\n",
      "Evaluating sentence 6800/9847\n",
      "Evaluating sentence 6850/9847\n",
      "Evaluating sentence 6900/9847\n",
      "Evaluating sentence 6950/9847\n",
      "Evaluating sentence 7000/9847\n",
      "Evaluating sentence 7050/9847\n",
      "Evaluating sentence 7100/9847\n",
      "Evaluating sentence 7150/9847\n",
      "Evaluating sentence 7200/9847\n",
      "Evaluating sentence 7250/9847\n",
      "Evaluating sentence 7300/9847\n",
      "Evaluating sentence 7350/9847\n",
      "Evaluating sentence 7400/9847\n",
      "Evaluating sentence 7450/9847\n",
      "Evaluating sentence 7500/9847\n",
      "Evaluating sentence 7550/9847\n",
      "Evaluating sentence 7600/9847\n",
      "Evaluating sentence 7650/9847\n",
      "Evaluating sentence 7700/9847\n",
      "Evaluating sentence 7750/9847\n",
      "Evaluating sentence 7800/9847\n",
      "Evaluating sentence 7850/9847\n",
      "Evaluating sentence 7900/9847\n",
      "Evaluating sentence 7950/9847\n",
      "Evaluating sentence 8000/9847\n",
      "Evaluating sentence 8050/9847\n",
      "Evaluating sentence 8100/9847\n",
      "Evaluating sentence 8150/9847\n",
      "Evaluating sentence 8200/9847\n",
      "Evaluating sentence 8250/9847\n",
      "Evaluating sentence 8300/9847\n",
      "Evaluating sentence 8350/9847\n",
      "Evaluating sentence 8400/9847\n",
      "Evaluating sentence 8450/9847\n",
      "Evaluating sentence 8500/9847\n",
      "Evaluating sentence 8550/9847\n",
      "Evaluating sentence 8600/9847\n",
      "Evaluating sentence 8650/9847\n",
      "Evaluating sentence 8700/9847\n",
      "Evaluating sentence 8750/9847\n",
      "Evaluating sentence 8800/9847\n",
      "Evaluating sentence 8850/9847\n",
      "Evaluating sentence 8900/9847\n",
      "Evaluating sentence 8950/9847\n",
      "Evaluating sentence 9000/9847\n",
      "Evaluating sentence 9050/9847\n",
      "Evaluating sentence 9100/9847\n",
      "Evaluating sentence 9150/9847\n",
      "Evaluating sentence 9200/9847\n",
      "Evaluating sentence 9250/9847\n",
      "Evaluating sentence 9300/9847\n",
      "Evaluating sentence 9350/9847\n",
      "Evaluating sentence 9400/9847\n",
      "Evaluating sentence 9450/9847\n",
      "Evaluating sentence 9500/9847\n",
      "Evaluating sentence 9550/9847\n",
      "Evaluating sentence 9600/9847\n",
      "Evaluating sentence 9650/9847\n",
      "Evaluating sentence 9700/9847\n",
      "Evaluating sentence 9750/9847\n",
      "Evaluating sentence 9800/9847\n",
      "Experiment C: Using ./C-checkpoints\\run-18-04-17-09-16-16_i1_s512.ckpt, evalulation perplexity avg: 90.599, min: 5.153, max: 5356.428\n",
      "9847 perplexity values written to ./outputs/group01.perplexityC\n",
      "INFO:tensorflow:Restoring parameters from ./C-checkpoints\\run-18-04-17-09-16-16_i1_s512.ckpt\n",
      "Generated sentence 50/10001\n",
      "they  ->  they were <unk> , and the <unk> was a <unk> . <eos>\n",
      "Generated sentence 100/10001\n",
      "uccello will  ->  uccello will be a <unk> . <eos>\n",
      "Generated sentence 150/10001\n",
      "i 'll share  ->  i 'll share the <unk> . '' <eos>\n",
      "Generated sentence 200/10001\n",
      "the third ,  ->  the third , and the <unk> was a <unk> . <eos>\n",
      "Generated sentence 250/10001\n",
      "`` are you  ->  `` are you , <unk> . <eos>\n",
      "Generated sentence 300/10001\n",
      "while her  ->  while her , she was a <unk> , and she was n't a <unk> . <eos>\n",
      "Generated sentence 350/10001\n",
      "acting like he  ->  acting like he was a good guy . <eos>\n",
      "Generated sentence 400/10001\n",
      "anne paused , unsure  ->  anne paused , unsure what to say . <eos>\n",
      "Generated sentence 450/10001\n",
      "i  ->  i was a <unk> , but i was n't sure what to say . <eos>\n",
      "Generated sentence 500/10001\n",
      "the room  ->  the room i <unk> the <unk> , and i was n't sure what to say . <eos>\n",
      "Generated sentence 550/10001\n",
      "well break that stubborn  ->  well break that stubborn <unk> . <eos>\n",
      "Generated sentence 600/10001\n",
      "lahn joined  ->  lahn joined the <unk> , and the <unk> was a <unk> . <eos>\n",
      "Generated sentence 650/10001\n",
      "i asked , although  ->  i asked , although i was n't sure what to say . <eos>\n",
      "Generated sentence 700/10001\n",
      "i ca n't cut  ->  i ca n't cut it . '' <eos>\n",
      "Generated sentence 750/10001\n",
      "``  ->  `` , i said . <eos>\n",
      "Generated sentence 800/10001\n",
      "`` i will  ->  `` i will you ? '' <eos>\n",
      "Generated sentence 850/10001\n",
      "`` i 'll  ->  `` i 'll you ? '' <eos>\n",
      "Generated sentence 900/10001\n",
      "angl : look  ->  angl : look , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk>\n",
      "Generated sentence 950/10001\n",
      "`` i 'm  ->  `` i 'm a <unk> . '' <eos>\n",
      "Generated sentence 1000/10001\n",
      "``  ->  `` , i said . <eos>\n",
      "Generated sentence 1050/10001\n",
      "if a  ->  if a <unk> , i would n't have been able to get out of the way . <eos>\n",
      "Generated sentence 1100/10001\n",
      "mesmerized by the crackling  ->  mesmerized by the crackling , she was n't sure what to say . <eos>\n",
      "Generated sentence 1150/10001\n",
      "`` first  ->  `` first , '' he said . <eos>\n",
      "Generated sentence 1200/10001\n",
      "trevize sat  ->  trevize sat there , his <unk> <unk> . <eos>\n",
      "Generated sentence 1250/10001\n",
      "she tried to  ->  she tried to <unk> the <unk> , but she was n't sure what to say . <eos>\n",
      "Generated sentence 1300/10001\n",
      "why  ->  why ? '' <eos>\n",
      "Generated sentence 1350/10001\n",
      "she  ->  she was a <unk> , and she was a <unk> . <eos>\n",
      "Generated sentence 1400/10001\n",
      "i think i  ->  i think i <unk> the <unk> . <eos>\n",
      "Generated sentence 1450/10001\n",
      "she had wormed  ->  she had wormed <unk> , and she was n't a <unk> . <eos>\n",
      "Generated sentence 1500/10001\n",
      "alex swings his car  ->  alex swings his car . <eos>\n",
      "Generated sentence 1550/10001\n",
      "she smiled an evil  ->  she smiled an evil , and then she turned to the <unk> . <eos>\n",
      "Generated sentence 1600/10001\n",
      "she  ->  she was a <unk> , and she was a <unk> . <eos>\n",
      "Generated sentence 1650/10001\n",
      "i do  ->  i do n't know what i 'm doing . '' <eos>\n",
      "Generated sentence 1700/10001\n",
      "in this  ->  in this , the <unk> of the <unk> . <eos>\n",
      "Generated sentence 1750/10001\n",
      "so i hiked back  ->  so i hiked back to the <unk> , and i was n't sure what to say . <eos>\n",
      "Generated sentence 1800/10001\n",
      "`` would  ->  `` would n't . <eos>\n",
      "Generated sentence 1850/10001\n",
      "neagley said  ->  neagley said as he turned to the <unk> . <eos>\n",
      "Generated sentence 1900/10001\n",
      "`` if  ->  `` if you do n't want to go back to the house . '' <eos>\n",
      "Generated sentence 1950/10001\n",
      "i was  ->  i was a <unk> , but i was n't sure what to say . <eos>\n",
      "Generated sentence 2000/10001\n",
      "``  ->  `` , i said . <eos>\n",
      "Generated sentence 2050/10001\n",
      "and something about that  ->  and something about that . <eos>\n",
      "Generated sentence 2100/10001\n",
      "i 'd seen similar  ->  i 'd seen similar to the <unk> . <eos>\n",
      "Generated sentence 2150/10001\n",
      "you should take  ->  you should take a long time . '' <eos>\n",
      "Generated sentence 2200/10001\n",
      "it was dark ,  ->  it was dark , and the <unk> was a <unk> . <eos>\n",
      "Generated sentence 2250/10001\n",
      "it seems as though  ->  it seems as though it is a <unk> . <eos>\n",
      "Generated sentence 2300/10001\n",
      "it was just  ->  it was just a <unk> . <eos>\n",
      "Generated sentence 2350/10001\n",
      "sounds  ->  sounds like a <unk> . '' <eos>\n",
      "Generated sentence 2400/10001\n",
      "now , at  ->  now , at least , i was n't sure what to say . <eos>\n",
      "Generated sentence 2450/10001\n",
      "i will give  ->  i will give you a chance to get back . '' <eos>\n",
      "Generated sentence 2500/10001\n",
      "they threw me  ->  they threw me and i was a <unk> . <eos>\n",
      "Generated sentence 2550/10001\n",
      "`` jed  ->  `` jed , '' he said . <eos>\n",
      "Generated sentence 2600/10001\n",
      "``  ->  `` , i said . <eos>\n",
      "Generated sentence 2650/10001\n",
      "his face  ->  his face <unk> the <unk> . <eos>\n",
      "Generated sentence 2700/10001\n",
      "their cloaks were  ->  their cloaks were <unk> . <eos>\n",
      "Generated sentence 2750/10001\n",
      "let  ->  let 's get some sleep . '' <eos>\n",
      "Generated sentence 2800/10001\n",
      "i laughed and put  ->  i laughed and put my hand on the doorknob . <eos>\n",
      "Generated sentence 2850/10001\n",
      "`` but that  ->  `` but that 's a good idea . '' <eos>\n",
      "Generated sentence 2900/10001\n",
      "why didnt the pharmacist  ->  why didnt the pharmacist have a <unk> ? <eos>\n",
      "Generated sentence 2950/10001\n",
      "he threw  ->  he threw his <unk> , and he was a <unk> . <eos>\n",
      "Generated sentence 3000/10001\n",
      "jamie  ->  jamie was a <unk> , but he was n't a <unk> . <eos>\n",
      "Generated sentence 3050/10001\n",
      "he  ->  he was a <unk> , and he was a <unk> . <eos>\n",
      "Generated sentence 3100/10001\n",
      "he grinned at  ->  he grinned at me . <eos>\n",
      "Generated sentence 3150/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxx  ->  maxx , <unk> , <unk> , and <unk> . <eos>\n",
      "Generated sentence 3200/10001\n",
      "`` hector said  ->  `` hector said you were a <unk> . '' <eos>\n",
      "Generated sentence 3250/10001\n",
      "despite my  ->  despite my , i was n't a bad guy . <eos>\n",
      "Generated sentence 3300/10001\n",
      "i am amazed that  ->  i am amazed that you are not <unk> . '' <eos>\n",
      "Generated sentence 3350/10001\n",
      "dignified ,  ->  dignified , <unk> . <eos>\n",
      "Generated sentence 3400/10001\n",
      "voices cold , their  ->  voices cold , their <unk> <unk> , and the <unk> of the <unk> . <eos>\n",
      "Generated sentence 3450/10001\n",
      "`` no ,  ->  `` no , '' she said . <eos>\n",
      "Generated sentence 3500/10001\n",
      "she  ->  she was a <unk> , and she was a <unk> . <eos>\n",
      "Generated sentence 3550/10001\n",
      "'it 's like they  ->  'it 's like they 're a <unk> . ' <eos>\n",
      "Generated sentence 3600/10001\n",
      "bagel lay  ->  bagel lay a <unk> , and he was a <unk> . <eos>\n",
      "Generated sentence 3650/10001\n",
      "he was still  ->  he was still a <unk> , but he was n't a <unk> . <eos>\n",
      "Generated sentence 3700/10001\n",
      "im sorry  ->  im sorry . <eos>\n",
      "Generated sentence 3750/10001\n",
      "smooth enough for  ->  smooth enough for the <unk> . <eos>\n",
      "Generated sentence 3800/10001\n",
      "he turned  ->  he turned and said , `` i 'm not sure i 'm going to be able to get you out of here\n",
      "Generated sentence 3850/10001\n",
      "her bed was  ->  her bed was a <unk> , and she was n't a <unk> . <eos>\n",
      "Generated sentence 3900/10001\n",
      "`` you  ->  `` you , '' she said . <eos>\n",
      "Generated sentence 3950/10001\n",
      "he blew first into  ->  he blew first into the <unk> . <eos>\n",
      "Generated sentence 4000/10001\n",
      "`` in fact  ->  `` in fact , i 'm not sure i 'm going to be able to get you out of here . '' <eos>\n",
      "Generated sentence 4050/10001\n",
      "is  ->  is n't you ? '' <eos>\n",
      "Generated sentence 4100/10001\n",
      "the child  ->  the child , <unk> , and <unk> . <eos>\n",
      "Generated sentence 4150/10001\n",
      "a wicked spark  ->  a wicked spark . <eos>\n",
      "Generated sentence 4200/10001\n",
      "so  ->  so , i was a <unk> . <eos>\n",
      "Generated sentence 4250/10001\n",
      "if  ->  if you want to get out of here , you 'll have to get out of here . '' <eos>\n",
      "Generated sentence 4300/10001\n",
      "he  ->  he was a <unk> , and he was a <unk> . <eos>\n",
      "Generated sentence 4350/10001\n",
      "inside the bubble  ->  inside the bubble , the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "Generated sentence 4400/10001\n",
      "ill  ->  ill , <unk> <unk> . <eos>\n",
      "Generated sentence 4450/10001\n",
      "he wanted  ->  he wanted to <unk> the <unk> , but he was n't sure what to say . <eos>\n",
      "Generated sentence 4500/10001\n",
      "he lands a  ->  he lands a <unk> , <unk> , and <unk> . <eos>\n",
      "Generated sentence 4550/10001\n",
      "hollered one of the  ->  hollered one of the <unk> . <eos>\n",
      "Generated sentence 4600/10001\n",
      "i  ->  i was a <unk> , but i was n't sure what to say . <eos>\n",
      "Generated sentence 4650/10001\n",
      "i  ->  i was a <unk> , but i was n't sure what to say . <eos>\n",
      "Generated sentence 4700/10001\n",
      "lhan had already  ->  lhan had already been a <unk> , but he was n't a <unk> . <eos>\n",
      "Generated sentence 4750/10001\n",
      "you have to promise  ->  you have to promise you you are n't . '' <eos>\n",
      "Generated sentence 4800/10001\n",
      "compscan and surface telemetry  ->  compscan and surface telemetry . <eos>\n",
      "Generated sentence 4850/10001\n",
      "i can feel  ->  i can feel the <unk> . <eos>\n",
      "Generated sentence 4900/10001\n",
      "he runs  ->  he runs his <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> . <eos>\n",
      "Generated sentence 4950/10001\n",
      "``  ->  `` , i said . <eos>\n",
      "Generated sentence 5000/10001\n",
      "`` probably , ''  ->  `` probably , '' she said . <eos>\n",
      "Generated sentence 5050/10001\n",
      "because i 'm forty-five and  ->  because i 'm forty-five and i 'm not going to be able to get out of here . '' <eos>\n",
      "Generated sentence 5100/10001\n",
      "'ehlana  ->  'ehlana , <unk> , <unk> , and <unk> . <eos>\n",
      "Generated sentence 5150/10001\n",
      "i picked up my  ->  i picked up my <unk> , and i was n't sure what to say . <eos>\n",
      "Generated sentence 5200/10001\n",
      "`` i almost think i 'd rather be interrogated by  ->  `` i almost think i 'd rather be interrogated by the <unk> . '' <eos>\n",
      "Generated sentence 5250/10001\n",
      "am i going  ->  am i going to be a <unk> ? '' <eos>\n",
      "Generated sentence 5300/10001\n",
      "i grabbed a glass and  ->  i grabbed a glass and then looked at the <unk> . <eos>\n",
      "Generated sentence 5350/10001\n",
      "``  ->  `` , i said . <eos>\n",
      "Generated sentence 5400/10001\n",
      "and no one i sent would 've learned what  ->  and no one i sent would 've learned what i 'd done . <eos>\n",
      "Generated sentence 5450/10001\n",
      "``  ->  `` , i said . <eos>\n",
      "Generated sentence 5500/10001\n",
      "and the youngster ... how many friends had  ->  and the youngster ... how many friends had you been ? '' <eos>\n",
      "Generated sentence 5550/10001\n",
      "that thing must have weighed five hundred pounds  ->  that thing must have weighed five hundred pounds . <eos>\n",
      "Generated sentence 5600/10001\n",
      "the  ->  the <unk> , the <unk> , and the <unk> of the <unk> . <eos>\n",
      "Generated sentence 5650/10001\n",
      "downworlders , mundanes- '' `` my mother 's marrying a downworlder  ->  downworlders , mundanes- '' `` my mother 's marrying a downworlder . '' <eos>\n",
      "Generated sentence 5700/10001\n",
      "she glanced over her shoulder relieved to  ->  she glanced over her shoulder relieved to see the <unk> <unk> . <eos>\n",
      "Generated sentence 5750/10001\n",
      "it is very curious how quickly they resort to myth and  ->  it is very curious how quickly they resort to myth and <unk> . <eos>\n",
      "Generated sentence 5800/10001\n",
      "i had n't had time to date while my mother was sick  ->  i had n't had time to date while my mother was sick . <eos>\n",
      "Generated sentence 5850/10001\n",
      "``  ->  `` , i said . <eos>\n",
      "Generated sentence 5900/10001\n",
      "jack  ->  jack was a <unk> , but he was n't a <unk> . <eos>\n",
      "Generated sentence 5950/10001\n",
      "i  ->  i was a <unk> , but i was n't sure what to say . <eos>\n",
      "Generated sentence 6000/10001\n",
      "when he  ->  when he <unk> the <unk> , the <unk> was a <unk> . <eos>\n",
      "Generated sentence 6050/10001\n",
      "how  ->  how could i have been so happy ? <eos>\n",
      "Generated sentence 6100/10001\n",
      "it 's like my mind 's been scrambled ;  ->  it 's like my mind 's been scrambled ; i 'm not sure i 'm not . '' <eos>\n",
      "Generated sentence 6150/10001\n",
      "therefore , the king found himself entangled in  ->  therefore , the king found himself entangled in the <unk> , and the <unk> was a <unk> . <eos>\n",
      "Generated sentence 6200/10001\n",
      "`` so far it has n't  ->  `` so far it has n't been a long time . '' <eos>\n",
      "Generated sentence 6250/10001\n",
      "`` that  ->  `` that , '' she said . <eos>\n",
      "Generated sentence 6300/10001\n",
      "although , i knew my father truly  ->  although , i knew my father truly had a good idea . <eos>\n",
      "Generated sentence 6350/10001\n",
      "before  ->  before , i was a <unk> . <eos>\n",
      "Generated sentence 6400/10001\n",
      "he appealed to the electorate to deliver their presence and their clamour  ->  he appealed to the electorate to deliver their presence and their clamour . <eos>\n",
      "Generated sentence 6450/10001\n",
      "somehow , after he believed that dorthia deserved a better end  ->  somehow , after he believed that dorthia deserved a better end , he was a <unk> . <eos>\n",
      "Generated sentence 6500/10001\n",
      "we  ->  we 're <unk> . '' <eos>\n",
      "Generated sentence 6550/10001\n",
      "did n't your mother ever teach  ->  did n't your mother ever teach me what happened ? '' <eos>\n",
      "Generated sentence 6600/10001\n",
      "the three shrugged  ->  the three shrugged . <eos>\n",
      "Generated sentence 6650/10001\n",
      "it played the theme  ->  it played the theme was a <unk> , but it was a <unk> . <eos>\n",
      "Generated sentence 6700/10001\n",
      "but not  ->  but not , i was n't sure what to say . <eos>\n",
      "Generated sentence 6750/10001\n",
      "she 'd have to pay  ->  she 'd have to pay for the <unk> . <eos>\n",
      "Generated sentence 6800/10001\n",
      "i see a counselor , dr. paltron , every week , and  ->  i see a counselor , dr. paltron , every week , and i have to go to the <unk> . <eos>\n",
      "Generated sentence 6850/10001\n",
      "`` nothing  ->  `` nothing . <eos>\n",
      "Generated sentence 6900/10001\n",
      "there were a few cars on the top  ->  there were a few cars on the top of the road . <eos>\n",
      "Generated sentence 6950/10001\n",
      "i knew  ->  i knew i had a <unk> . <eos>\n",
      "Generated sentence 7000/10001\n",
      "``  ->  `` , i said . <eos>\n",
      "Generated sentence 7050/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 's men who think something as inconsequential as  ->  it 's men who think something as inconsequential as the <unk> . <eos>\n",
      "Generated sentence 7100/10001\n",
      "of course , we might  ->  of course , we might have to be a <unk> . '' <eos>\n",
      "Generated sentence 7150/10001\n",
      "we were in the air before madigan could sputter out  ->  we were in the air before madigan could sputter out the <unk> . <eos>\n",
      "Generated sentence 7200/10001\n",
      "i  ->  i was a <unk> , but i was n't sure what to say . <eos>\n",
      "Generated sentence 7250/10001\n",
      "linnet frowned at us and then at the creased  ->  linnet frowned at us and then at the creased <unk> . <eos>\n",
      "Generated sentence 7300/10001\n",
      "`` yeah , i did , raul , '' i say  ->  `` yeah , i did , raul , '' i say . <eos>\n",
      "Generated sentence 7350/10001\n",
      "the goth leader looked around now  ->  the goth leader looked around now . <eos>\n",
      "Generated sentence 7400/10001\n",
      "`` but  ->  `` but i do n't know what you 're doing . '' <eos>\n",
      "Generated sentence 7450/10001\n",
      "``  ->  `` , i said . <eos>\n",
      "Generated sentence 7500/10001\n",
      "jack clamped a hand over his little sister  ->  jack clamped a hand over his little sister . <eos>\n",
      "Generated sentence 7550/10001\n",
      "i 'm awesome at lying to everyone  ->  i 'm awesome at lying to everyone . '' <eos>\n",
      "Generated sentence 7600/10001\n",
      "the relief she 'd felt upon first  ->  the relief she 'd felt upon first . <eos>\n",
      "Generated sentence 7650/10001\n",
      "it had been an hour-long ride and my legs felt like they  ->  it had been an hour-long ride and my legs felt like they were going to be a <unk> . <eos>\n",
      "Generated sentence 7700/10001\n",
      "dufour , set up our tent and make sure  ->  dufour , set up our tent and make sure you 're not . '' <eos>\n",
      "Generated sentence 7750/10001\n",
      "`` of course  ->  `` of course , '' she said . <eos>\n",
      "Generated sentence 7800/10001\n",
      "she covered his hand with a  ->  she covered his hand with a <unk> . <eos>\n",
      "Generated sentence 7850/10001\n",
      "a ticker read : wanted-criminal  ->  a ticker read : wanted-criminal , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk>\n",
      "Generated sentence 7900/10001\n",
      "every sailor quietly sucks  ->  every sailor quietly sucks . <eos>\n",
      "Generated sentence 7950/10001\n",
      "``  ->  `` , i said . <eos>\n",
      "Generated sentence 8000/10001\n",
      "he  ->  he was a <unk> , and he was a <unk> . <eos>\n",
      "Generated sentence 8050/10001\n",
      "a number of  ->  a number of <unk> <unk> <unk> <unk> . <eos>\n",
      "Generated sentence 8100/10001\n",
      "'well  ->  'well , <unk> , ' said <unk> . <eos>\n",
      "Generated sentence 8150/10001\n",
      "few of us live like finlay- indulging  ->  few of us live like finlay- indulging a <unk> . <eos>\n",
      "Generated sentence 8200/10001\n",
      "he stopped there ,  ->  he stopped there , his <unk> <unk> , and his <unk> . <eos>\n",
      "Generated sentence 8250/10001\n",
      "my stomachs in my  ->  my stomachs in my room . <eos>\n",
      "Generated sentence 8300/10001\n",
      "`` yes , you can ,  ->  `` yes , you can , '' he said . <eos>\n",
      "Generated sentence 8350/10001\n",
      "this is my career  ->  this is my career . <eos>\n",
      "Generated sentence 8400/10001\n",
      "`` in candor , '' says al , nudging  ->  `` in candor , '' says al , nudging the <unk> . <eos>\n",
      "Generated sentence 8450/10001\n",
      "lights burned in the front window ,  ->  lights burned in the front window , the <unk> of the <unk> . <eos>\n",
      "Generated sentence 8500/10001\n",
      "when emma got up and excused herself , i made my  ->  when emma got up and excused herself , i made my way to the bathroom . <eos>\n",
      "Generated sentence 8550/10001\n",
      "`` i  ->  `` i , '' she said . <eos>\n",
      "Generated sentence 8600/10001\n",
      "i 'm going to have to move in with you until that  ->  i 'm going to have to move in with you until that 's what you 're doing . '' <eos>\n",
      "Generated sentence 8650/10001\n",
      "``  ->  `` , i said . <eos>\n",
      "Generated sentence 8700/10001\n",
      "the building is  ->  the building is a <unk> , and i 'm not sure i 'm not . <eos>\n",
      "Generated sentence 8750/10001\n",
      "`` that was a projection  ->  `` that was a projection . <eos>\n",
      "Generated sentence 8800/10001\n",
      "and the thoughts , the endless thoughts : it was easy to  ->  and the thoughts , the endless thoughts : it was easy to see . <eos>\n",
      "Generated sentence 8850/10001\n",
      "i nodded into  ->  i nodded into the <unk> . <eos>\n",
      "Generated sentence 8900/10001\n",
      "it was frozen on the stream for 610 am  ->  it was frozen on the stream for 610 am . <eos>\n",
      "Generated sentence 8950/10001\n",
      "she  ->  she was a <unk> , and she was a <unk> . <eos>\n",
      "Generated sentence 9000/10001\n",
      "i took his hand and said , ``  ->  i took his hand and said , `` i 'm not sure i 'm going to be able to get you out of here . '' <eos>\n",
      "Generated sentence 9050/10001\n",
      "it was her fault tom  ->  it was her fault tom had a <unk> . <eos>\n",
      "Generated sentence 9100/10001\n",
      "there was hardly  ->  there was hardly a long time . <eos>\n",
      "Generated sentence 9150/10001\n",
      "occasionally ,  ->  occasionally , i was a <unk> . <eos>\n",
      "Generated sentence 9200/10001\n",
      "madame  ->  madame <unk> the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "Generated sentence 9250/10001\n",
      "she is such a tease galeren , but  ->  she is such a tease galeren , but i 'm not sure what to say . <eos>\n",
      "Generated sentence 9300/10001\n",
      "he 'd promised to look into it  ->  he 'd promised to look into it . <eos>\n",
      "Generated sentence 9350/10001\n",
      "*** trace stood in his office , his gaze on the city  ->  *** trace stood in his office , his gaze on the city . <eos>\n",
      "Generated sentence 9400/10001\n",
      "wyatt stood up and turned  ->  wyatt stood up and turned to face the <unk> . <eos>\n",
      "Generated sentence 9450/10001\n",
      "on so  ->  on so , the <unk> was a <unk> . <eos>\n",
      "Generated sentence 9500/10001\n",
      "`` we have chasm duty  ->  `` we have chasm duty . '' <eos>\n",
      "Generated sentence 9550/10001\n",
      "`` natalie  ->  `` natalie , you 're not a <unk> . '' <eos>\n",
      "Generated sentence 9600/10001\n",
      "she put her camera  ->  she put her camera , and the <unk> was a <unk> . <eos>\n",
      "Generated sentence 9650/10001\n",
      "when he 'd toweled off and turned  ->  when he 'd toweled off and turned to the <unk> , he was still standing in front of him . <eos>\n",
      "Generated sentence 9700/10001\n",
      "we ca n't talk about any of this  ->  we ca n't talk about any of this . '' <eos>\n",
      "Generated sentence 9750/10001\n",
      "he cut  ->  he cut his <unk> , and he was a <unk> . <eos>\n",
      "Generated sentence 9800/10001\n",
      "`` there 's a  ->  `` there 's a <unk> , '' she said . <eos>\n",
      "Generated sentence 9850/10001\n",
      "the one you  ->  the one you were a <unk> . '' <eos>\n",
      "Generated sentence 9900/10001\n",
      "it kept  ->  it kept the <unk> , and the <unk> , the <unk> , the <unk> . <eos>\n",
      "Generated sentence 9950/10001\n",
      "the sprinklers  ->  the sprinklers , the <unk> , and the <unk> of the <unk> . <eos>\n",
      "Generated sentence 10000/10001\n",
      "well then , he said raising  ->  well then , he said raising a nod . <eos>\n",
      "Using ./C-checkpoints :\n",
      "10001 continued sentences written to ./outputs/group01.continuation\n"
     ]
    }
   ],
   "source": [
    "# Ready, set, ...\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
