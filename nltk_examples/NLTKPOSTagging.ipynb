{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of Speech Tagging with NLTK\n",
    "[Michael Elhadad NLP 2017](http://www.cs.bgu.ac.il/~elhadad/nlp17.html)\n",
    "\n",
    "Last Modified 21 Nov 16\n",
    "\n",
    "[Download as Jupyter Notebook](http://www.cs.bgu.ac.il/~elhadad/nlp17/NLTKPOSTagging.ipynb)\n",
    "\n",
    "## Definition of the Task\n",
    "\n",
    "One of the most basic and most useful task when processing text is to tokenize each word separately \n",
    "and label each word according to its most likely part of speech. \n",
    "This task is called _part of speech tagging_ (POST).\n",
    "\n",
    "Refer to the [Wikipedia](http://en.wikipedia.org/wiki/Part-of-speech_tagging) presentation for a short definition of the task of parts of speech tagging.\n",
    "\n",
    "We will follow the step by step description introduced in [Chapter 5](http://www.nltk.org/book/ch05.html) of the \n",
    "[Natural Language Processing with Python --- Analyzing Text with the Natural Language Toolkit](http://www.nltk.org/book) book by Steven Bird, Ewan Klein, and Edward Loper (2009).\n",
    "\n",
    "This task will give us the opportunity to first address methods of statistical analysis and of machine learning. Machine learning creates computational tools that generalize data viewed over a collection of examples. In the case of POST, an influential dataset that is used in many studies was published in 1967, known as the [Brown Corpus](http://en.wikipedia.org/wiki/Brown_Corpus). The Brown Corpus defined a tagset (specific collection of part-of-speech labels) that has been reused in many other annotated resources in English. \n",
    "\n",
    "More recently, a different version of the tagset has been defined which is \n",
    "called the [Universal Parts of Speech Tagset](http://universaldependencies.org/u/pos/).  \n",
    "One of the objectives of the universal tagset is to define a tagset that can be used uniformly across multiple languages.\n",
    "We will use this tagset in this empirical exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing NLTK\n",
    "\n",
    "[NLTK](http://www.nltk.org/) (Natural Language ToolKit) is a collection of open source Python modules, linguistic data and\n",
    "documentation for research and development in natural language processing.It provides excellent combination of hands-on access to data, explanation and real-life data.\n",
    "\n",
    "To install NLTK on your machine, follow these [instructions](http://www.nltk.org/install.html).\n",
    "\n",
    "If you installed Python using Anaconda, NLTK comes already installed.\n",
    "\n",
    "Make sure to also install the datasets that come with NLTK as explained [here](http://www.nltk.org/data.html).\n",
    "From the command line, type:\n",
    "```\n",
    "python -m nltk.downloader all\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The FreqDist NLTK Object: Counting Things\n",
    "\n",
    "One of the most useful thing to do on corpora is to count things: \n",
    "count words, count words with specific properties, count sequences, count tags. NLTK defines a useful class for this: nltk.FreqDist. \n",
    "(There is a closely related class in standard Python called Counter.)\n",
    "\n",
    "FreqDist (Frequency Distribution) is an object that counts occurrences of objects.\n",
    "It is defined in the nltk.probability module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'a': 2, 'b': 1})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "list = ['a', 'b', 'a']\n",
    "fdist = FreqDist(list)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the FreqDist to explore the distribution of objects in the stream 'list':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist['a']  # We saw 'a' 2 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist['b']  # We saw 'b' 1 time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist['c']  # We saw 'c' 0 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.max()      # 'a' is the most frequent sample seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fdist)      # we observed 2 types of objects in the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['a', 'b'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.keys()     # Return the types of the objects that were observed in the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.freq('a')  # 2/3 of the samples we saw were 'a' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.N()        # How many samples did we count?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on the Brown Corpus with NLTK\n",
    "\n",
    "NLTK contains a collection of tagged corpora, arranged as convenient Python objects. We will use the Brown corpus in this experiment.  The tagged_sents version of the corpus is a list of sentences. Each sentence is a list of pairs (tuples) (word, tag). Similarly, one can access the corpus as a flat list of tagged words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_news_tagged = brown.tagged_sents(categories='news', tagset='universal')\n",
    "brown_news_words = brown.tagged_words(categories='news',  tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now check: which word is the most common among the about 100,000 words in this part of the Brown corpus? Which tag is the most common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100554"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdistw = FreqDist([w for (w, t) in brown_news_words])\n",
    "fdistw.N()    # We saw 100,554 words in this section of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14394"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fdistw)  # How many distinct words are there|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdistw.max()  # What is the most frequent word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5580"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdistw['the']   # How often does 'the' occur in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5.55%\n"
     ]
    }
   ],
   "source": [
    "print('%5.2f%%' % (fdistw.freq('the') * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the distribution of word tokens to word types is extremely unbalanced - a single word (_the_) accounts for over 6% of the word tokens.  This is a general observation of linguistic data -- known as _Zipf's Law_ -- few types are extremely frequent, and many types are extremely rare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinguishing Word Type and Work Token \n",
    "\n",
    "When distinguishing _word type_ and _word token_, we can decide to consider that strings that vary only because of case variants correspond to the same _word type_ - for example, the token _Book_ and _book_ correspond to the same word type _book_ (and so do _bOOk_ and _bOok_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let us count the words without distinction of upper/lower case and the tags\n",
    "fdistwl = FreqDist([w.lower() for (w, t) in brown_news_words])\n",
    "fdistt = FreqDist([t for (w, t) in brown_news_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we ignore case variants, there are only 13,112 word types instead of 14,394 when case differences are kept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13112"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fdistwl)  # How many distinct words when we ignore case distinctions ('the' same as 'The')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.49%\n"
     ]
    }
   ],
   "source": [
    "print('%5.2f%%' % (fdistt.freq('NOUN') * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity of the POST task\n",
    "\n",
    "The first question we address about the task of POS tagging is: how complex is it?\n",
    "\n",
    "One way to quantify the complexity of a task is to measure its [perplexity](http://en.wikipedia.org/wiki/Perplexity). \n",
    "The intuitive meaning of perplexity is: when the tagger is considering which tag to associate to a word, how \"confused\" is it? That is, how many options does it have to choose from?\n",
    "\n",
    "Obviously, this depends on the number of tags in the tagset. The [universal tagset](http://universaldependencies.org/u/pos/) includes 17 tags:\n",
    "\n",
    "Tag\t| Meaning\t | Examples\n",
    "----|------------|----------\n",
    "ADJ\t| adjective\t | new, good, high, special, big, local\n",
    "ADV\t| adverb\t | really, already, still, early, now\n",
    "CONJ| conjunction| and, or, but, if, while, although\n",
    "DET\t| determiner | the, a, some, most, every, no\n",
    "X\t| other, foreign words | dolce, ersatz, esprit, quo, maitre\n",
    "NOUN | noun\t     | year, home, costs, time, education\n",
    "PROPN| proper noun | Alison, Africa, April, Washington\n",
    "NUM\t | numeral\t| twenty-four, fourth, 1991, 14:24\n",
    "PRON | pronoun\t| he, their, her, its, my, I, us\n",
    "ADP  | adposition, preposition | on, of, at, with, by, into, under\n",
    "AUX\t | auxiliary verb | has (done), is (doing), will (do), should (do), must (do), can (do)\n",
    "INTJ | interjection | ah, bang, ha, whee, hmpf, oops\n",
    "VERB | verb | is, has, get, do, make, see, run\n",
    "PART | particle | possessive marker 's, negation 'not'\n",
    "SCONJ | subordinating conjunction: complementizer, adverbial clause introducer | I believe 'that' he will come, if, while\n",
    "SYM\t| symbol | $, %, (C), +, *, /, =, :), john.doe@example.com\n",
    "\n",
    "\n",
    "In the absence of any knowledge about words and tags, the perplexity of the task with a tagset of size 17 will be 17. We will see that adding knowledge will reduce the perplexity of the task.\n",
    "\n",
    "Note that the decision on how to tag a word, without more information is ambiguous for multiple reasons:\n",
    "\n",
    "- The same string can be understood as a noun or a verb (book).\n",
    "- Some POS tags have a systematically ambiguous definition: a present participle can be used in progressive verb usages (I am going:VERB), but it can also be used in an adjectival position modifying a noun: (A striking:ADJ comparison). In other words, it is unclear in the definition itself of the tag whether the tag refers to a syntactic function or to a morphological property of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring success: Accuracy, Training Dataset, Test Dataset\n",
    "\n",
    "Assume we develop a tagger. How do we know how successful it is? Can we trust the decisions the tagger makes? \n",
    "\n",
    "The way to address these issues is to define a criterion for success, and to test the tagger on a large test dataset. \n",
    "Assume we have a large dataset of 1M words with their tags assigned manually. \n",
    "We first split the dataset into 2 parts: one part on which we will \"learn\" facts about the words and their tags (we call this the _training dataset_), and one part which we use to test the results of our tagger (we call this the _test dataset_). \n",
    "\n",
    "It is critical NOT to test our tagger on the training dataset -- because we want to test whether the tagger is able to generalize from data it has seen and make decision on unseen data. (A \"stupid\" tagger would learn the exact data seen in the training dataset \"by heart\", and respond exactly as shown when asked on training data -- it would get a perfect score on the training data, but a poor score on any unseen data.) \n",
    "\n",
    "This is one way to split the dataset into training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged:  [('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]\n",
      "Untagged:  ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_news_tagged = brown.tagged_sents(categories='news', tagset='universal')\n",
    "brown_news_words = brown.tagged_words(categories='news', tagset='universal')\n",
    "\n",
    "brown_train = brown_news_tagged[100:]\n",
    "brown_test = brown_news_tagged[:100]\n",
    "\n",
    "from nltk.tag import untag\n",
    "test_sent = untag(brown_test[0])\n",
    "print(\"Tagged: \", brown_test[0])\n",
    "print(\"Untagged: \", test_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure success, in this task, we will measure accuracy. The tagger object in NLTK includes a method called `evaluate` to measure the accuracy of a tagger on a given test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'XYZ'), ('is', 'XYZ'), ('a', 'XYZ'), ('test', 'XYZ')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A default tagger assigns the same tag to all words\n",
    "from nltk import DefaultTagger\n",
    "default_tagger = DefaultTagger('XYZ')\n",
    "default_tagger.tag('This is a test'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 'NOUN' is the most frequent universal tag in the Brown corpus, we can use a tagger that assigns 'NOUN' \n",
    "to all words as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'NOUN'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'NOUN'), ('Jury', 'NOUN'), ('said', 'NOUN'), ('Friday', 'NOUN'), ('an', 'NOUN'), ('investigation', 'NOUN'), ('of', 'NOUN'), (\"Atlanta's\", 'NOUN'), ('recent', 'NOUN'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'NOUN'), ('``', 'NOUN'), ('no', 'NOUN'), ('evidence', 'NOUN'), (\"''\", 'NOUN'), ('that', 'NOUN'), ('any', 'NOUN'), ('irregularities', 'NOUN'), ('took', 'NOUN'), ('place', 'NOUN'), ('.', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "default_tagger = DefaultTagger('NOUN')\n",
    "print(default_tagger.tag(test_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this baseline, we achieve a low accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 31.8%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %4.1f%%' % (100.0 * default_tagger.evaluate(brown_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, note that we improved the expected accuracy from picking one out of 17 answers with no knowledge, to picking one out of about 3 answers with very little knowledge (what is the most frequent tag in the dataset).\n",
    "\n",
    "The computation of the accuracy compares the answers obtained by the tagger with that listed in the \"gold standard\" dataset. This is defined as follows in the NLTK code (the izip method in Python is defined in the itertools package, given two iterable collections, it returns an iterator over a collection of pairs - iterators are \"consumed\" by code such as \"for ... in ...iterator...):\n",
    "\n",
    "```python\n",
    " def accuracy(reference, test): \n",
    "       if len(reference) != len(test): \n",
    "           raise ValueError(\"Lists must have the same length.\") \n",
    "       num_correct = 0 \n",
    "       for x, y in izip(reference, test): \n",
    "           if x == y: \n",
    "               num_correct += 1 \n",
    "       return float(num_correct) / len(reference)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources of Knowledge to Improve Tagging Accuracy\n",
    "\n",
    "Intuitively, the sources of knowledge that can help us decide what is the tag of a word include:\n",
    "- A dictionary that lists the possible parts of speech for each word\n",
    "- The context of the word in a sentence (neighboring words)\n",
    "- The morphological form of the word (suffixes, prefixes)\n",
    "\n",
    "We will now develop a sequence of taggers that use these knowledge sources, and combine them. \n",
    "The taggers we develop will implement the `nltk.tag.TaggerI` interface:\n",
    "\n",
    "```python\n",
    "class TaggerI(object): \n",
    "  def tag(self, tokens): \n",
    "    raise NotImplementedError() \n",
    "    \n",
    "  def batch_tag(self, sentences): \n",
    "    return [self.tag(sent) for sent in sentences] \n",
    "\n",
    "  def evaluate(self, gold): \n",
    "    tagged_sents = self.batch_tag([untag(sent) for sent in gold]) \n",
    "    gold_tokens = sum(gold, []) \n",
    "    test_tokens = sum(tagged_sents, []) \n",
    "    return accuracy(gold_tokens, test_tokens) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Taggers: Backoff Tagger\n",
    "\n",
    "We will see as we proceed that there is a tradeoff in machine learning between _precision_ and _recall_: \n",
    "a classifier has high precision if when it makes a decision, it is often right; \n",
    "a classifier has high recall if it can make the right decision on most of the questions.\n",
    "\n",
    "To understand the difference, consider that a classifier may be given the option to \"pass\" on difficult questions. \n",
    "A cautious classifier will only commit to an answer if it is sure of the answer. \n",
    "Such a classifier could reach high precision, but may run the risk of low recall. \n",
    "In contrast, a risky classifier will suggest an answer even if it is not sure of the answer. \n",
    "Such a classifier could reach high recall, but runs the risk of low precision.\n",
    "\n",
    "One way to balance between different knowledge sources is to combine the classifiers they produce. \n",
    "For example, a tagger that looks at the morphological structure of words and one that looks at the neighbors of the words to be classified use completely different knowledge sources. We say they are orthogonal.\n",
    "\n",
    "One would want to combine the 2 classifiers into one \"better\" classifier than either one alone. \n",
    "Intuitively, it makes sense that the two sources of knowledge should produce better decisions than either one alone. \n",
    "How can we encode such combination?\n",
    "\n",
    "One way is to use a chain of classifiers and order the classifiers in a sequence of backoff: the first classifier, the most \"picky\" starts. If it decides to \"pass\", then it asks another, less picky tagger to contribute. The chain can be as long as needed. This method of tagger combination is implemented in the module `nltk.tag.sequential`. \n",
    "The interface `sequentialBackoffTagger` inherits from the `TaggerI` interface.\n",
    "\n",
    "```python\n",
    "class SequentialBackoffTagger(TaggerI): \n",
    "  \"\"\" \n",
    " An abstract base class for taggers that tags words sequentially, \n",
    " left to right.  Tagging of individual words is performed by the \n",
    " method L{choose_tag()}, which should be defined by subclasses.  If \n",
    " a tagger is unable to determine a tag for the specified token, \n",
    " then its backoff tagger is consulted. \n",
    "\"\"\" \n",
    " \n",
    " def __init__(self, backoff=None): \n",
    "   if backoff is None: \n",
    "     self._taggers = [self] \n",
    "   else: \n",
    "     self._taggers = [self] + backoff._taggers \n",
    " \n",
    "   def _get_backoff(self): \n",
    "     if len(self._taggers) < 2: return None \n",
    "       else: return self._taggers[1] \n",
    " \n",
    "   backoff = property(_get_backoff, doc='''The backoff tagger for this tagger.''') \n",
    " \n",
    "   def tag(self, tokens): \n",
    "      # docs inherited from TaggerI \n",
    "       tags = [] \n",
    "      for i in range(len(tokens)): \n",
    "        tags.append(self.tag_one(tokens, i, tags)) \n",
    "      return zip(tokens, tags) \n",
    " \n",
    "   def tag_one(self, tokens, index, history): \n",
    "     \"\"\" \n",
    "     Determine an appropriate tag for the specified token, and \n",
    "     return that tag.  If this tagger is unable to determine a tag \n",
    "     for the specified token, then its backoff tagger is consulted. \n",
    "     \"\"\" \n",
    "     tag = None \n",
    "     for tagger in self._taggers: \n",
    "       tag = tagger.choose_tag(tokens, index, history) \n",
    "       if tag is not None:  break \n",
    "     return tag \n",
    " \n",
    "   def choose_tag(self, tokens, index, history): \n",
    "     \"\"\" \n",
    "     Decide which tag should be used for the specified token, and \n",
    "     return that tag.  If this tagger is unable to determine a tag \n",
    "     for the specified token, return C{None} -- do I{not} consult \n",
    "     the backoff tagger.  This method should be overridden by \n",
    "     subclasses of C{SequentialBackoffTagger}. \n",
    "     \"\"\" \n",
    "    raise AssertionError('SequentialBackoffTagger is an abstract class') \n",
    "```\n",
    "    \n",
    "This class encodes 2 properties of taggers:\n",
    "\n",
    "- **Backoff**: They try to tag a word, and if they are not sure, they pass to a backoff tagger.\n",
    "- **Sequential**: They tag words from left to right, in order, and at each step, they can look at the decisions made in the past words within the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Tagger\n",
    "\n",
    "Here is the simplest implementation of this method:\n",
    "\n",
    "```python\n",
    "class DefaultTagger(SequentialBackoffTagger, yaml.YAMLObject): \n",
    "  \"\"\" \n",
    "  A tagger that assigns the same tag to every token. \n",
    "  \"\"\" \n",
    "  yaml_tag = '!nltk.DefaultTagger' \n",
    "\n",
    "  def __init__(self, tag): \n",
    "    \"\"\" \n",
    "    Construct a new tagger that assigns C{tag} to all tokens. \n",
    "    \"\"\" \n",
    "    self._tag = tag \n",
    "    SequentialBackoffTagger.__init__(self, None) \n",
    "\n",
    "  def choose_tag(self, tokens, index, history): \n",
    "    return self._tag  # ignore token and history \n",
    "\n",
    "  def __repr__(self): \n",
    "    return '<DefaultTagger: tag=%s>' % self._tag \n",
    "```\n",
    "\n",
    "This is the default tagger we used above. When it is given the 'NOUN' tag, it achieves accuracy of about 31%. \n",
    "This tagger is not discriminative at all: it does not look at any of the input data. It always finds a good candidate. \n",
    "It is, therefore, a good choice as the last member of a chain of taggers organized in a backoff chain. When all other methods cannot decide, then choose a reasonable default (which is better than not deciding at all, or deciding completely randomly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookup Tagger: Using Dictionary Knowledge\n",
    "\n",
    "Assume we have a dictionary that lists the possible tags for each word in English. Could we use this information to perform better tagging?\n",
    "\n",
    "The intuition is that we would only assign to a word a tag that it can have in the dictionary. For example, if \"box\" can only be a Verb or a Noun, when we have to tag an instance of the word \"box\", we only choose between 2 options - and not between 17 options. Thus, dictionary knowledge will reduce the perplexity of the task.\n",
    "\n",
    "There are 3 issues we must address to turn this into working code:\n",
    "\n",
    "- Where do we get the dictionary?\n",
    "- How do we choose between the various tags associated to a word in the dictionary? (For example, how do we choose between VERB and NOUN for \"box\").\n",
    "- What do we do for words that do not appear in the dictionary?\n",
    "\n",
    "The simple solutions we will test are the following - note that for each question, there exist other strategies that we will investigate later:\n",
    "\n",
    "- Where do we get the dictionary: we will learn it from a sample dataset.\n",
    "- How do we choose between the various tags associated to a word in the dictionary: we will choose the most likely tag as observed in the sample dataset.\n",
    "- What do we do for words that do not appear in the dictionary: we will pass unknown words to a backoff tagger.\n",
    "\n",
    "The `nltk.UnigramTagger` implements this overall strategy. It must be trained on a dataset, from which it builds a model of \"unigrams\". The following code shows how it is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('Fulton', None),\n",
       " ('County', 'NOUN'),\n",
       " ('Grand', 'ADJ'),\n",
       " ('Jury', 'NOUN'),\n",
       " ('said', 'VERB'),\n",
       " ('Friday', 'NOUN'),\n",
       " ('an', 'DET'),\n",
       " ('investigation', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " (\"Atlanta's\", 'NOUN'),\n",
       " ('recent', 'ADJ'),\n",
       " ('primary', 'NOUN'),\n",
       " ('election', 'NOUN'),\n",
       " ('produced', 'VERB'),\n",
       " ('``', '.'),\n",
       " ('no', 'DET'),\n",
       " ('evidence', 'NOUN'),\n",
       " (\"''\", '.'),\n",
       " ('that', 'ADP'),\n",
       " ('any', 'DET'),\n",
       " ('irregularities', None),\n",
       " ('took', 'VERB'),\n",
       " ('place', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare training and test datasets\n",
    "from nltk.corpus import brown\n",
    "from nltk import UnigramTagger\n",
    "\n",
    "brown_news_tagged = brown.tagged_sents(categories='news', tagset='universal')\n",
    "brown_train = brown_news_tagged[100:]\n",
    "brown_test = brown_news_tagged[:100]\n",
    "\n",
    "# Train the unigram model\n",
    "unigram_tagger = UnigramTagger(brown_train)\n",
    "\n",
    "# Test it on a single sentence\n",
    "unigram_tagger.tag(untag(brown_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the unigram tagger leaves some words tagged as 'None' -- these are **unknown words**, words that were not observed in the training dataset.\n",
    "\n",
    "How successful is this tagger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram tagger accuracy: 88.9%\n"
     ]
    }
   ],
   "source": [
    "print('Unigram tagger accuracy: %4.1f%%' % ( 100.0 * unigram_tagger.evaluate(brown_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "88.9% is quite an improvement on the 31% of the default tagger. \n",
    "And this is without any backoff and without using morphological clues.\n",
    "\n",
    "Is 88.9% a good level of accuracy? In fact it is not. It is accuracy per word. It means that on average, in every sentence of about 20 words, we will accumulate 2 errors. 2 errors in each sentence is a very high error rate. It makes it difficult to run another task on the output of such a tagger. Think how difficult the life of a parser would be if 2 words in every sentence are wrongly tagged. The problem is known as the **pipeline effect** -- when language processing tools are chained in a pipeline, error rates accumulate from module to module.\n",
    "\n",
    "How much would a good backoff help? Let's try first to add the NN-default tagger as a backoff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram tagger with backoff accuracy: 94.5%\n"
     ]
    }
   ],
   "source": [
    "nn_tagger = DefaultTagger('NOUN')\n",
    "ut2 = UnigramTagger(brown_train, backoff=nn_tagger)\n",
    "print('Unigram tagger with backoff accuracy: %4.1f%%' % ( 100.0 * ut2.evaluate(brown_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a simple backoff (with accuracy of 31%) improved accuracy from 88.9% to 94.5%. \n",
    "\n",
    "One way to report this is in terms of **error reduction**: the error rate went down from 11.1% (100-88.9) to 5.5%. \n",
    "That's an **absolute error reduction** of 11.1-5.5 = 4.6%. \n",
    "Error reduction is generally reported as a percentage of the error: 100.0 * (4.6 / 11.1) = 41.4% relative error reduction. \n",
    "\n",
    "In other words, out of the words not tagged by the original model (with no backoff), 41.4% were corrected by the backoff.\n",
    "\n",
    "What can we say about this error reduction? It is different from the accuracy of the backoff (41.4% vs 31%). \n",
    "\n",
    "One lesson to learn from this is that the **distribution of unknown words is significantly different from the distribution of all the words in the corpus**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Morphological Clues\n",
    "\n",
    "As mentioned above, another knowledge source to perform tagging is to look at the letter structure of the words. \n",
    "We will look at 2 different methods to use this knowledge. \n",
    "First, we will use nltk.RegexpTagger to recognize specific regular expressions in words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regexp accuracy 48.2%\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpTagger\n",
    "\n",
    "regexp_tagger = RegexpTagger(\n",
    "     [(r'^-?[0-9]+(.[0-9]+)?$', 'NUM'),   # cardinal numbers\n",
    "      (r'(The|the|A|a|An|an)$', 'DET'),   # articles\n",
    "      (r'.*able$', 'ADJ'),                # adjectives\n",
    "      (r'.*ness$', 'NOUN'),               # nouns formed from adjectives\n",
    "      (r'.*ly$', 'ADV'),                  # adverbs\n",
    "      (r'.*s$', 'NOUN'),                  # plural nouns\n",
    "      (r'.*ing$', 'VERB'),                # gerunds\n",
    "      (r'.*ed$', 'VERB'),                 # past tense verbs\n",
    "      (r'.*', 'NOUN')                     # nouns (default)\n",
    "])\n",
    "\n",
    "print('Regexp accuracy %4.1f%%' % (100.0 * regexp_tagger.evaluate(brown_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regular expressions are tested in order. If one matches, it decides the tag. Else it tries the next tag. The implementation of the RegexpTagger is quite simple: the choose_tag method of the SequenceBackoffTagger ignores the history (left context) of the word to be tagged. It tries the regular expressions in order.\n",
    "\n",
    "**Note**: The notation `r'.*able'` in Python refers to a _raw string_ - that is, a string in which backslash escape characters are interpreted in a different manner than those used in regular strings. For example '\\n' is translated to a newline character by the Python reader, but r'\\n' is read as a string of 2 characters containing a backslash. Since regular expressions often contain backslash characters to escape special regular expression characters (such as ., ^, *, +, $ etc), it is most convenient to encode regular expression strings with the \"r-prefix\" notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class RegexpTagger(SequentialBackoffTagger, yaml.YAMLObject):\n",
    "    \"\"\"\n",
    "    A tagger that assigns tags to words based on regular expressions over word strings.\n",
    "    \"\"\"\n",
    "    yaml_tag = '!nltk.RegexpTagger'\n",
    "    def __init__(self, regexps, backoff=None):\n",
    "        self._regexps = regexps\n",
    "        SequentialBackoffTagger.__init__(self, backoff)\n",
    "\n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        for regexp, tag in self._regexps:\n",
    "            if re.match(regexp, tokens[index]): # ignore history\n",
    "                return tag\n",
    "        return None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<Regexp Tagger: size=%d>' % len(self._regexps)\n",
    "```\n",
    "\n",
    "The question we face when we see such a \"rule-based\" tagger are:\n",
    "\n",
    "- How do we find the most successful regular expressions?\n",
    "- In which order should we try the regular expressions?\n",
    "\n",
    "A typical answer to such questions is: \n",
    "\n",
    "- let's learn these parameters from a training corpus. \n",
    "\n",
    "The `nltk.AffixTagger` is a trainable tagger that attempts to learn word patterns. \n",
    "It only looks at the last letters in the words in the training corpus, and counts how often a word suffix \n",
    "can predict the word tag. \n",
    "In other words, we only learn rules of the form ('.*xyz' , POS). \n",
    "This is how the affix tagger is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affix tagger accuracy: 42.2%\n"
     ]
    }
   ],
   "source": [
    "from nltk import AffixTagger\n",
    "\n",
    "affix_tagger = AffixTagger(brown_train, backoff=nn_tagger)\n",
    "print('Affix tagger accuracy: %4.1f%%' % (100.0 * affix_tagger.evaluate(brown_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we be disappointed that the \"data-based approach\" performs worse than the hand-written rules (42% vs. 48%)? \n",
    "\n",
    "Not necessarily: note that our hand-written rules include cases that the AffixTagger cannot learn - we match cardinal numbers and suffixes with more than 3 letters. \n",
    "\n",
    "Let us see whether the combination of the 2 taggers helps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affix tagger with regexp backoff accuracy: 52.8%\n"
     ]
    }
   ],
   "source": [
    "at2 = AffixTagger(brown_train, backoff=regexp_tagger)\n",
    "print(\"Affix tagger with regexp backoff accuracy: %4.1f%%\" % (100.0 * at2.evaluate(brown_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not bad - the machine learning in AffixTagger helped us reduce the error from 52% to 47% (10% error reduction).\n",
    "\n",
    "How much does this tagger help the unigram tagger if we use it as a backoff instead of the NOUN-default tagger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram with affix backoff accuracy: 95.4%\n"
     ]
    }
   ],
   "source": [
    "ut3 = UnigramTagger(brown_train, backoff=at2)\n",
    "print('Unigram with affix backoff accuracy: %4.1f%%' % (100.0 * ut3.evaluate(brown_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error reduction is from 88.9% to 95.4% -- better that the 94.5% obtained with the NOUN backoff.\n",
    "\n",
    "At this point, we have combined 2 major sources of information: dictionary and morphology and obtained about 95.4% accuracy. \n",
    "\n",
    "The last type of knowledge we will use is syntax: look at the context of word to be tagged to make a decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the Context\n",
    "\n",
    "The last source of knowledge we want to exploit to better tag is the context of the word to be tagged. \n",
    "By context, we mean the words that appear around the word to be tagged. \n",
    "The intuition is that if we have to decide between \"book\" as a verb or a noun, the word preceding \"books\" can give us strong cues: for example, if it is an article (\"the\" or \"a\") then we would be sure it is a noun; if it is \"to\", then we would be sure it is a verb.\n",
    "\n",
    "How can we turn this intuition into working code? The basic approach we will take is to learn \"predictive contexts\" from our training data set. A context is \"predictive\" if when we see the context, we can predict the tag of a word with high confidence. The easiest way to detect predictive contexts is to construct a list of contexts - and for each context, keep track of the distribution of tags that follow it.\n",
    "\n",
    "There are several questions we must address to verify that this strategy will work:\n",
    "\n",
    "- What is a context?\n",
    "- How reliable is the observation of a context in the training set to learn predictions on the test set?\n",
    "- How reliable is the inference from a context to the tag of the next word?\n",
    "\n",
    "We start with easy answers to these questions - naturally, more sophisticated answers will lead better performance:\n",
    "\n",
    "- A context is the list of the N tags that appear before the word to be tagged together with the word to be tagged itself.\n",
    "- A context is reliable if it is seen more often than C times in the training set.\n",
    "- The inference from context to tag is always reliable - we just take the argmax of the distribution (context --> tag).\n",
    "\n",
    "When we tag unseen text, we tag words from left to right, sentence by sentence. \n",
    "When we reach word number i within a sentence, we remember the history of the decisions we've made up to i, \n",
    "and use these as the context of the next tagging decision.\n",
    "\n",
    "This strategy is implemented in the following 2 nltk classes: `ContextTagger` and `NgramTagger`. \n",
    "`ContextTagger` is an abstract class that extends `SeqentialBackoffTagger` and that captures:\n",
    "\n",
    "- The notion of an abstract context (through the abstract method `context`)\n",
    "- training to learn a context table (through the method `_train`)\n",
    "- Tag inference by looking up the context table (method `choose_tag`)\n",
    "\n",
    "As usual, if `choose_tag` cannot make a decision (because the observed context was never seen at training time), \n",
    "the decision is delegated to a backoff tagger.\n",
    "\n",
    "At the core of the `ContextTagger`, lies the `context_to_tag` dictionary. \n",
    "The train method constructs this dictionary by constructing a `CondFreqDist` object that counts how often a context is mapped to each observed tag.\n",
    "\n",
    "```python\n",
    "class ContextTagger(SequentialBackoffTagger):\n",
    "    \"\"\"\n",
    "    An abstract base class for sequential backoff taggers that choose\n",
    "    a tag for a token based on the value of its \"context\".  Different\n",
    "    subclasses are used to define different contexts.\n",
    "\n",
    "    A C{ContextTagger} chooses the tag for a token by calculating the\n",
    "    token's context, and looking up the corresponding tag in a table.\n",
    "    This table can be constructed manually; or it can be automatically\n",
    "    constructed based on a training corpus, using the L{_train()}\n",
    "    factory method.\n",
    "\n",
    "    @ivar _context_to_tag: Dictionary mapping contexts to tags.\n",
    "    \"\"\"\n",
    "    def __init__(self, context_to_tag, backoff=None):\n",
    "        SequentialBackoffTagger.__init__(self, backoff)\n",
    "        if context_to_tag:\n",
    "            self._context_to_tag = context_to_tag\n",
    "        else:\n",
    "            self._context_to_tag = {}\n",
    "\n",
    "    def context(self, tokens, index, history):\n",
    "        \"\"\"\n",
    "        @return: the context that should be used to look up the tag\n",
    "            for the specified token; or C{None} if the specified token\n",
    "            should not be handled by this tagger.\n",
    "        @rtype: (hashable)\n",
    "        \"\"\"\n",
    "        raise AssertionError('Abstract base class')\n",
    "\n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        context = self.context(tokens, index, history)\n",
    "        return self._context_to_tag.get(context)\n",
    "\n",
    "    def _train(self, tagged_corpus, cutoff=0, verbose=False):\n",
    "        \"\"\"\n",
    "        Initialize this C{ContextTagger}'s L{_context_to_tag} table\n",
    "        based on the given training data.  In particular, for each\n",
    "        context C{I{c}} in the training data, set\n",
    "        C{_context_to_tag[I{c}]} to the most frequent tag for that\n",
    "        context.  However, exclude any contexts that are already\n",
    "        tagged perfectly by the backoff tagger(s).\n",
    "\n",
    "        @param tagged_corpus: A tagged corpus.  Each item should be\n",
    "            a C{list} of C{(word, tag)} tuples.\n",
    "        @param cutoff: If the most likely tag for a context occurs\n",
    "            fewer than C{cutoff} times, then exclude it from the\n",
    "            context-to-tag table for the new tagger.\n",
    "        \"\"\"\n",
    "\n",
    "        token_count = hit_count = 0\n",
    "\n",
    "        # A context is considered 'useful' if it's not already tagged\n",
    "        # perfectly by the backoff tagger.\n",
    "        useful_contexts = set()\n",
    "        \n",
    "        # Count how many times each tag occurs in each context.\n",
    "        fd = ConditionalFreqDist()\n",
    "        for sentence in tagged_corpus:\n",
    "            tokens, tags = zip(*sentence)\n",
    "            for index, (token, tag) in enumerate(sentence):\n",
    "                # Record the event.\n",
    "                token_count += 1\n",
    "                context = self.context(tokens, index, tags[:index])\n",
    "                if context is None: continue\n",
    "                fd[context].inc(tag)\n",
    "                # If the backoff got it wrong, this context is useful:\n",
    "                if (self.backoff is None or\n",
    "                    tag != self.backoff.tag_one(tokens, index, tags[:index])):\n",
    "                    useful_contexts.add(context)\n",
    "\n",
    "        # Build the context_to_tag table -- for each context, figure\n",
    "        # out what the most likely tag is.  Only include contexts that\n",
    "        # we've seen at least `cutoff` times.\n",
    "        for context in useful_contexts:\n",
    "            best_tag = fd[context].max()\n",
    "            hits = fd[context][best_tag]\n",
    "            if hits > cutoff:\n",
    "                self._context_to_tag[context] = best_tag\n",
    "                hit_count += hits\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The following Python calls need to be explained: zip(*sentence) and enumerate(sentence):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x5e35e08>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zip(*sentence) splits a list of tuples with n elements each into n flat lists \n",
    "zip(*[('a', 1, 'z'), ('b', 2, 'y')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip returns a generator - it is a lazy function. To consume the generator, we must consume it with for example a for iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 'b')\n",
      "(1, 2)\n",
      "('z', 'y')\n"
     ]
    }
   ],
   "source": [
    "for t in zip(*[('a', 1, 'z'), ('b', 2, 'y')]): print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enumerate(generator) returns a new iterator over pairs that are numbered in increasing numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'a'), (1, 'b')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enumerate(list) returns an iterator over pairs of the form (index, item-in-list)\n",
    "[i for i in enumerate(['a', 'b'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `NgramTagger` extends `ContextTagger` by specifying that the context to be used contains the n previous tags in the sentence and the word to be tagged.\n",
    "\n",
    "```python\n",
    "class NgramTagger(ContextTagger, yaml.YAMLObject):\n",
    "    \"\"\"\n",
    "    A tagger that chooses a token's tag based on its word string and\n",
    "    on the preceeding I{n} word's tags.  In particular, a tuple\n",
    "    C{(tags[i-n:i-1], words[i])} is looked up in a table, and the\n",
    "    corresponding tag is returned.  N-gram taggers are typically\n",
    "    trained on a tagged corpus.\n",
    "    \"\"\"\n",
    "    def __init__(self, n, train=None, model=None,\n",
    "                 backoff=None, cutoff=0, verbose=False):\n",
    "        self._n = n\n",
    "        ContextTagger.__init__(self, model, backoff)\n",
    "        if train:\n",
    "            self._train(train, cutoff, verbose)\n",
    "            \n",
    "    def context(self, tokens, index, history):\n",
    "        tag_context = tuple(history[max(0,index-self._n+1):index])\n",
    "        return (tag_context, tokens[index])\n",
    "```\n",
    "\n",
    "How much does such a simple model of context help? \n",
    "\n",
    "Let us try it with our best backoff tagger so far (ut3 is the unigram tagger backed off by the affix tagger backed off by the regexp tagger backed off by the NN tagger):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9541446208112875"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Where we stand before looking at the context\n",
    "ut3.evaluate(brown_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9611992945326279"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import NgramTagger\n",
    "\n",
    "ct2 = NgramTagger(2, brown_train, backoff=ut3)\n",
    "ct2.evaluate(brown_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9576719576719577"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct3 = NgramTagger(3, brown_train, backoff=ut3)\n",
    "ct3.evaluate(brown_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find on our dataset that looking at a context of 2 tags in the past improves the accuracy from 95.4% to 96.1% -- this is 18% error reduction. \n",
    "\n",
    "If we try to use a larger context of 3 tags, we get less improvement (from 95.4% to 95.8%). \n",
    "\n",
    "The main problem we face is that of **data sparseness**: there are not enough samples of each context for the context to help. We will return to this issue in the next lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This chapter introduced tools to tag parts of speech in free text.\n",
    "\n",
    "Parts of speech are classes of words that can be characterized by criteria of different type:\n",
    "\n",
    "- **Syntactic**: 2 words of the same class can substitute each other in a sentence and leave the sentence syntactically acceptable\n",
    "- **Morphological**: words of the same class are inflected in similar manner\n",
    "- **Semantic**: words of the same class denote entities of similar semantic types (object, action, property, relation)\n",
    "\n",
    "Tagsets of various granularity can be considered. We mentioned the standard Brown corpus tagset (about 60 tags for the complete tagset) and the reduced universal tagset (17 tags). \n",
    "\n",
    "The key point of the approach we investigated is that it is **data-driven**: we attempt to solve the task by:\n",
    "\n",
    "- Obtain sample data annotated manually: we used the Brown corpus\n",
    "- Define a success metric: we used the definition of accuracy\n",
    "- Measure the adequacy of a method by measuring its success\n",
    "- Measure the complexity of the problem by using the notion of **perplexity**\n",
    "\n",
    "The computational methods we developed are characterized by:\n",
    "\n",
    "- We first define possible knowledge sources that can help us solve the task. Specifically, we investigated \n",
    "  * dictionary, \n",
    "  * morphological \n",
    "  * context\n",
    "  as possible sources.\n",
    "\n",
    "- We developed computational models that represent each of these knowledge sources in simple data structures (hashtables, frequency distributions, conditional frequency distributions).\n",
    "- We tested simple machine learning methods: data is acquired by inspecting a training dataset, then evaluated by testing on a test dataset.\n",
    "- We investigated one method to combine several systems into a combined system: backoff models.\n",
    "\n",
    "This methodology will be further developed in the next chapters, as we will address more complex tasks (parsing, summarization) and use more sophisticated machine learning methods.\n",
    "\n",
    "The task of Parts of Speech tagging is very well studied in English. The most efficient systems obtain accuracy rates of over 98% even on fine granularity tagsets - which is equivalent to the rate of success human beings obtain and the best agreement among human taggers generally obtained. The best systems use better machine learning algorithms (HMM, SVM) and treat unknown words (words not seen in training data) with more sophistication than what we have observed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
