{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T14:55:12.814549Z",
     "start_time": "2018-03-20T14:55:12.800845Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"1600\"\n",
       "            src=\"./assignment4.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x109258470>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"./assignment4.pdf\", width=800, height=1600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T14:55:20.186376Z",
     "start_time": "2018-03-20T14:55:20.178823Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T14:55:20.834862Z",
     "start_time": "2018-03-20T14:55:20.831772Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_string(string):\n",
    "    return string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T14:55:21.887463Z",
     "start_time": "2018-03-20T14:55:21.697634Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_and_labels(data_file_path, max_vocabSize, past_words):\n",
    "    \"\"\"\n",
    "    Loads training data, creates vocabulary and returns the respective ids for words and tags\n",
    "    \"\"\"\n",
    "    # Load data from file\n",
    "    cwd = os.getcwd()\n",
    "    # Collect word counts and unique PoS tags\n",
    "    word_counts = Counter()\n",
    "    unique_posTags = set()\n",
    "    with open(data_file_path, \"r\") as tagged_sentences:\n",
    "        for sentence in tagged_sentences:\n",
    "            for tag in sentence.strip().split(\" \"):\n",
    "                splitted_tag = tag.split(\"/\")\n",
    "                if len(splitted_tag) != 2:\n",
    "                    continue\n",
    "                word = clean_string(splitted_tag[0])\n",
    "                pos = splitted_tag[1]\n",
    "                unique_posTags.add(pos) # collect all unique PoS tags\n",
    "                if word in word_counts: # collect word frequencies (used later to prune vocabulary)\n",
    "                    word_counts[word] += 1\n",
    "                else:\n",
    "                    word_counts[word] = 1\n",
    "    # Prune vocabulary to max_vocabSize\n",
    "    words_toKeep = [tupl[0] for tupl in word_counts.most_common(max_vocabSize-1)]\n",
    "    # Create mapping from words/PoS tags to ids\n",
    "    word_toId = {word: i for i, word in enumerate(words_toKeep, 1)}\n",
    "    word_toId[\"<UNK>\"] = 0 # add unknown token to vocabulary (all words not contained in it will be mapped to this)\n",
    "    pos_toId = {pos: i for i, pos in enumerate(list(unique_posTags))}\n",
    "    # Save vocabulary and PoS tags ids for evaluation\n",
    "    if not os.path.exists(cwd+\"/vocab\"):\n",
    "        os.makedirs(cwd+\"/vocab\")\n",
    "    with open(cwd+\"/vocab/wordIds.pkl\", \"wb\") as f:\n",
    "        pickle.dump(word_toId, f)\n",
    "    with open(cwd+\"/vocab/posIds.pkl\", \"wb\") as f:\n",
    "        pickle.dump(pos_toId, f)\n",
    "    # Replace each word with the id of the previous \"past_words\" words\n",
    "    # and replace each PoS tag by its respective id\n",
    "    x = []\n",
    "    y = []\n",
    "    with open(data_file_path, \"r\") as tagged_sentences:\n",
    "        for sentence in tagged_sentences:\n",
    "            pairs = sentence.strip().split(\" \")\n",
    "            words_and_tags = list(pair.split(\"/\") for pair in pairs if len(pair.split(\"/\")) == 2)\n",
    "            if len(words_and_tags) == 0:\n",
    "                continue\n",
    "            words, pos_tags = zip(*words_and_tags)\n",
    "            words = [clean_string(word) for word in words]\n",
    "            for j in range(len(words)):\n",
    "                y.append(pos_toId[ pos_tags[j] ])\n",
    "                pastWords_ids = []\n",
    "                for k in range(0, past_words+1): # for previous words\n",
    "                    if j-k < 0: # out of bounds\n",
    "                        pastWords_ids.append(0) # <UNK>\n",
    "                    elif words[j-k] in word_toId: # word in vocabulary\n",
    "                        pastWords_ids.append(word_toId[ words[j-k] ])\n",
    "                    else: # word not in vocabulary\n",
    "                        pastWords_ids.append(0) # <UNK>\t\n",
    "                x.append(pastWords_ids)\n",
    "\n",
    "    return [np.array(x), np.array(y), len(unique_posTags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T14:55:22.635662Z",
     "start_time": "2018-03-20T14:55:22.564891Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_and_labels_test(data_file_path, past_words):\n",
    "    \"\"\"\n",
    "    Loads test data and vocabulary and returns the respective ids for words and tags\n",
    "    \"\"\"\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    # Load vocabulary and PoS tags ids from training\n",
    "    if not os.path.exists(cwd+\"/vocab\"):\n",
    "        raise FileNotFoundError(\"You need to run train.py first in order to generate the vocabulary.\")\n",
    "    with open(cwd+\"/vocab/wordIds.pkl\", \"rb\") as f:\n",
    "        word_toId = pickle.load(f)\n",
    "    with open(cwd+\"/vocab/posIds.pkl\", \"rb\") as f:\n",
    "        pos_toId = pickle.load(f)\n",
    "    # Replace each word with the id of the previous \"past_words\" words\n",
    "    # and replace each PoS tag by its respective id\n",
    "    x = []\n",
    "    y = []\n",
    "    with open(cwd+data_file_path, \"r\") as tagged_sentences:\n",
    "        for sentence in tagged_sentences:\n",
    "            pairs = sentence.strip().split(\" \")\n",
    "            words, pos_tags = zip(*(pair.split(\"/\") for pair in pairs if len(pair.split(\"/\")) == 2))\n",
    "            for j in range(len(words)): # for each word in the sentence\n",
    "                if pos_tags[j] in pos_toId: \n",
    "                    y.append(pos_toId[ pos_tags[j] ])\n",
    "                else:\n",
    "                    y.append(0) # TODO: This is not correct, but we should have seen all posible output tags in advance...\n",
    "                pastWords_ids = []\n",
    "                for k in range(1, past_words+1): # for previous words\n",
    "                    if j-k < 0: # out of bounds\n",
    "                        pastWords_ids.append(0) # <UNK>\n",
    "                    elif words[j-k] in word_toId: # word in vocabulary\n",
    "                        pastWords_ids.append(word_toId[ words[j-k] ])\n",
    "                    else: # word not in vocabulary\n",
    "                        pastWords_ids.append(0) # <UNK>\t\n",
    "                x.append(pastWords_ids)\n",
    "\n",
    "    return [np.array(x), np.array(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T14:55:23.349684Z",
     "start_time": "2018-03-20T14:55:23.331615Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PosTagger: Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T14:55:26.014851Z",
     "start_time": "2018-03-20T14:55:25.827713Z"
    }
   },
   "outputs": [],
   "source": [
    "class PoSTagger(object):\n",
    "    \"\"\"\n",
    "    A simple PoS tagger implementation in Tensorflow.\n",
    "    Uses an embedding layer followed by a fully connected layer with ReLU and a softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, vocab_size, embedding_size, past_words): # sequence_length, filter_sizes, num_filters, l2_reg_lambda=0.0\n",
    "        # Minibatch placeholders for input and output\n",
    "        # The word indices of the window\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, past_words+1], name=\"input_x\")\n",
    "        # The target pos-tags\n",
    "        self.input_y = tf.placeholder(tf.int64, [None], name=\"input_y\") \n",
    "\n",
    "        with tf.device('/gpu:0'):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Embedding layer\n",
    "            with tf.name_scope(\"embedding\"):\n",
    "                # Create an embedding matrix: |vocab_size x embedding_size|\n",
    "                embedding_matrix = tf.get_variable('embedding_matrix',\n",
    "                   shape=[vocab_size, embedding_size],\n",
    "                   initializer=tf.random_uniform_initializer(minval=-1, maxval=1))\n",
    "                \n",
    "                # Create word embeddings tensor with embedding matrix lookup:\n",
    "                # |None x past_words x embedding_size|\n",
    "                embeddings = tf.nn.embedding_lookup(embedding_matrix,\n",
    "                                                         self.input_x,\n",
    "                                                         name='embedding_lookup')\n",
    "                \n",
    "                # Create feature vector:\n",
    "                # Flatten out embeddings tensor to get e/a input sequence on row:\n",
    "                # |None x (past_words+1)*embedding_size\n",
    "                x_flat_dim = (past_words+1)*embedding_size\n",
    "                self.x_flat = tf.reshape(embeddings, [-1, x_flat_dim])\n",
    "                \n",
    "            # Fully connected layer with ReLU \n",
    "            with tf.name_scope(\"model\"):\n",
    "                # Set initializer handle for readability below\n",
    "                xavi = tf.contrib.layers.xavier_initializer()\n",
    "                \n",
    "                # Set size d' for (first) hidden layer\n",
    "                d_prime = embedding_size\n",
    "                \n",
    "                # Create/get weight matrix and bias vector for hidden layer\n",
    "                W_1 = tf.get_variable('W_1', [x_flat_dim, d_prime], initializer=xavi)\n",
    "                b_1 = tf.get_variable('b_1', [d_prime], initializer=tf.zeros_initializer)\n",
    "                \n",
    "                # Send feature vector through hidden layer\n",
    "                out_1 = tf.nn.relu(tf.nn.xw_plus_b(x_flat, W_1, b_1))\n",
    "                \n",
    "                # Compute softmax logits :\n",
    "                # Create/get weight matrix and bias vector for softmax layer\n",
    "                W_softmax = tf.get_variable('W_softmax', [d_prime ,num_classes], initializer=xavi)\n",
    "                b_softmax = tf.get_variable('b_softmax', [num_classes], initializer=tf.zeros_initializer)\n",
    "                \n",
    "                # Calculate logits\n",
    "                self.logits = tf.nn.xw_plus_b(out_1, W_softmax, b_softmax)\n",
    "                \n",
    "                # Apply softmax on logits to get predicted probabilities\n",
    "                self.predictions = tf.nn.softmax(logits, name='predictions')\n",
    "                \n",
    "                # Find most probable prediction along axis=1\n",
    "                self.best_prediction = tf.argmax(self.predictions, 1, name='best_prediction')\n",
    "                \n",
    "                # Compute the mean loss using tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "                self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=self.logits, labels=self.input_y))\n",
    "\n",
    "            # Calculate accuracy\n",
    "            with tf.name_scope(\"accuracy\"):\n",
    "                # TODO compute the average accuracy over the batch (remember tf.argmax and tf.equal)\n",
    "                self.correct_predictions = tf.cast(tf.equal(self.best_prediction,\n",
    "                                                            self.input_y),\n",
    "                                                   'float')\n",
    "                self.accuracy = tf.reduce_mean(self.correct_predictions, name='accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T14:55:37.000926Z",
     "start_time": "2018-03-20T14:55:36.606839Z"
    }
   },
   "outputs": [
    {
     "ename": "UnrecognizedFlagError",
     "evalue": "Unknown command line flag 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-119a645b7c60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nParameters:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# a flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m       \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, argv, known_only)\u001b[0m\n\u001b[1;32m    628\u001b[0m       \u001b[0msuggestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flag_suggestions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       raise _exceptions.UnrecognizedFlagError(\n\u001b[0;32m--> 630\u001b[0;31m           name, value, suggestions=suggestions)\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m: Unknown command line flag 'f'"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "\n",
    "## PARAMETERS ##\n",
    "\n",
    "# Data loading parameters\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data used for validation (default: 10%)\")\n",
    "\n",
    "tf.flags.DEFINE_string(\"data_file_path\", \"/data/corpus-00\", \"Path to the training data\")\n",
    "# Model parameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of word embeddings (default: 128)\")\n",
    "tf.flags.DEFINE_integer(\"vocab_size\", 50000, \"Size of the vocabulary (default: 50k)\")\n",
    "tf.flags.DEFINE_integer(\"past_words\", 3, \"How many previous words are used for prediction (default: 3)\")\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Tensorflow Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "\tprint(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T15:04:48.598522Z",
     "start_time": "2018-03-20T15:04:48.166876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing traning and dev datasets \n",
      "\n"
     ]
    },
    {
     "ename": "UnrecognizedFlagError",
     "evalue": "Unknown command line flag 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-dc3906330bc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading and preprocessing traning and dev datasets \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_outputTags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpast_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Randomly shuffle data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# a flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m       \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, argv, known_only)\u001b[0m\n\u001b[1;32m    628\u001b[0m       \u001b[0msuggestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flag_suggestions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       raise _exceptions.UnrecognizedFlagError(\n\u001b[0;32m--> 630\u001b[0;31m           name, value, suggestions=suggestions)\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m: Unknown command line flag 'f'"
     ]
    }
   ],
   "source": [
    "## DATA PREPARATION ##\n",
    "\n",
    "# Load data\n",
    "print(\"Loading and preprocessing traning and dev datasets \\n\")\n",
    "x, y, num_outputTags = load_data_and_labels(FLAGS.data_file_path, FLAGS.vocab_size, FLAGS.past_words)\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffled_indices = np.random.permutation(len(y))\n",
    "x_shuffled = x[shuffled_indices]\n",
    "y_shuffled = y[shuffled_indices]\n",
    "\n",
    "# Split train/dev sets\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "# Generate training batches\n",
    "batches = batch_iter(list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "print(\"Done \\n\")\n",
    "\n",
    "## MODEL AND TRAINING PROCEDURE DEFINITION ##\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "        log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Initialize model\n",
    "        pos_tagger = PoSTagger(\n",
    "            num_classes=num_outputTags, \n",
    "            vocab_size=FLAGS.vocab_size, \n",
    "            embedding_size=FLAGS.embedding_dim, \n",
    "            past_words=FLAGS.past_words\n",
    "        )\n",
    "\n",
    "        # Define training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        # Define an optimizer, e.g. AdamOptimizer\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        # Define an optimizer step\n",
    "        train_op = minimize(pos_taggeros_tagger.loss, global_step=global_step)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", pos_tagger.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", pos_tagger.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory (Tensorflow assumes this directory already exists so we need to create it)\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.graph.finalize()\n",
    "\n",
    "        # Define training and dev steps (batch) \n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "                pos_tagger.input_x: x_batch,\n",
    "                pos_tagger.input_y: y_batch\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, pos_tagger.loss, pos_tagger.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "                pos_tagger.input_x: x_batch,\n",
    "                pos_tagger.input_y: y_batch\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, pos_tagger.loss, pos_tagger.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        ## TRAINING LOOP ##\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "\n",
    "# PARAMETERS ##\n",
    "\n",
    "# Data loading parameters\n",
    "tf.flags.DEFINE_string(\"data_file_path\", \"/data/corpus-01\", \"Path to the test data\")\n",
    "# Model parameters\n",
    "tf.flags.DEFINE_integer(\"past_words\", 3, \"How many previous words are used for prediction (default: 3)\")\n",
    "# Test parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_string(\"checkpoint_dir\", \"./runs/1490130308/checkpoints/\", \"Checkpoint directory from training run\")\n",
    "# Tensorflow Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "\tprint(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA PREPARATION ##\n",
    "\n",
    "# Load data\n",
    "print(\"Loading and preprocessing test dataset \\n\")\n",
    "x_test, y_test = data_utils.load_data_and_labels_test(FLAGS.data_file_path, FLAGS.past_words)\n",
    "\n",
    "## EVALUATION ##\n",
    "\n",
    "checkpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "        log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"accuracy\").outputs[0]\n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        batches = data_utils.batch_iter(list(x_test), FLAGS.batch_size, 1, shuffle=False)\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "\n",
    "        for x_test_batch in batches:\n",
    "            batch_predictions = sess.run(predictions, {input_x: x_test_batch})\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "\n",
    "# Print accuracy\n",
    "correct_predictions = float(sum(all_predictions == y_test))\n",
    "print(\"Total number of test examples: {}\".format(len(y_test)))\n",
    "print(\"Accuracy: {:g}\".format(correct_predictions/float(len(y_test))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
