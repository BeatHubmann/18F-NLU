{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T09:55:46.262255Z",
     "start_time": "2018-03-22T09:55:46.239730Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"1600\"\n",
       "            src=\"./assignment5.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x104362748>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"./assignment5.pdf\", width=800, height=1600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T14:55:20.186376Z",
     "start_time": "2018-03-20T14:55:20.178823Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T14:55:20.834862Z",
     "start_time": "2018-03-20T14:55:20.831772Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_string(string):\n",
    "    return string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T16:30:43.768783Z",
     "start_time": "2018-03-20T16:30:43.577854Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_and_labels(data_file_path, max_vocabSize, past_words):\n",
    "    \"\"\"\n",
    "    Loads training data, creates vocabulary and returns the respective ids for words and tags\n",
    "    \"\"\"\n",
    "    # Load data from file\n",
    "    cwd = os.getcwd()\n",
    "    # Collect word counts and unique PoS tags\n",
    "    word_counts = Counter()\n",
    "    unique_posTags = set()\n",
    "    with open(cwd+data_file_path, \"r\") as tagged_sentences:\n",
    "        for sentence in tagged_sentences:\n",
    "            for tag in sentence.strip().split(\" \"):\n",
    "                splitted_tag = tag.split(\"/\")\n",
    "                if len(splitted_tag) != 2:\n",
    "                    continue\n",
    "                word = clean_string(splitted_tag[0])\n",
    "                pos = splitted_tag[1]\n",
    "                unique_posTags.add(pos) # collect all unique PoS tags\n",
    "                if word in word_counts: # collect word frequencies (used later to prune vocabulary)\n",
    "                    word_counts[word] += 1\n",
    "                else:\n",
    "                    word_counts[word] = 1\n",
    "    # Prune vocabulary to max_vocabSize\n",
    "    words_toKeep = [tupl[0] for tupl in word_counts.most_common(max_vocabSize-1)]\n",
    "    # Create mapping from words/PoS tags to ids\n",
    "    word_toId = {word: i for i, word in enumerate(words_toKeep, 1)}\n",
    "    word_toId[\"<UNK>\"] = 0 # add unknown token to vocabulary (all words not contained in it will be mapped to this)\n",
    "    pos_toId = {pos: i for i, pos in enumerate(list(unique_posTags))}\n",
    "    # Save vocabulary and PoS tags ids for evaluation\n",
    "    if not os.path.exists(cwd+\"/vocab\"):\n",
    "        os.makedirs(cwd+\"/vocab\")\n",
    "    with open(cwd+\"/vocab/wordIds.pkl\", \"wb\") as f:\n",
    "        pickle.dump(word_toId, f)\n",
    "    with open(cwd+\"/vocab/posIds.pkl\", \"wb\") as f:\n",
    "        pickle.dump(pos_toId, f)\n",
    "    # Replace each word with the id of the previous \"past_words\" words\n",
    "    # and replace each PoS tag by its respective id\n",
    "    x = []\n",
    "    y = []\n",
    "    with open(cwd+data_file_path, \"r\") as tagged_sentences:\n",
    "        for sentence in tagged_sentences:\n",
    "            pairs = sentence.strip().split(\" \")\n",
    "            words_and_tags = list(pair.split(\"/\") for pair in pairs if len(pair.split(\"/\")) == 2)\n",
    "            if len(words_and_tags) == 0:\n",
    "                continue\n",
    "            words, pos_tags = zip(*words_and_tags)\n",
    "            words = [clean_string(word) for word in words]\n",
    "            for j in range(len(words)):\n",
    "                y.append(pos_toId[ pos_tags[j] ])\n",
    "                pastWords_ids = []\n",
    "                for k in range(0, past_words+1): # for previous words\n",
    "                    if j-k < 0: # out of bounds\n",
    "                        pastWords_ids.append(0) # <UNK>\n",
    "                    elif words[j-k] in word_toId: # word in vocabulary\n",
    "                        pastWords_ids.append(word_toId[ words[j-k] ])\n",
    "                    else: # word not in vocabulary\n",
    "                        pastWords_ids.append(0) # <UNK>\t\n",
    "                x.append(pastWords_ids)\n",
    "\n",
    "    return [np.array(x), np.array(y), len(unique_posTags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T14:55:22.635662Z",
     "start_time": "2018-03-20T14:55:22.564891Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_and_labels_test(data_file_path, past_words):\n",
    "    \"\"\"\n",
    "    Loads test data and vocabulary and returns the respective ids for words and tags\n",
    "    \"\"\"\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    # Load vocabulary and PoS tags ids from training\n",
    "    if not os.path.exists(cwd+\"/vocab\"):\n",
    "        raise FileNotFoundError(\"You need to run train.py first in order to generate the vocabulary.\")\n",
    "    with open(cwd+\"/vocab/wordIds.pkl\", \"rb\") as f:\n",
    "        word_toId = pickle.load(f)\n",
    "    with open(cwd+\"/vocab/posIds.pkl\", \"rb\") as f:\n",
    "        pos_toId = pickle.load(f)\n",
    "    # Replace each word with the id of the previous \"past_words\" words\n",
    "    # and replace each PoS tag by its respective id\n",
    "    x = []\n",
    "    y = []\n",
    "    with open(cwd+data_file_path, \"r\") as tagged_sentences:\n",
    "        for sentence in tagged_sentences:\n",
    "            pairs = sentence.strip().split(\" \")\n",
    "            words, pos_tags = zip(*(pair.split(\"/\") for pair in pairs if len(pair.split(\"/\")) == 2))\n",
    "            for j in range(len(words)): # for each word in the sentence\n",
    "                if pos_tags[j] in pos_toId: \n",
    "                    y.append(pos_toId[ pos_tags[j] ])\n",
    "                else:\n",
    "                    y.append(0) # TODO: This is not correct, but we should have seen all posible output tags in advance...\n",
    "                pastWords_ids = []\n",
    "                for k in range(1, past_words+1): # for previous words\n",
    "                    if j-k < 0: # out of bounds\n",
    "                        pastWords_ids.append(0) # <UNK>\n",
    "                    elif words[j-k] in word_toId: # word in vocabulary\n",
    "                        pastWords_ids.append(word_toId[ words[j-k] ])\n",
    "                    else: # word not in vocabulary\n",
    "                        pastWords_ids.append(0) # <UNK>\t\n",
    "                x.append(pastWords_ids)\n",
    "\n",
    "    return [np.array(x), np.array(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T14:55:23.349684Z",
     "start_time": "2018-03-20T14:55:23.331615Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PosTagger: Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T15:45:31.281889Z",
     "start_time": "2018-03-20T15:45:31.112206Z"
    }
   },
   "outputs": [],
   "source": [
    "class PoSTagger(object):\n",
    "    \"\"\"\n",
    "    A simple PoS tagger implementation in Tensorflow.\n",
    "    Uses an embedding layer followed by a fully connected layer with ReLU and a softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, vocab_size, embedding_size, past_words): # sequence_length, filter_sizes, num_filters, l2_reg_lambda=0.0\n",
    "        # Minibatch placeholders for input and output\n",
    "        # The word indices of the window\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, past_words+1], name=\"input_x\")\n",
    "        # The target pos-tags\n",
    "        self.input_y = tf.placeholder(tf.int64, [None], name=\"input_y\") \n",
    "\n",
    "        with tf.device('/gpu:0'):\n",
    "\n",
    "            # Embedding layer\n",
    "            with tf.name_scope(\"embedding\"):\n",
    "                # Create an embedding matrix: |vocab_size x embedding_size|\n",
    "                embedding_matrix = tf.get_variable('embedding_matrix',\n",
    "                   shape=[vocab_size, embedding_size],\n",
    "                   initializer=tf.random_uniform_initializer(minval=-1, maxval=1))\n",
    "                \n",
    "                # Create word embeddings tensor with embedding matrix lookup:\n",
    "                # |None x past_words x embedding_size|\n",
    "                embeddings = tf.nn.embedding_lookup(embedding_matrix,\n",
    "                                                         self.input_x,\n",
    "                                                         name='embedding_lookup')\n",
    "                \n",
    "                # Create feature vector:\n",
    "                # Flatten out embeddings tensor to get e/a input sequence on row:\n",
    "                # |None x (past_words+1)*embedding_size|\n",
    "                x_flat_dim = (past_words+1)*embedding_size\n",
    "                self.x_flat = tf.reshape(embeddings, [-1, x_flat_dim])\n",
    "                \n",
    "            # Fully connected layer with ReLU \n",
    "            with tf.name_scope(\"model\"):\n",
    "                # Set initializer handle for readability below\n",
    "                xavi = tf.contrib.layers.xavier_initializer()\n",
    "                \n",
    "                # Set size d' for (first) hidden layer: Heuristic guess\n",
    "                d_prime = embedding_size\n",
    "                \n",
    "                # Create/get weight matrix and bias vector for hidden layer\n",
    "                W_1 = tf.get_variable('W_1', [x_flat_dim, d_prime], initializer=xavi)\n",
    "                b_1 = tf.get_variable('b_1', [d_prime], initializer=tf.zeros_initializer)\n",
    "                \n",
    "                # Send feature vector through hidden layer\n",
    "                out_1 = tf.nn.relu(tf.nn.xw_plus_b(self.x_flat, W_1, b_1))\n",
    "                \n",
    "                # Compute softmax logits :\n",
    "                # Create/get weight matrix and bias vector for softmax layer\n",
    "                W_softmax = tf.get_variable('W_softmax', [d_prime, num_classes], initializer=xavi)\n",
    "                b_softmax = tf.get_variable('b_softmax', [num_classes], initializer=tf.zeros_initializer)\n",
    "                \n",
    "                # Calculate logits\n",
    "                self.logits = tf.nn.xw_plus_b(out_1, W_softmax, b_softmax)\n",
    "                \n",
    "                # Apply softmax on logits to get predicted probabilities\n",
    "                self.predictions = tf.nn.softmax(self.logits, name='predictions')\n",
    "                \n",
    "                # Find most probable prediction along axis=1\n",
    "                self.best_prediction = tf.argmax(self.predictions, 1, name='best_prediction')\n",
    "                \n",
    "                # Compute the mean loss using tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "                self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=self.logits, labels=self.input_y))\n",
    "\n",
    "            # Calculate accuracy\n",
    "            with tf.name_scope(\"accuracy\"):\n",
    "                # TODO compute the average accuracy over the batch (remember tf.argmax and tf.equal)\n",
    "                self.correct_predictions = tf.cast(tf.equal(self.best_prediction,\n",
    "                                                            self.input_y),\n",
    "                                                   'float')\n",
    "                self.accuracy = tf.reduce_mean(self.correct_predictions, name='accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T16:29:30.977272Z",
     "start_time": "2018-03-20T16:29:30.859171Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConfigTrain(object):\n",
    "    \"\"\"\n",
    "    Replacement class for training flags abomination originally given\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 dev_sample_percentage=.1,\n",
    "                 data_file_path='/data/corpus.small',\n",
    "                 embedding_dim=128,\n",
    "                 vocab_size=50000,\n",
    "                 past_words=3,\n",
    "                 batch_size=64,\n",
    "                 num_epochs=200,\n",
    "                 evaluate_every=100,\n",
    "                 checkpoint_every=100,\n",
    "                 num_checkpoints=5,\n",
    "                 allow_soft_placement=True,\n",
    "                 log_device_placement=False):\n",
    "        \n",
    "        # Percentage of the training data used for validation (default: 10%)\n",
    "        self.dev_sample_percentage=dev_sample_percentage\n",
    "        \n",
    "        # Path to the training data \n",
    "        self.data_file_path=data_file_path\n",
    "        \n",
    "        # Dimensionality of word embeddings (default: 128)\n",
    "        self.embedding_dim=embedding_dim\n",
    "        \n",
    "        # Size of the vocabulary (default: 50k)\n",
    "        self.vocab_size=vocab_size\n",
    "        \n",
    "        # How many previous words are used for prediction (default: 3)\n",
    "        self.past_words=past_words\n",
    "        \n",
    "        # Batch Size (default: 64)\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        # Number of training epochs (default: 200)\n",
    "        self.num_epochs=num_epochs\n",
    "        \n",
    "        # Evaluate model on dev set after this many steps (default: 100)\n",
    "        self.evaluate_every=evaluate_every\n",
    "        \n",
    "        # Save model after this many steps (default: 100)\n",
    "        self.checkpoint_every=checkpoint_every\n",
    "        \n",
    "        # Number of checkpoints to store (default: 5)\n",
    "        self.num_checkpoints=num_checkpoints\n",
    "        \n",
    "        # Allow device soft device placement\n",
    "        self.allow_soft_placement=allow_soft_placement\n",
    "        \n",
    "        # Log placement of ops on devices\n",
    "        self.log_device_placement=log_device_placement\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T16:31:42.471792Z",
     "start_time": "2018-03-20T16:30:49.045039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing training and dev datasets \n",
      "\n",
      "Done \n",
      "\n",
      "Writing to /Users/z7717/Dropbox/Git/OwnRepos/18F-NLU/assignment_4/runs/1521563490\n",
      "\n",
      "Training step:    1 \t loss: 3.918804 \t accuracy: 0.000000 \t 0.089248 sec/batch\n",
      "Training step:    2 \t loss: 3.664735 \t accuracy: 0.000000 \t 0.042374 sec/batch\n",
      "Training step:    3 \t loss: 3.529639 \t accuracy: 0.140625 \t 0.043224 sec/batch\n",
      "Training step:    4 \t loss: 3.602353 \t accuracy: 0.062500 \t 0.042690 sec/batch\n",
      "Training step:    5 \t loss: 3.440485 \t accuracy: 0.140625 \t 0.043103 sec/batch\n",
      "Training step:    6 \t loss: 3.375297 \t accuracy: 0.093750 \t 0.043877 sec/batch\n",
      "Training step:    7 \t loss: 3.486463 \t accuracy: 0.093750 \t 0.064447 sec/batch\n",
      "Training step:    8 \t loss: 3.252808 \t accuracy: 0.140625 \t 0.042779 sec/batch\n",
      "Training step:    9 \t loss: 3.359285 \t accuracy: 0.203125 \t 0.052125 sec/batch\n",
      "Training step:   10 \t loss: 3.064720 \t accuracy: 0.250000 \t 0.046288 sec/batch\n",
      "Training step:   11 \t loss: 3.037189 \t accuracy: 0.296875 \t 0.067895 sec/batch\n",
      "Training step:   12 \t loss: 3.049788 \t accuracy: 0.187500 \t 0.042755 sec/batch\n",
      "Training step:   13 \t loss: 3.043957 \t accuracy: 0.296875 \t 0.042916 sec/batch\n",
      "Training step:   14 \t loss: 3.182301 \t accuracy: 0.203125 \t 0.043247 sec/batch\n",
      "Training step:   15 \t loss: 2.771992 \t accuracy: 0.312500 \t 0.063890 sec/batch\n",
      "Training step:   16 \t loss: 2.707042 \t accuracy: 0.359375 \t 0.042601 sec/batch\n",
      "Training step:   17 \t loss: 2.758004 \t accuracy: 0.265625 \t 0.042890 sec/batch\n",
      "Training step:   18 \t loss: 2.526383 \t accuracy: 0.437500 \t 0.042535 sec/batch\n",
      "Training step:   19 \t loss: 2.752577 \t accuracy: 0.359375 \t 0.043204 sec/batch\n",
      "Training step:   20 \t loss: 2.624328 \t accuracy: 0.359375 \t 0.046005 sec/batch\n",
      "Training step:   21 \t loss: 2.873261 \t accuracy: 0.296875 \t 0.065314 sec/batch\n",
      "Training step:   22 \t loss: 2.663666 \t accuracy: 0.328125 \t 0.043434 sec/batch\n",
      "Training step:   23 \t loss: 2.343828 \t accuracy: 0.453125 \t 0.042398 sec/batch\n",
      "Training step:   24 \t loss: 2.731730 \t accuracy: 0.375000 \t 0.042196 sec/batch\n",
      "Training step:   25 \t loss: 2.858427 \t accuracy: 0.281250 \t 0.068711 sec/batch\n",
      "Training step:   26 \t loss: 2.527228 \t accuracy: 0.312500 \t 0.047940 sec/batch\n",
      "Training step:   27 \t loss: 2.283905 \t accuracy: 0.421875 \t 0.043007 sec/batch\n",
      "Training step:   28 \t loss: 2.582113 \t accuracy: 0.281250 \t 0.042791 sec/batch\n",
      "Training step:   29 \t loss: 2.417602 \t accuracy: 0.406250 \t 0.050230 sec/batch\n",
      "Training step:   30 \t loss: 2.170051 \t accuracy: 0.390625 \t 0.062635 sec/batch\n",
      "Training step:   31 \t loss: 2.340925 \t accuracy: 0.390625 \t 0.052753 sec/batch\n",
      "Training step:   32 \t loss: 2.338267 \t accuracy: 0.406250 \t 0.048872 sec/batch\n",
      "Training step:   33 \t loss: 2.198925 \t accuracy: 0.390625 \t 0.048059 sec/batch\n",
      "Training step:   34 \t loss: 2.180394 \t accuracy: 0.484375 \t 0.044491 sec/batch\n",
      "Training step:   35 \t loss: 2.285947 \t accuracy: 0.437500 \t 0.069705 sec/batch\n",
      "Training step:   36 \t loss: 2.435623 \t accuracy: 0.359375 \t 0.044170 sec/batch\n",
      "Training step:   37 \t loss: 2.349881 \t accuracy: 0.296875 \t 0.046129 sec/batch\n",
      "Training step:   38 \t loss: 2.273388 \t accuracy: 0.421875 \t 0.046364 sec/batch\n",
      "Training step:   39 \t loss: 2.030553 \t accuracy: 0.437500 \t 0.045778 sec/batch\n",
      "Training step:   40 \t loss: 2.141571 \t accuracy: 0.468750 \t 0.050535 sec/batch\n",
      "Training step:   41 \t loss: 2.185412 \t accuracy: 0.406250 \t 0.060062 sec/batch\n",
      "Training step:   42 \t loss: 1.922891 \t accuracy: 0.453125 \t 0.042908 sec/batch\n",
      "Training step:   43 \t loss: 2.093289 \t accuracy: 0.453125 \t 0.044998 sec/batch\n",
      "Training step:   44 \t loss: 1.976320 \t accuracy: 0.484375 \t 0.046315 sec/batch\n",
      "Training step:   45 \t loss: 1.869645 \t accuracy: 0.515625 \t 0.073134 sec/batch\n",
      "Training step:   46 \t loss: 1.928333 \t accuracy: 0.578125 \t 0.069357 sec/batch\n",
      "Training step:   47 \t loss: 2.062724 \t accuracy: 0.421875 \t 0.046271 sec/batch\n",
      "Training step:   48 \t loss: 2.004559 \t accuracy: 0.406250 \t 0.054849 sec/batch\n",
      "Training step:   49 \t loss: 1.684623 \t accuracy: 0.546875 \t 0.048010 sec/batch\n",
      "Training step:   50 \t loss: 2.152313 \t accuracy: 0.421875 \t 0.103288 sec/batch\n",
      "Training step:   51 \t loss: 2.297420 \t accuracy: 0.390625 \t 0.051526 sec/batch\n",
      "Training step:   52 \t loss: 1.846138 \t accuracy: 0.500000 \t 0.045653 sec/batch\n",
      "Training step:   53 \t loss: 1.938648 \t accuracy: 0.453125 \t 0.074484 sec/batch\n",
      "Training step:   54 \t loss: 2.099803 \t accuracy: 0.453125 \t 0.045331 sec/batch\n",
      "Training step:   55 \t loss: 2.134058 \t accuracy: 0.453125 \t 0.061973 sec/batch\n",
      "Training step:   56 \t loss: 1.825870 \t accuracy: 0.515625 \t 0.047358 sec/batch\n",
      "Training step:   57 \t loss: 2.185419 \t accuracy: 0.406250 \t 0.064686 sec/batch\n",
      "Training step:   58 \t loss: 1.710793 \t accuracy: 0.484375 \t 0.045312 sec/batch\n",
      "Training step:   59 \t loss: 1.959885 \t accuracy: 0.484375 \t 0.047289 sec/batch\n",
      "Training step:   60 \t loss: 1.718514 \t accuracy: 0.515625 \t 0.047638 sec/batch\n",
      "Training step:   61 \t loss: 2.232948 \t accuracy: 0.343750 \t 0.073051 sec/batch\n",
      "Training step:   62 \t loss: 2.207108 \t accuracy: 0.421875 \t 0.051364 sec/batch\n",
      "Training step:   63 \t loss: 1.831810 \t accuracy: 0.468750 \t 0.049786 sec/batch\n",
      "Training step:   64 \t loss: 1.852240 \t accuracy: 0.515625 \t 0.075460 sec/batch\n",
      "Training step:   65 \t loss: 1.794354 \t accuracy: 0.500000 \t 0.075475 sec/batch\n",
      "Training step:   66 \t loss: 2.040625 \t accuracy: 0.453125 \t 0.046782 sec/batch\n",
      "Training step:   67 \t loss: 1.736259 \t accuracy: 0.500000 \t 0.046554 sec/batch\n",
      "Training step:   68 \t loss: 2.119689 \t accuracy: 0.453125 \t 0.046207 sec/batch\n",
      "Training step:   69 \t loss: 1.709975 \t accuracy: 0.500000 \t 0.075102 sec/batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-6a521b2a2a72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-6a521b2a2a72>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_batch, y_batch)\u001b[0m\n\u001b[1;32m     89\u001b[0m             _, step, summaries, loss, accuracy = sess.run(\n\u001b[1;32m     90\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 feed_dict)\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## DATA PREPARATION ##\n",
    "\n",
    "CONFIG = ConfigTrain()\n",
    "\n",
    "# Load data\n",
    "print(\"Loading and preprocessing training and dev datasets \\n\")\n",
    "x, y, num_outputTags = load_data_and_labels(CONFIG.data_file_path, CONFIG.vocab_size, CONFIG.past_words)\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffled_indices = np.random.permutation(len(y))\n",
    "x_shuffled = x[shuffled_indices]\n",
    "y_shuffled = y[shuffled_indices]\n",
    "\n",
    "# Split train/dev sets\n",
    "dev_sample_index = -1 * int(CONFIG.dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "# Generate training batches\n",
    "batches = batch_iter(list(zip(x_train, y_train)), CONFIG.batch_size, CONFIG.num_epochs)\n",
    "print(\"Done \\n\")\n",
    "\n",
    "## MODEL AND TRAINING PROCEDURE DEFINITION ##\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=CONFIG.allow_soft_placement,\n",
    "        log_device_placement=CONFIG.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Initialize model\n",
    "        pos_tagger = PoSTagger(\n",
    "            num_classes=num_outputTags, \n",
    "            vocab_size=CONFIG.vocab_size, \n",
    "            embedding_size=CONFIG.embedding_dim, \n",
    "            past_words=CONFIG.past_words\n",
    "        )\n",
    "\n",
    "        # Define training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        \n",
    "        # Define an optimizer, e.g. AdamOptimizer\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        \n",
    "        # Define an optimizer step\n",
    "        train_op = optimizer.minimize(pos_tagger.loss, global_step=global_step)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", pos_tagger.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", pos_tagger.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory (Tensorflow assumes this directory already exists so we need to create it)\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=CONFIG.num_checkpoints)\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.graph.finalize()\n",
    "\n",
    "        # Define training and dev steps (batch) \n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            start = time.time()\n",
    "            feed_dict = {\n",
    "                pos_tagger.input_x: x_batch,\n",
    "                pos_tagger.input_y: y_batch\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, pos_tagger.loss, pos_tagger.accuracy],\n",
    "                feed_dict)\n",
    "            stop = time.time()\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"Training step: {:4d} \\t loss: {:3f} \\t accuracy: {:3f} \\t {:3f} sec/batch\".format(step, loss, accuracy, (stop-start)))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "                pos_tagger.input_x: x_batch,\n",
    "                pos_tagger.input_y: y_batch\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, pos_tagger.loss, pos_tagger.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"Evaluation step: {:4d} \\t loss: {:3f} \\t accuracy: {:3f}\".format(step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        ## TRAINING LOOP ##\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % CONFIG.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % CONFIG.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T16:31:55.823033Z",
     "start_time": "2018-03-20T16:31:55.796864Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConfigTest(object):\n",
    "    \"\"\"\n",
    "    Replacement class for testing flags abomination originally given\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 data_file_path='/data/corpus.small',\n",
    "                 past_words=3,\n",
    "                 batch_size=64,\n",
    "                 checkpoint_dir='./runs/xxxxx/checkpoints/',\n",
    "                 allow_soft_placement=True,\n",
    "                 log_device_placement=False):\n",
    "        \n",
    "        # Path to the test data \n",
    "        self.data_file_path=data_file_path\n",
    "        \n",
    "        # How many previous words are used for prediction (default: 3)\n",
    "        self.past_words=past_words\n",
    "        \n",
    "        # Batch Size (default: 64)\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        # Checkpoint directory from training run\n",
    "        self.checkpoint_dir=checkpoint_dir\n",
    "        \n",
    "        # Allow device soft device placement\n",
    "        self.allow_soft_placement=allow_soft_placement\n",
    "        \n",
    "        # Log placement of ops on devices\n",
    "        self.log_device_placement=log_device_placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T16:32:15.944286Z",
     "start_time": "2018-03-20T16:31:56.961214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing test dataset \n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "File None.meta does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-05f239591cd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Load the saved meta graph and restore variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}.meta\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1918\u001b[0m                        \"execution is enabled.\")\n\u001b[1;32m   1919\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1920\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_meta_graph_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1921\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mread_meta_graph_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    630\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File %s does not exist.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m   \u001b[0;31m# First try to read it as a binary file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: File None.meta does not exist."
     ]
    }
   ],
   "source": [
    "## DATA PREPARATION ##\n",
    "\n",
    "CONFIG = ConfigTest()\n",
    "\n",
    "# Load data\n",
    "print(\"Loading and preprocessing test dataset \\n\")\n",
    "x_test, y_test = load_data_and_labels_test(CONFIG.data_file_path, CONFIG.past_words)\n",
    "\n",
    "## EVALUATION ##\n",
    "\n",
    "checkpoint_file = tf.train.latest_checkpoint(CONFIG.checkpoint_dir)\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=CONFIG.allow_soft_placement,\n",
    "        log_device_placement=CONFIG.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"accuracy\").outputs[0]\n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        batches = data_utils.batch_iter(list(x_test), CONFIG.batch_size, 1, shuffle=False)\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "\n",
    "        for x_test_batch in batches:\n",
    "            batch_predictions = sess.run(predictions, {input_x: x_test_batch})\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "\n",
    "# Print accuracy\n",
    "correct_predictions = float(sum(all_predictions == y_test))\n",
    "print(\"Total number of test examples: {}\".format(len(y_test)))\n",
    "print(\"Accuracy: {:g}\".format(correct_predictions/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
