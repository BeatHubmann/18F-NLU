{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import IFrame\n",
    "# IFrame('project2.pdf', width=600, height=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this notebook, make sure you have placed the following into a ```./data/``` subdirectory:\n",
    "- Place all cloze task files for training and validation as in the polybox directory into ```./data/cloze/```\n",
    "- Download and place Stanford's GloVe 6B vector set [glove.6B](http://nlp.stanford.edu/data/glove.6B.zip) as in the polybox directory into ```./data/glove/```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T09:01:22.018863Z",
     "start_time": "2018-05-18T09:01:22.013875Z"
    }
   },
   "source": [
    "# Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T09:01:06.640116Z",
     "start_time": "2018-05-18T09:01:06.633210Z"
    }
   },
   "source": [
    "# Config variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SENT_IN_STORY = 5\n",
    "ENCODER_DIM = 600\n",
    "MAX_SEQ_LENGTH = 30\n",
    "MAX_VOCAB_SIZE = 400000\n",
    "EMBEDDING_DIM = 300\n",
    "ENCODER_TYPE = 'GRU'\n",
    "OOV_TOKEN = '<unk>'\n",
    "CONTEXT_SIZE = 1\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "CLIP_GRAD_NORM = 5.0\n",
    "\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 100\n",
    "DROPOUT_RATE = 0.25\n",
    "SAVE_DIR = './checkpoints/'\n",
    "SUMMARIES_DIR = './summaries/'\n",
    "MAX_TO_KEEP = 1\n",
    "DISPLAY_STEP = 100\n",
    "VALIDATE_STEP = 20\n",
    "SAVE_STEP = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec(word2vec_file='./data/glove.6B/glove.6B.{}d.txt'.format(EMBEDDING_DIM)):\n",
    "    word2vec = {}\n",
    "#     with open(os.path.join(word2vec_file), encoding='utf8') as f:\n",
    "    with open(word2vec_file, encoding='utf8') as f:\n",
    "        for row in f:\n",
    "            values = row.split()\n",
    "            word = values[0]\n",
    "            vec = np.asarray(values[1:], dtype='float32')\n",
    "            word2vec[word] = vec\n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(train_file='./data/cloze/train_stories.csv'):\n",
    "    train = pd.read_csv(train_file)\n",
    "    train.drop_duplicates(subset='storyid') # make sure we have no duplicates\n",
    "    titles = np.expand_dims(train['storytitle'].values, axis=1)\n",
    "    sentences_1 = np.expand_dims(train['sentence1'].values, axis=1)\n",
    "    sentences_2 = np.expand_dims(train['sentence2'].values, axis=1)\n",
    "    sentences_3 = np.expand_dims(train['sentence3'].values, axis=1)\n",
    "    sentences_4 = np.expand_dims(train['sentence4'].values, axis=1)\n",
    "    sentences_5 = np.expand_dims(train['sentence5'].values, axis=1)\n",
    "    mains = np.column_stack((sentences_1, sentences_2, sentences_3, sentences_4))\n",
    "    stories = np.hstack((mains, sentences_5))\n",
    "    sentences = [s for story in stories for s in story]\n",
    "    print('{} has {} stories with a total of {} sentences.'.format(train_file,\n",
    "                                                                   len(stories),\n",
    "                                                                   len(sentences)))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_data(val_file='./data/cloze/cloze_test_val__spring2016 - cloze_test_ALL_val.csv'):\n",
    "    validation = pd.read_csv(val_file)\n",
    "    sentences_4 = np.expand_dims(validation['InputSentence4'].values, axis=1)\n",
    "    quiz_1 = np.expand_dims(validation['RandomFifthSentenceQuiz1'].values, axis=1)\n",
    "    quiz_2 = np.expand_dims(validation['RandomFifthSentenceQuiz2'].values, axis=1)\n",
    "    answers = np.expand_dims(validation['AnswerRightEnding'].values, axis=1)\n",
    "    quizzes = np.hstack((sentences_4, quiz_1, quiz_2))\n",
    "    sentences = [s for quiz in quizzes for s in quiz]\n",
    "    print('{} has {} quizzes with a total of {} sentences.'.format(val_file,\n",
    "                                                                   len(quizzes),\n",
    "                                                                   len(sentences)))\n",
    "    return sentences, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(test_file='./data/cloze/cloze_test_test__spring2016 - cloze_test_ALL_test.csv'):\n",
    "    test = pd.read_csv(test_file)\n",
    "    sentences_4 = np.expand_dims(test['InputSentence4'].values, axis=1)\n",
    "    quiz_1 = np.expand_dims(test['RandomFifthSentenceQuiz1'].values, axis=1)\n",
    "    quiz_2 = np.expand_dims(test['RandomFifthSentenceQuiz2'].values, axis=1)\n",
    "    answers = np.expand_dims(test['AnswerRightEnding'].values, axis=1)\n",
    "    quizzes = np.hstack((sentences_4, quiz_1, quiz_2))\n",
    "    sentences = [s for quiz in quizzes for s in quiz]\n",
    "    print('{} has {} quizzes with a total of {} sentences.'.format(test_file,\n",
    "                                                                   len(quizzes),\n",
    "                                                                   len(sentences)))\n",
    "    return sentences, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an ugly-but-necessary hack thx to the NLU test data being of a different format than all the other data files\n",
    "def get_NLU_test_data(test_file='./data/cloze/test_nlu18.csv'):\n",
    "    test = pd.read_csv(test_file, header=None, encoding='latin-1')\n",
    "    sentences_4 = np.expand_dims(test[3].values, axis=1)\n",
    "    quiz_1 = np.expand_dims(test[4].values, axis=1)\n",
    "    quiz_2 = np.expand_dims(test[5].values, axis=1)\n",
    "    quizzes = np.hstack((sentences_4, quiz_1, quiz_2))\n",
    "    sentences = [s for quiz in quizzes for s in quiz]\n",
    "    print('{} has {} quizzes with a total of {} sentences.'.format(test_file,\n",
    "                                                                   len(quizzes),\n",
    "                                                                   len(sentences)))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_sequence(text):\n",
    "    filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    split = ' '\n",
    "    text = text.lower().translate({ord(c): split for c in filters})\n",
    "    seq = text.split(split)\n",
    "    return [t for t in seq if t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocabs(texts):\n",
    "    word_counts = OrderedDict()\n",
    "    max_seq_len = 0\n",
    "    for text in texts:\n",
    "        if isinstance(text, list):\n",
    "            seq = text\n",
    "        else:\n",
    "            seq = text_to_word_sequence(text)\n",
    "        if len(seq) > max_seq_len:\n",
    "            max_seq_len = len(seq)\n",
    "        for w in seq:\n",
    "            if w in word_counts:\n",
    "                word_counts[w] += 1\n",
    "            else:\n",
    "                word_counts[w] = 1\n",
    "    word_counts = list(word_counts.items())\n",
    "    word_counts.sort(key = lambda x: x[1], reverse=True)\n",
    "    sorted_vocab = [word_count[0] for word_count in word_counts]\n",
    "    word2idx = dict(list(zip(sorted_vocab, list(range(1, len(sorted_vocab) + 1)))))\n",
    "    i = word2idx.get(OOV_TOKEN)\n",
    "    if i is None:\n",
    "        word2idx[OOV_TOKEN] = len(word2idx) + 1\n",
    "    idx2word = {value : key for key, value in word2idx.items()}\n",
    "    return word2idx, idx2word, max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_pad_mask(texts, seq_length):\n",
    "    vectors, masks = [], []\n",
    "    for text in texts:\n",
    "        if isinstance(text, list):\n",
    "            seq = text\n",
    "        else:\n",
    "            seq = text_to_word_sequence(text)\n",
    "        seq = seq[:seq_length]\n",
    "        vector, mask  = [], []\n",
    "        for w in seq:\n",
    "            vector.append(word2idx.get(w, word2idx[OOV_TOKEN]))\n",
    "            mask.append(1)\n",
    "        while len(vector) < seq_length:\n",
    "            vector.append(0)\n",
    "            mask.append(0)\n",
    "        vectors.append(vector)\n",
    "        masks.append(mask)\n",
    "    return np.array(vectors, dtype='int64'), np.array(masks, dtype='int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    encoder_inputs = tf.placeholder(tf.int64, [None, None], name='encoder_inputs')\n",
    "    encoder_input_masks = tf.placeholder(tf.int8, [None, None], name='input_masks')\n",
    "    encoder_targets = tf.placeholder(tf.float32, [None, None], name='encoder_targets')\n",
    "    label_weights = tf.placeholder(tf.float32, [None,], name='label_weights')\n",
    "    dropout_rate = tf.placeholder(tf.float32, [], name='dropout_rate')\n",
    "    return encoder_inputs, encoder_input_masks, encoder_targets, label_weights, dropout_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(num_words):\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    for word, idx in word2idx.items():\n",
    "        if idx < num_words:\n",
    "            embedding_vector = word2vec.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[idx] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(encode_ids, trainable=False):\n",
    "    word_embeddings = []\n",
    "    encode_emb = []\n",
    "    for suffix in ['_f', '_g']:\n",
    "        word_emb = tf.get_variable(name='word_embedding'+suffix,\n",
    "                                   shape=embedding_matrix.shape,\n",
    "                                   trainable=trainable)\n",
    "        word_emb.assign(embedding_matrix)\n",
    "        word_embeddings.append(word_emb)\n",
    "        encode_ = tf.nn.embedding_lookup(word_emb, encode_ids)\n",
    "        encode_emb.append(encode_)\n",
    "    return word_embeddings, encode_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rnn_cells(num_units, cell_type):\n",
    "    if cell_type == 'GRU':\n",
    "        return tf.nn.rnn_cell.GRUCell(num_units=num_units)\n",
    "    elif cell_type == 'LSTM':\n",
    "        return tf.nn.rnn_cell.LSTMCell(num_units=num_units)\n",
    "    else:\n",
    "        raise ValueError('Invalid cell type given')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_encoder(embeds, mask, scope, num_units=600, cell_type='GRU'):\n",
    "    sequence_length = tf.to_int32(tf.reduce_sum(mask, 1), name='length')\n",
    "    cell_fw = make_rnn_cells(num_units, cell_type)\n",
    "    cell_bw = make_rnn_cells(num_units, cell_type)\n",
    "    outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_fw,\n",
    "                                                      cell_bw=cell_bw,\n",
    "                                                      inputs=embeds,\n",
    "                                                      sequence_length=sequence_length,\n",
    "                                                      dtype=tf.float32,\n",
    "                                                      scope=scope)\n",
    "    if cell_type == 'LSTM':\n",
    "        states = [states[0][1], states[1][1]]\n",
    "    state = tf.concat(states, 1)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_encoder(embeds, mask):\n",
    "    mask_expand = tf.expand_dims(tf.cast(mask, tf.float32), -1)\n",
    "    embeds_masked = embeds * mask_expand\n",
    "    return tf.reduce_sum(embeds_masked, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thought_vectors(encode_emb, encode_mask):\n",
    "    suffixes = ['_f', '_g']\n",
    "    thought_vectors = []\n",
    "    for i in range(len(suffixes)):\n",
    "        with tf.variable_scope('encoder' + suffixes[i]) as scope:\n",
    "            if ENCODER_TYPE == 'GRU':\n",
    "                encoded = rnn_encoder(encode_emb[i], encode_mask, scope,\n",
    "                                     ENCODER_DIM, ENCODER_TYPE)\n",
    "            elif ENCODER_TYPE == 'LSTM':\n",
    "                encoded = rnn_encoder(encode_emb[i], encode_mask, scope,\n",
    "                                     ENCODER_DIM, ENCODER_TYPE)\n",
    "            elif ENCODER_TYPE == 'bow':\n",
    "                encoded = bow_encoder(encode_emb[i], encode_mask)\n",
    "            else:\n",
    "                raise ValueError('Invalid encoder type given')\n",
    "\n",
    "        thought_vector = tf.identity(encoded, name='thought_vector' + suffixes[i])\n",
    "        thought_vectors.append(thought_vector)\n",
    "    return thought_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets_weights(batch_size, is_cloze, n_sent_in_story, is_quiz=False, quiz_answer=None):\n",
    "    if is_quiz:\n",
    "        assert quiz_answer in [1, 2], 'must indicate correct quiz answer'\n",
    "        targets = np.zeros((3, 3), dtype='float32')\n",
    "        targets[0, quiz_answer] = 1\n",
    "        targets[quiz_answer, 0] = 1\n",
    "        weights = np.array([1,0])\n",
    "    else:\n",
    "        context_idx = list(range(-CONTEXT_SIZE, CONTEXT_SIZE + 1))\n",
    "        context_idx.remove(0)\n",
    "        weights = np.ones(batch_size - 1)\n",
    "        if is_cloze:\n",
    "            sub_targets = np.zeros((n_sent_in_story, n_sent_in_story), dtype='float32')    \n",
    "            for i in context_idx:\n",
    "                sub_targets += np.eye(n_sent_in_story, k=i)\n",
    "            targets = np.zeros((batch_size, batch_size), dtype='float32')\n",
    "            weights = np.ones(batch_size - 1)\n",
    "            for i in range(n_sent_in_story - 1, len(weights), n_sent_in_story):\n",
    "                weights[i] = 0\n",
    "            for i in range(0, batch_size, n_sent_in_story):\n",
    "                targets[i:i+n_sent_in_story, i:i+n_sent_in_story] += sub_targets\n",
    "        else:\n",
    "            targets = np.zeros((batch_size, batch_size), dtype='float32')\n",
    "            for i in context_idx:\n",
    "                targets += np.eye(batch_size, k=i)\n",
    "        targets /= np.sum(targets, axis=1, keepdims=True)\n",
    "    return targets, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(thought_vectors, dropout_rate):\n",
    "    def use_dropout():\n",
    "        a, b = thought_vectors[0], thought_vectors[1]\n",
    "        dropout_mask_shape = tf.transpose(tf.shape(a))\n",
    "        dropout_mask = tf.random_uniform(dropout_mask_shape) > DROPOUT_RATE\n",
    "        dropout_mask = tf.where(dropout_mask,\n",
    "                                tf.ones(dropout_mask_shape),\n",
    "                                tf.zeros(dropout_mask_shape))\n",
    "        dropout_mask *= (1/dropout_rate)\n",
    "        a *= dropout_mask\n",
    "        b *= dropout_mask\n",
    "        return a, b\n",
    "    def no_dropout():\n",
    "        return thought_vectors[0], thought_vectors[1]\n",
    "    a, b = tf.cond(dropout_rate > 0, use_dropout, no_dropout)\n",
    "\n",
    "    scores = tf.matmul(a, b, transpose_b=True)\n",
    "    scores = tf.matrix_set_diag(scores, tf.zeros_like(scores[0]))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_predictions(scores, n_sent_in_story=N_SENT_IN_STORY, is_cloze=True):\n",
    "    bwd_scores = scores[1:  ]\n",
    "    fwd_scores = scores[ :-1]\n",
    "    bwd_predictions = tf.to_int64(tf.argmax(bwd_scores, axis=1))\n",
    "    fwd_predictions = tf.to_int64(tf.argmax(fwd_scores, axis=1))\n",
    "    bwd_labels = tf.range(tf.shape(bwd_scores)[0])\n",
    "    fwd_labels = bwd_labels + 1\n",
    "    \n",
    "    return (bwd_labels, fwd_labels), (bwd_predictions, fwd_predictions)#, label_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_acc(labels, predictions, label_weights):\n",
    "    total_weight = tf.reduce_sum(label_weights)\n",
    "    bwd_acc = tf.cast(tf.equal(tf.to_int64(labels[0]) , predictions[0]), tf.float32)\n",
    "    bwd_acc *= label_weights\n",
    "    bwd_acc = tf.reduce_sum(bwd_acc)\n",
    "    bwd_acc /= total_weight\n",
    "    fwd_acc = tf.cast(tf.equal(tf.to_int64(labels[1]), predictions[1]), tf.float32)\n",
    "    fwd_acc *= label_weights\n",
    "    fwd_acc = tf.reduce_sum(fwd_acc)\n",
    "    fwd_acc /= total_weight\n",
    "    return bwd_acc, fwd_acc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(inputs, masks, batch_size, is_cloze=True, n_sent_in_story=5, is_quiz=False, quiz_answers=None, shuffle=True):\n",
    "    if is_cloze:\n",
    "        assert (batch_size % n_sent_in_story) == 0, 'batch_size must be multiple of n_sent_in_story for cloze task training.'\n",
    "    rows, cols = inputs.shape\n",
    "    if shuffle and is_cloze and not is_quiz:        \n",
    "        row_blocks = rows // n_sent_in_story\n",
    "        shuffle_idx = np.random.permutation(row_blocks) \n",
    "        inputs = inputs.reshape((row_blocks, -1, cols))[shuffle_idx].reshape((-1, cols))\n",
    "        masks = masks.reshape((row_blocks, -1, cols))[shuffle_idx].reshape((-1, cols))\n",
    "    n_batches = len(inputs) // batch_size\n",
    "    for batch_i in range(n_batches):\n",
    "        start_i = batch_i * batch_size\n",
    "        batch_inputs = inputs[start_i : start_i + batch_size]\n",
    "        batch_masks = masks[start_i : start_i + batch_size]\n",
    "        if is_quiz:\n",
    "            batch_targets, batch_weights = get_targets_weights(batch_size, is_cloze, n_sent_in_story,\n",
    "                                                               is_quiz, quiz_answers[batch_i])\n",
    "        else:\n",
    "            batch_targets, batch_weights = get_targets_weights(batch_size, is_cloze, n_sent_in_story)\n",
    "        yield batch_inputs, batch_masks, batch_targets, batch_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading cloze story training data...')\n",
    "sentences = get_training_data()\n",
    "print('Loaded training stories.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading cloze story validation data...')\n",
    "validation_sentences, validation_answers = get_val_data()\n",
    "print('Loaded validation stories.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading cloze story test data...')\n",
    "test_sentences, test_answers = get_test_data()\n",
    "print('Loaded test stories.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading NLU 2018 story test data...')\n",
    "NLU_test_sentences = get_NLU_test_data()\n",
    "print('Loaded NLU 2018 test stories.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating vocabulary from training data ...')\n",
    "word2idx, idx2word, max_seq_len = generate_vocabs(sentences)\n",
    "print('Found {} unique word tokens\\n\\\n",
    "       Longest sentence has {} tokens.'.format(len(word2idx), max_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading pretrained word embedding vectors...')\n",
    "word2vec = get_word2vec()\n",
    "print('Loaded {} word vectors.'.format(len(word2vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\n",
    "\n",
    "print('Constructing embedding matrix...')\n",
    "embedding_matrix = get_embedding_matrix(num_words)\n",
    "print('Finished embedding matrix has shape {}.'.format(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq_length = min(max_seq_len, MAX_SEQ_LENGTH)\n",
    "\n",
    "print('Word2idx, pad and mask training sentences to length {} ...'.format(seq_length))\n",
    "enc_sentences, enc_masks = tokenize_pad_mask(sentences, seq_length)\n",
    "print('{} training sentences processed.'.format(len(enc_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Word2idx, pad and mask validation sentences to length {} ...'.format(seq_length))\n",
    "validation_inputs, validation_masks = tokenize_pad_mask(validation_sentences, seq_length)\n",
    "print('{} validation sentences sentences processed.'.format(len(validation_inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Word2idx, pad and mask test sentences to length {} ...'.format(seq_length))\n",
    "test_inputs, test_masks = tokenize_pad_mask(test_sentences, seq_length)\n",
    "print('{} test sentences sentences processed.'.format(len(test_inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Word2idx, pad and mask NLU 2018 test sentences to length {} ...'.format(seq_length))\n",
    "NLU_test_inputs, NLU_test_masks = tokenize_pad_mask(NLU_test_sentences, seq_length)\n",
    "print('{} NLU 2018 test sentences sentences processed.'.format(len(NLU_test_inputs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T09:49:55.821535Z",
     "start_time": "2018-05-20T09:49:55.817650Z"
    }
   },
   "source": [
    "# Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Building graph...')\n",
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "   \n",
    "    with tf.name_scope('input_data'):\n",
    "        encoder_inputs, encoder_input_masks, encoder_targets, label_weights, dropout_rate = get_inputs()\n",
    "        \n",
    "    with tf.name_scope('embeddings'):\n",
    "        word_embeddings, encode_emb = get_embeddings(encoder_inputs, trainable=True)\n",
    "        \n",
    "    with tf.name_scope('encoders'):\n",
    "        thoughts = get_thought_vectors(encode_emb, encoder_input_masks)\n",
    " \n",
    "    with tf.name_scope('losses_accuracies'):\n",
    "        scores = get_scores(thoughts, dropout_rate)\n",
    "        labels, predictions = get_labels_predictions(scores)\n",
    "\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(labels=encoder_targets,\n",
    "                                                       logits=scores))\n",
    "        tf.summary.scalar('batch_ent_loss', loss)\n",
    "        \n",
    "        bwd_acc, fwd_acc = get_batch_acc(labels, predictions, label_weights)\n",
    "        tf.summary.scalar('batch_bwd_accuracy', bwd_acc)\n",
    "        tf.summary.scalar('batch_fwd_accuracy', fwd_acc)\n",
    "        \n",
    "        _, stream_bwd_acc = tf.metrics.accuracy(labels[0], predictions[0], weights=label_weights)\n",
    "        _, stream_fwd_acc = tf.metrics.accuracy(labels[1], predictions[1], weights=label_weights)\n",
    "        tf.summary.scalar('stream_bwd_accuracy', stream_bwd_acc)\n",
    "        tf.summary.scalar('stream_fwd_accuracy', stream_fwd_acc)\n",
    "        \n",
    "    with tf.name_scope('optimization'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), CLIP_GRAD_NORM)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        \n",
    "    merged = tf.summary.merge_all()\n",
    "print('Graph assembled.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### BEGIN TRAINING SECTION - comment out if you want to use the latest trained model in SAVE_DIR \n",
    "print('Starting training...')\n",
    "print('Run \"tensorboard --logdir {}\" in the current directory to keep you from doing other work in the meantime.'.format(SUMMARIES_DIR))\n",
    "start_time = time.strftime('%y-%m-%d-%H-%M-%S')\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    saver = tf.train.Saver(max_to_keep=MAX_TO_KEEP)\n",
    "    train_writer = tf.summary.FileWriter('{}run-{}/{}'.format(SUMMARIES_DIR, start_time, 'train'), sess.graph)\n",
    "    valid_writer = tf.summary.FileWriter('{}run-{}/{}'.format(SUMMARIES_DIR, start_time, 'valid'), sess.graph)\n",
    "    step = 0\n",
    "    for e in range(EPOCHS):\n",
    "        valid_batch =  get_batches(validation_inputs, validation_masks, batch_size=3, n_sent_in_story=3,\n",
    "                                   is_quiz=True, quiz_answers=answers, shuffle=False)\n",
    "        for batch_i, (batch_inputs, batch_masks, batch_targets, batch_weights) in \\\n",
    "        enumerate(get_batches(enc_sentences, enc_masks, batch_size=BATCH_SIZE, n_sent_in_story=5, shuffle=True)):\n",
    "            \n",
    "            feed_dict = {encoder_inputs: batch_inputs,\n",
    "                         encoder_input_masks: batch_masks,\n",
    "                         encoder_targets: batch_targets,\n",
    "                         label_weights: batch_weights,\n",
    "                         dropout_rate: DROPOUT_RATE}\n",
    "              \n",
    "            _, batch_loss, bwd_accuracy, fwd_accuracy, summary = sess.run([train_op,\n",
    "                                                                           loss,\n",
    "                                                                           bwd_acc,\n",
    "                                                                           fwd_acc,\n",
    "                                                                           merged],\n",
    "                                                                           feed_dict=feed_dict)\n",
    "            train_writer.add_summary(summary, step)\n",
    "            \n",
    "            if batch_i % DISPLAY_STEP == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Batch bwd acc: {:>3.2%}, Batch fwd acc: {:>3.2%}, Batch loss: {:>6.4f}'\n",
    "                      .format(e, batch_i, len(enc_sentences) // BATCH_SIZE, bwd_accuracy, fwd_accuracy, batch_loss))\n",
    "                \n",
    "            if batch_i % VALIDATE_STEP == 0 and batch_i > 0:\n",
    "                valid_input, valid_mask, valid_target, valid_weight = next(valid_batch)\n",
    "                feed_dict = {encoder_inputs: valid_input,\n",
    "                             encoder_input_masks: valid_mask,\n",
    "                             encoder_targets: valid_target,\n",
    "                             label_weights: valid_weight,\n",
    "                             dropout_rate: 0}\n",
    "                \n",
    "                valid_loss, stream_bwd_accuracy, stream_fwd_accuracy, summary = sess.run([loss,\n",
    "                                                                                             stream_bwd_acc,\n",
    "                                                                                             stream_fwd_acc,\n",
    "                                                                                             merged],\n",
    "                                                                                             feed_dict=feed_dict)\n",
    "                \n",
    "                valid_writer.add_summary(summary, step)\n",
    "            \n",
    "            if batch_i % SAVE_STEP == 0 and batch_i > 0:\n",
    "                saver.save(sess, '{}/run-{}_ep_{}_step_{}_enc_{}_bsize_{}.ckpt'.format(\n",
    "                    SAVE_DIR, start_time, e, step, ENCODER_TYPE, BATCH_SIZE))\n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "        saver.save(sess, '{}/run-{}_ep_{}_step_{}_enc_{}_bsize_{}.ckpt'.format(\n",
    "            SAVE_DIR, start_time, e, step, ENCODER_TYPE, BATCH_SIZE))\n",
    "    train_writer.close()\n",
    "    valid_writer.close()\n",
    "print('Training finished.')\n",
    "### END OF TRAINING SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get _cloze_ predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Checking for trained model at latest checkpoint...')\n",
    "checkpoint = tf.train.latest_checkpoint(SAVE_DIR)\n",
    "assert checkpoint is not None, 'No checkpoints found, check SAVE_DIR & README.txt'\n",
    "print('Found model {}.'.format(checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Checking validation score...')\n",
    "cloze_preds = []\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, checkpoint)\n",
    "    for valid_i, (valid_input, valid_mask, valid_target, valid_weight) \\\n",
    "    in enumerate(get_batches(validation_inputs, validation_masks, batch_size=3, n_sent_in_story=3,\n",
    "                             is_quiz=True, quiz_answers=validation_answers, shuffle=False)):\n",
    "        scr = sess.run(scores,\n",
    "                      {encoder_inputs: valid_input,\n",
    "                       encoder_input_masks: valid_mask,\n",
    "                       encoder_targets: valid_target,\n",
    "                       label_weights: valid_weight,\n",
    "                       dropout_rate: 0})\n",
    "        cloze_pred = np.argmax(scr[0, 1:]) + 1\n",
    "        cloze_preds.append(cloze_pred)\n",
    "    cloze_preds = np.array(cloze_preds).reshape((-1, 1))\n",
    "cloze_score = np.mean((validation_answers == cloze_preds))\n",
    "print('For checkpoint {}:'.format(checkpoint))\n",
    "print('Validation score: {:>3.2%}'.format(cloze_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run at very end after training & when everything else is done\n",
    "print('Checking test score...')\n",
    "cloze_preds = []\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, checkpoint)\n",
    "    for test_i, (test_input, test_mask, test_target, test_weight) \\\n",
    "    in enumerate(get_batches(test_inputs, test_masks, batch_size=3, n_sent_in_story=3,\n",
    "                             is_quiz=True, quiz_answers=test_answers, shuffle=False)):\n",
    "        scr = sess.run(scores,\n",
    "                      {encoder_inputs: test_input,\n",
    "                       encoder_input_masks: test_mask,\n",
    "                       encoder_targets: test_target,\n",
    "                       label_weights: test_weight,\n",
    "                       dropout_rate: 0})\n",
    "        cloze_pred = np.argmax(scr[0, 1:]) + 1\n",
    "        cloze_preds.append(cloze_pred)\n",
    "    cloze_preds = np.array(cloze_preds).reshape((-1, 1))\n",
    "cloze_score = np.mean((test_answers == cloze_preds))\n",
    "print('For checkpoint {}:'.format(checkpoint))\n",
    "print('Test score: {:>3.2%}'.format(cloze_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run at very end after training & when everything else is done\n",
    "print('Generating NLU 2018 test predictions...')\n",
    "cloze_preds = []\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, checkpoint)\n",
    "    for test_i, (test_input, test_mask, test_target, test_weight) \\\n",
    "    in enumerate(get_batches(NLU_test_inputs, NLU_test_masks, batch_size=3, n_sent_in_story=3,\n",
    "                             is_quiz=True, quiz_answers=np.ones((len(NLU_test_inputs), 1), dtype='int8'), shuffle=False)):\n",
    "        scr = sess.run(scores,\n",
    "                      {encoder_inputs: test_input,\n",
    "                       encoder_input_masks: test_mask,\n",
    "                       encoder_targets: test_target,\n",
    "                       label_weights: test_weight,\n",
    "                       dropout_rate: 0})\n",
    "        cloze_pred = np.argmax(scr[0, 1:]) + 1\n",
    "        cloze_preds.append(cloze_pred)\n",
    "    cloze_preds = np.array(cloze_preds).reshape((-1, 1))\n",
    "output_file = 'NLU_2018_taest_preds.csv'\n",
    "np.savetxt(output_file, cloze_preds, fmt='%1u', delimiter=',')\n",
    "print('NLU 2018 test predictions written to {} - Bye!'.format(output_file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
