{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T13:44:56.592787Z",
     "start_time": "2018-06-05T13:44:56.586200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"1200\"\n",
       "            src=\"project2.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1298ad588>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from IPython.display import IFrame\n",
    "# IFrame('project2.pdf', width=600, height=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this notebook, make sure you have placed the following into a ```./data/``` subdirectory:\n",
    "- Place all cloze task files for training and validation as in the polybox directory into ```./data/cloze/```\n",
    "- Download and place Stanford's GloVe 6B vector set [glove.6B](http://nlp.stanford.edu/data/glove.6B.zip) as in the polybox directory into ```./data/glove/```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T09:01:22.018863Z",
     "start_time": "2018-05-18T09:01:22.013875Z"
    }
   },
   "source": [
    "# Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:15.033144Z",
     "start_time": "2018-06-05T12:53:07.229923Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T09:01:06.640116Z",
     "start_time": "2018-05-18T09:01:06.633210Z"
    }
   },
   "source": [
    "# Config variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:15.047543Z",
     "start_time": "2018-06-05T12:53:15.038687Z"
    }
   },
   "outputs": [],
   "source": [
    "N_SENT_IN_STORY = 5\n",
    "ENCODER_DIM = 600\n",
    "MAX_SEQ_LENGTH = 30\n",
    "MAX_VOCAB_SIZE = 400000\n",
    "EMBEDDING_DIM = 300\n",
    "ENCODER_TYPE = 'GRU'\n",
    "OOV_TOKEN = '<unk>'\n",
    "CONTEXT_SIZE = 1\n",
    "\n",
    "LEARNING_RATE = 0.001 # 0.0005 in paper\n",
    "CLIP_GRAD_NORM = 5.0\n",
    "DROPOUT_RATE = 0.\n",
    "\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 25\n",
    "DROPOUT_RATE = 0.25\n",
    "SAVE_DIR = './checkpoints/'\n",
    "SUMMARIES_DIR = './summaries/'\n",
    "MAX_TO_KEEP = 5\n",
    "DISPLAY_STEP = 100\n",
    "VALIDATE_STEP = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:15.058283Z",
     "start_time": "2018-06-05T12:53:15.052506Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_word2vec(word2vec_file='./data/glove.6B/glove.6B.{}d.txt'.format(EMBEDDING_DIM)):\n",
    "    word2vec = {}\n",
    "    with open(os.path.join(word2vec_file), encoding='utf8') as f:\n",
    "        for row in f:\n",
    "            values = row.split()\n",
    "            word = values[0]\n",
    "            vec = np.asarray(values[1:], dtype='float32')\n",
    "            word2vec[word] = vec\n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T13:26:56.998323Z",
     "start_time": "2018-06-05T13:26:56.991043Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_training_data(train_file='./data/cloze/train_stories_all.csv'):\n",
    "    train = pd.read_csv(train_file)\n",
    "    train.drop_duplicates(subset='storyid') # make sure we have no duplicates\n",
    "    titles = np.expand_dims(train['storytitle'].values, axis=1)\n",
    "    sentences_1 = np.expand_dims(train['sentence1'].values, axis=1)\n",
    "    sentences_2 = np.expand_dims(train['sentence2'].values, axis=1)\n",
    "    sentences_3 = np.expand_dims(train['sentence3'].values, axis=1)\n",
    "    sentences_4 = np.expand_dims(train['sentence4'].values, axis=1)\n",
    "    sentences_5 = np.expand_dims(train['sentence5'].values, axis=1)\n",
    "    mains = np.column_stack((sentences_1, sentences_2, sentences_3, sentences_4))\n",
    "    stories = np.hstack((mains, sentences_5))\n",
    "    sentences = [s for story in stories for s in story]\n",
    "    print('{} has {} stories with a total of {} sentences.'.format(train_file,\n",
    "                                                                   len(stories),\n",
    "                                                                   len(sentences)))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:15.090882Z",
     "start_time": "2018-06-05T12:53:15.078634Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_val_data(val_file='./data/cloze/cloze_test_val__spring2016 - cloze_test_ALL_val.csv'):\n",
    "    validation = pd.read_csv(val_file)\n",
    "    sentences_4 = np.expand_dims(validation['InputSentence4'].values, axis=1)\n",
    "    quiz_1 = np.expand_dims(validation['RandomFifthSentenceQuiz1'].values, axis=1)\n",
    "    quiz_2 = np.expand_dims(validation['RandomFifthSentenceQuiz2'].values, axis=1)\n",
    "    answers = np.expand_dims(validation['AnswerRightEnding'].values, axis=1)\n",
    "    quizzes = np.hstack((sentences_4, quiz_1, quiz_2))\n",
    "    sentences = [s for quiz in quizzes for s in quiz]\n",
    "    print('{} has {} quizzes with a total of {} sentences.'.format(val_file,\n",
    "                                                                   len(quizzes),\n",
    "                                                                   len(sentences)))\n",
    "    return sentences, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:15.116342Z",
     "start_time": "2018-06-05T12:53:15.103533Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_test_data(test_file='./data/cloze/cloze_test_test__spring2016 - cloze_test_ALL_test.csv'):\n",
    "    test = pd.read_csv(test_file)\n",
    "    sentences_4 = np.expand_dims(test['InputSentence4'].values, axis=1)\n",
    "    quiz_1 = np.expand_dims(test['RandomFifthSentenceQuiz1'].values, axis=1)\n",
    "    quiz_2 = np.expand_dims(test['RandomFifthSentenceQuiz2'].values, axis=1)\n",
    "    quizzes = np.hstack((sentences_4, quiz_1, quiz_2))\n",
    "    sentences = [s for quiz in quizzes for s in quiz]\n",
    "    print('{} has {} quizzes with a total of {} sentences.'.format(test_file,\n",
    "                                                                   len(quizzes),\n",
    "                                                                   len(sentences)))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T13:12:16.639299Z",
     "start_time": "2018-06-05T13:12:16.634459Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is an ugly-but-necessary hack thx to the NLU test data being of a different format than all the other data files\n",
    "def get_NLU_test_data(test_file='./data/cloze/test_nlu18.csv'):\n",
    "    test = pd.read_csv(test_file, header=None, encoding='latin-1')\n",
    "    sentences_4 = np.expand_dims(test[3].values, axis=1)\n",
    "    quiz_1 = np.expand_dims(test[4].values, axis=1)\n",
    "    quiz_2 = np.expand_dims(test[5].values, axis=1)\n",
    "    quizzes = np.hstack((sentences_4, quiz_1, quiz_2))\n",
    "    sentences = [s for quiz in quizzes for s in quiz]\n",
    "    print('{} has {} quizzes with a total of {} sentences.'.format(test_file,\n",
    "                                                                   len(quizzes),\n",
    "                                                                   len(sentences)))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:15.125340Z",
     "start_time": "2018-06-05T12:53:15.120089Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_to_word_sequence(text):\n",
    "    filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    split = ' '\n",
    "    text = text.lower().translate({ord(c): split for c in filters})\n",
    "    seq = text.split(split)\n",
    "    return [t for t in seq if t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:15.152653Z",
     "start_time": "2018-06-05T12:53:15.130131Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_vocabs(texts):\n",
    "    word_counts = OrderedDict()\n",
    "    max_seq_len = 0\n",
    "    for text in texts:\n",
    "        if isinstance(text, list):\n",
    "            seq = text\n",
    "        else:\n",
    "            seq = text_to_word_sequence(text)\n",
    "        if len(seq) > max_seq_len:\n",
    "            max_seq_len = len(seq)\n",
    "        for w in seq:\n",
    "            if w in word_counts:\n",
    "                word_counts[w] += 1\n",
    "            else:\n",
    "                word_counts[w] = 1\n",
    "    word_counts = list(word_counts.items())\n",
    "    word_counts.sort(key = lambda x: x[1], reverse=True)\n",
    "    sorted_vocab = [word_count[0] for word_count in word_counts]\n",
    "    word2idx = dict(list(zip(sorted_vocab, list(range(1, len(sorted_vocab) + 1)))))\n",
    "    i = word2idx.get(OOV_TOKEN)\n",
    "    if i is None:\n",
    "        word2idx[OOV_TOKEN] = len(word2idx) + 1\n",
    "    idx2word = {value : key for key, value in word2idx.items()}\n",
    "    return word2idx, idx2word, max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:15.176299Z",
     "start_time": "2018-06-05T12:53:15.159599Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_pad_mask(texts, seq_length):\n",
    "    vectors, masks = [], []\n",
    "    for text in texts:\n",
    "        if isinstance(text, list):\n",
    "            seq = text\n",
    "        else:\n",
    "            seq = text_to_word_sequence(text)\n",
    "        seq = seq[:seq_length]\n",
    "        vector, mask  = [], []\n",
    "        for w in seq:\n",
    "            vector.append(word2idx.get(w, word2idx[OOV_TOKEN]))\n",
    "            mask.append(1)\n",
    "        while len(vector) < seq_length:\n",
    "            vector.append(0)\n",
    "            mask.append(0)\n",
    "        vectors.append(vector)\n",
    "        masks.append(mask)\n",
    "    return np.array(vectors, dtype='int64'), np.array(masks, dtype='int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:15.189124Z",
     "start_time": "2018-06-05T12:53:15.180740Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    encoder_inputs = tf.placeholder(tf.int64, [None, None], name='encoder_inputs')\n",
    "    encoder_input_masks = tf.placeholder(tf.int8, [None, None], name='input_masks')\n",
    "    encoder_targets = tf.placeholder(tf.float32, [None, None], name='encoder_targets')\n",
    "    label_weights = tf.placeholder(tf.float32, [None,], name='label_weights')\n",
    "    dropout_rate = tf.placeholder(tf.float32, [], name='dropout_rate')\n",
    "    return encoder_inputs, encoder_input_masks, encoder_targets, label_weights, dropout_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:15.201496Z",
     "start_time": "2018-06-05T12:53:15.192828Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_embedding_matrix(num_words):\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    for word, idx in word2idx.items():\n",
    "        if idx < num_words:\n",
    "            embedding_vector = word2vec.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[idx] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:15.215369Z",
     "start_time": "2018-06-05T12:53:15.205510Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings(encode_ids, trainable=False):\n",
    "    word_embeddings = []\n",
    "    encode_emb = []\n",
    "    for suffix in ['_f', '_g']:\n",
    "        word_emb = tf.get_variable(name='word_embedding'+suffix,\n",
    "                                   shape=embedding_matrix.shape,\n",
    "                                   trainable=trainable)\n",
    "        word_emb.assign(embedding_matrix)\n",
    "        word_embeddings.append(word_emb)\n",
    "        encode_ = tf.nn.embedding_lookup(word_emb, encode_ids)\n",
    "        encode_emb.append(encode_)\n",
    "    return word_embeddings, encode_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:15.223984Z",
     "start_time": "2018-06-05T12:53:15.218228Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_rnn_cells(num_units, cell_type):\n",
    "    if cell_type == 'GRU':\n",
    "        return tf.nn.rnn_cell.GRUCell(num_units=num_units)\n",
    "    elif cell_type == 'LSTM':\n",
    "        return tf.nn.rnn_cell.LSTMCell(num_units=num_units)\n",
    "    else:\n",
    "        raise ValueError('Invalid cell type given')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:15.238898Z",
     "start_time": "2018-06-05T12:53:15.228258Z"
    }
   },
   "outputs": [],
   "source": [
    "def rnn_encoder(embeds, mask, scope, num_units=600, cell_type='GRU'):\n",
    "    sequence_length = tf.to_int32(tf.reduce_sum(mask, 1), name='length')\n",
    "    cell_fw = make_rnn_cells(num_units, cell_type)\n",
    "    cell_bw = make_rnn_cells(num_units, cell_type)\n",
    "    outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_fw,\n",
    "                                                      cell_bw=cell_bw,\n",
    "                                                      inputs=embeds,\n",
    "                                                      sequence_length=sequence_length,\n",
    "                                                      dtype=tf.float32,\n",
    "                                                      scope=scope)\n",
    "    if cell_type == 'LSTM':\n",
    "        states = [states[0][1], states[1][1]]\n",
    "    state = tf.concat(states, 1)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:15.259347Z",
     "start_time": "2018-06-05T12:53:15.248567Z"
    }
   },
   "outputs": [],
   "source": [
    "def bow_encoder(embeds, mask):\n",
    "    mask_expand = tf.expand_dims(tf.cast(mask, tf.float32), -1)\n",
    "    embeds_masked = embeds * mask_expand\n",
    "    return tf.reduce_sum(embeds_masked, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:15.282499Z",
     "start_time": "2018-06-05T12:53:15.269533Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_thought_vectors(encode_emb, encode_mask):\n",
    "    suffixes = ['_f', '_g']\n",
    "    thought_vectors = []\n",
    "    for i in range(len(suffixes)):\n",
    "        with tf.variable_scope('encoder' + suffixes[i]) as scope:\n",
    "            if ENCODER_TYPE == 'GRU':\n",
    "                encoded = rnn_encoder(encode_emb[i], encode_mask, scope,\n",
    "                                     ENCODER_DIM, ENCODER_TYPE)\n",
    "            elif ENCODER_TYPE == 'LSTM':\n",
    "                encoded = rnn_encoder(encode_emb[i], encode_mask, scope,\n",
    "                                     ENCODER_DIM, ENCODER_TYPE)\n",
    "            elif ENCODER_TYPE == 'bow':\n",
    "                encoded = bow_encoder(encode_emb[i], encode_mask)\n",
    "            else:\n",
    "                raise ValueError('Invalid encoder type given')\n",
    "\n",
    "        thought_vector = tf.identity(encoded, name='thought_vector' + suffixes[i])\n",
    "        thought_vectors.append(thought_vector)\n",
    "    return thought_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:15.302084Z",
     "start_time": "2018-06-05T12:53:15.286689Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_targets_weights(batch_size, is_cloze, n_sent_in_story, is_quiz=False, quiz_answer=None):\n",
    "    if is_quiz:\n",
    "        assert quiz_answer in [1, 2], 'must indicate correct quiz answer'\n",
    "        targets = np.zeros((3, 3), dtype='float32')\n",
    "        targets[0, quiz_answer] = 1\n",
    "        targets[quiz_answer, 0] = 1\n",
    "        weights = np.array([1,0])\n",
    "    else:\n",
    "        context_idx = list(range(-CONTEXT_SIZE, CONTEXT_SIZE + 1))\n",
    "        context_idx.remove(0)\n",
    "        weights = np.ones(batch_size - 1)\n",
    "        if is_cloze:\n",
    "            sub_targets = np.zeros((n_sent_in_story, n_sent_in_story), dtype='float32')    \n",
    "            for i in context_idx:\n",
    "                sub_targets += np.eye(n_sent_in_story, k=i)\n",
    "            targets = np.zeros((batch_size, batch_size), dtype='float32')\n",
    "            weights = np.ones(batch_size - 1)\n",
    "            for i in range(n_sent_in_story - 1, len(weights), n_sent_in_story):\n",
    "                weights[i] = 0\n",
    "            for i in range(0, batch_size, n_sent_in_story):\n",
    "                targets[i:i+n_sent_in_story, i:i+n_sent_in_story] += sub_targets\n",
    "        else:\n",
    "            targets = np.zeros((batch_size, batch_size), dtype='float32')\n",
    "            for i in context_idx:\n",
    "                targets += np.eye(batch_size, k=i)\n",
    "        targets /= np.sum(targets, axis=1, keepdims=True)\n",
    "    return targets, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:15.314458Z",
     "start_time": "2018-06-05T12:53:15.305985Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_scores(thought_vectors, dropout_rate):\n",
    "    def use_dropout():\n",
    "        a, b = thought_vectors[0], thought_vectors[1]\n",
    "        dropout_mask_shape = tf.transpose(tf.shape(a))\n",
    "        dropout_mask = tf.random_uniform(dropout_mask_shape) > DROPOUT_RATE\n",
    "        dropout_mask = tf.where(dropout_mask,\n",
    "                                tf.ones(dropout_mask_shape),\n",
    "                                tf.zeros(dropout_mask_shape))\n",
    "        dropout_mask *= (1/dropout_rate)\n",
    "        a *= dropout_mask\n",
    "        b *= dropout_mask\n",
    "        return a, b\n",
    "    def no_dropout():\n",
    "        return thought_vectors[0], thought_vectors[1]\n",
    "    a, b = tf.cond(dropout_rate > 0, use_dropout, no_dropout)\n",
    "\n",
    "    scores = tf.matmul(a, b, transpose_b=True)\n",
    "    scores = tf.matrix_set_diag(scores, tf.zeros_like(scores[0]))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:15.326608Z",
     "start_time": "2018-06-05T12:53:15.317806Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_labels_predictions(scores, n_sent_in_story=N_SENT_IN_STORY, is_cloze=True):\n",
    "    bwd_scores = scores[1:  ]\n",
    "    fwd_scores = scores[ :-1]\n",
    "    bwd_predictions = tf.to_int64(tf.argmax(bwd_scores, axis=1))\n",
    "    fwd_predictions = tf.to_int64(tf.argmax(fwd_scores, axis=1))\n",
    "    bwd_labels = tf.range(tf.shape(bwd_scores)[0])\n",
    "    fwd_labels = bwd_labels + 1\n",
    "    \n",
    "    return (bwd_labels, fwd_labels), (bwd_predictions, fwd_predictions)#, label_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:15.735205Z",
     "start_time": "2018-06-05T12:53:15.728569Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batch_acc(labels, predictions, label_weights):\n",
    "    total_weight = tf.reduce_sum(label_weights)\n",
    "    bwd_acc = tf.cast(tf.equal(tf.to_int64(labels[0]) , predictions[0]), tf.float32)\n",
    "    bwd_acc *= label_weights\n",
    "    bwd_acc = tf.reduce_sum(bwd_acc)\n",
    "    bwd_acc /= total_weight\n",
    "    fwd_acc = tf.cast(tf.equal(tf.to_int64(labels[1]), predictions[1]), tf.float32)\n",
    "    fwd_acc *= label_weights\n",
    "    fwd_acc = tf.reduce_sum(fwd_acc)\n",
    "    fwd_acc /= total_weight\n",
    "    return bwd_acc, fwd_acc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T14:15:42.946558Z",
     "start_time": "2018-06-05T14:15:42.938852Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batches(inputs, masks, batch_size, is_cloze=True, n_sent_in_story=5, is_quiz=False, quiz_answers=None, shuffle=True):\n",
    "    if is_cloze:\n",
    "        assert (batch_size % n_sent_in_story) == 0, 'batch_size must be multiple of n_sent_in_story for cloze task training.'\n",
    "    rows, cols = inputs.shape\n",
    "    if shuffle and is_cloze and not is_quiz:        \n",
    "        row_blocks = rows // n_sent_in_story\n",
    "        shuffle_idx = np.random.permutation(row_blocks) \n",
    "        inputs = inputs.reshape((row_blocks, -1, cols))[shuffle_idx].reshape((-1, cols))\n",
    "        masks = masks.reshape((row_blocks, -1, cols))[shuffle_idx].reshape((-1, cols))\n",
    "    n_batches = len(inputs) // batch_size\n",
    "    for batch_i in range(n_batches):\n",
    "        start_i = batch_i * batch_size\n",
    "        batch_inputs = inputs[start_i : start_i + batch_size]\n",
    "        batch_masks = masks[start_i : start_i + batch_size]\n",
    "        if is_quiz:\n",
    "            batch_targets, batch_weights = get_targets_weights(batch_size, is_cloze, n_sent_in_story,\n",
    "                                                               is_quiz, quiz_answers[batch_i])\n",
    "        else:\n",
    "            batch_targets, batch_weights = get_targets_weights(batch_size, is_cloze, n_sent_in_story)\n",
    "        yield batch_inputs, batch_masks, batch_targets, batch_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:18.958768Z",
     "start_time": "2018-06-05T12:53:18.025138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cloze story training data...\n",
      "./data/cloze/train_stories.csv has 88161 stories with a total of 440805 sentences.\n",
      "Loaded training stories.\n"
     ]
    }
   ],
   "source": [
    "print('Loading cloze story training data...')\n",
    "sentences = get_training_data()\n",
    "print('Loaded training stories.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:24.673310Z",
     "start_time": "2018-06-05T12:53:24.640169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cloze story validation data...\n",
      "./data/cloze/cloze_test_val__spring2016 - cloze_test_ALL_val.csv has 1871 quizzes with a total of 5613 sentences.\n",
      "Loaded validation stories.\n"
     ]
    }
   ],
   "source": [
    "print('Loading cloze story validation data...')\n",
    "validation_sentences, answers = get_val_data()\n",
    "print('Loaded validation stories.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:25.461958Z",
     "start_time": "2018-06-05T12:53:25.417533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cloze story test data...\n",
      "./data/cloze/cloze_test_test__spring2016 - cloze_test_ALL_test.csv has 1871 quizzes with a total of 5613 sentences.\n",
      "Loaded test stories.\n"
     ]
    }
   ],
   "source": [
    "print('Loading cloze story test data...')\n",
    "test_sentences = get_test_data()\n",
    "print('Loaded test stories.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T13:13:51.030635Z",
     "start_time": "2018-06-05T13:13:51.007663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NLU 2018 story test data...\n",
      "./data/cloze/test_nlu18.csv has 2343 quizzes with a total of 7029 sentences.\n",
      "Loaded NLU 2018 test stories.\n"
     ]
    }
   ],
   "source": [
    "print('Loading NLU 2018 story test data...')\n",
    "NLU_test_sentences = get_NLU_test_data()\n",
    "print('Loaded NLU 2018 test stories.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:53:32.477642Z",
     "start_time": "2018-06-05T12:53:26.125840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating vocabulary from training data ...\n",
      "Found 35591 unique word tokens\n",
      "       Longest sentence has 19 tokens.\n"
     ]
    }
   ],
   "source": [
    "print('Generating vocabulary from training data ...')\n",
    "word2idx, idx2word, max_seq_len = generate_vocabs(sentences)\n",
    "print('Found {} unique word tokens\\n\\\n",
    "       Longest sentence has {} tokens.'.format(len(word2idx), max_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:54:24.384854Z",
     "start_time": "2018-06-05T12:53:32.481464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained word embedding vectors...\n",
      "Loaded 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Loading pretrained word embedding vectors...')\n",
    "word2vec = get_word2vec()\n",
    "print('Loaded {} word vectors.'.format(len(word2vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:54:24.815739Z",
     "start_time": "2018-06-05T12:54:24.387101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing embedding matrix...\n",
      "Finished embedding matrix has shape (35592, 300).\n"
     ]
    }
   ],
   "source": [
    "num_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\n",
    "\n",
    "print('Constructing embedding matrix...')\n",
    "embedding_matrix = get_embedding_matrix(num_words)\n",
    "print('Finished embedding matrix has shape {}.'.format(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:54:39.879691Z",
     "start_time": "2018-06-05T12:54:24.819334Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2idx, pad and mask training sentences to length 19 ...\n",
      "440805 training sentences processed.\n"
     ]
    }
   ],
   "source": [
    "seq_length = min(max_seq_len, MAX_SEQ_LENGTH)\n",
    "\n",
    "print('Word2idx, pad and mask training sentences to length {} ...'.format(seq_length))\n",
    "enc_sentences, enc_masks = tokenize_pad_mask(sentences, seq_length)\n",
    "print('{} training sentences processed.'.format(len(enc_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:54:40.314954Z",
     "start_time": "2018-06-05T12:54:39.883103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2idx, pad and mask validation sentences to length 19 ...\n",
      "5613 validation_sentences sentences processed.\n"
     ]
    }
   ],
   "source": [
    "print('Word2idx, pad and mask validation sentences to length {} ...'.format(seq_length))\n",
    "validation_inputs, validation_masks = tokenize_pad_mask(validation_sentences, seq_length)\n",
    "print('{} validation sentences sentences processed.'.format(len(validation_inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:54:40.855463Z",
     "start_time": "2018-06-05T12:54:40.329241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2idx, pad and mask test sentences to length 19 ...\n",
      "5613 validation_sentences sentences processed.\n"
     ]
    }
   ],
   "source": [
    "print('Word2idx, pad and mask test sentences to length {} ...'.format(seq_length))\n",
    "test_inputs, test_masks = tokenize_pad_mask(test_sentences, seq_length)\n",
    "print('{} test sentences sentences processed.'.format(len(test_inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T13:15:23.469667Z",
     "start_time": "2018-06-05T13:15:23.120723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2idx, pad and mask NLU 2018 test sentences to length 19 ...\n",
      "7029 NLU 2018 test sentences sentences processed.\n"
     ]
    }
   ],
   "source": [
    "print('Word2idx, pad and mask NLU 2018 test sentences to length {} ...'.format(seq_length))\n",
    "NLU_test_inputs, NLU_test_masks = tokenize_pad_mask(NLU_test_sentences, seq_length)\n",
    "print('{} NLU 2018 test sentences sentences processed.'.format(len(NLU_test_inputs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T09:49:55.821535Z",
     "start_time": "2018-05-20T09:49:55.817650Z"
    }
   },
   "source": [
    "# Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:54:45.282363Z",
     "start_time": "2018-06-05T12:54:40.859411Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building graph...\n",
      "Graph assembled.\n"
     ]
    }
   ],
   "source": [
    "print('Building graph...')\n",
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "   \n",
    "    with tf.name_scope('input_data'):\n",
    "        encoder_inputs, encoder_input_masks, encoder_targets, label_weights, dropout_rate = get_inputs()\n",
    "        \n",
    "    with tf.name_scope('embeddings'):\n",
    "        word_embeddings, encode_emb = get_embeddings(encoder_inputs, trainable=True)\n",
    "        \n",
    "    with tf.name_scope('encoders'):\n",
    "        thoughts = get_thought_vectors(encode_emb, encoder_input_masks)\n",
    " \n",
    "    with tf.name_scope('losses_accuracies'):\n",
    "        scores = get_scores(thoughts, dropout_rate)\n",
    "        labels, predictions = get_labels_predictions(scores)\n",
    "\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(labels=encoder_targets,\n",
    "                                                       logits=scores))\n",
    "        tf.summary.scalar('batch_ent_loss', loss)\n",
    "        \n",
    "        bwd_acc, fwd_acc = get_batch_acc(labels, predictions, label_weights)\n",
    "        tf.summary.scalar('batch_bwd_accuracy', bwd_acc)\n",
    "        tf.summary.scalar('batch_fwd_accuracy', fwd_acc)\n",
    "        \n",
    "        _, stream_bwd_acc = tf.metrics.accuracy(labels[0], predictions[0], weights=label_weights)\n",
    "        _, stream_fwd_acc = tf.metrics.accuracy(labels[1], predictions[1], weights=label_weights)\n",
    "        tf.summary.scalar('stream_bwd_accuracy', stream_bwd_acc)\n",
    "        tf.summary.scalar('stream_fwd_accuracy', stream_fwd_acc)\n",
    "        \n",
    "    with tf.name_scope('optimization'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), CLIP_GRAD_NORM)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        \n",
    "    merged = tf.summary.merge_all()\n",
    "print('Graph assembled.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T13:54:26.636615Z",
     "start_time": "2018-05-20T13:54:12.293894Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch   50/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 5.00%, Batch loss: 3.0775\n",
      "Epoch   0 Batch  100/17632 - Batch bwd acc: 5.00%, Batch fwd acc: 15.00%, Batch loss: 3.0684\n",
      "Epoch   0 Batch  150/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 5.00%, Batch loss: 3.1634\n",
      "Epoch   0 Batch  200/17632 - Batch bwd acc: 5.00%, Batch fwd acc: 20.00%, Batch loss: 2.8143\n",
      "Epoch   0 Batch  250/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 15.00%, Batch loss: 2.5857\n",
      "Epoch   0 Batch  300/17632 - Batch bwd acc: 0.00%, Batch fwd acc: 0.00%, Batch loss: 3.0625\n",
      "Epoch   0 Batch  350/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 10.00%, Batch loss: 3.0197\n",
      "Epoch   0 Batch  400/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 10.00%, Batch loss: 2.9890\n",
      "Epoch   0 Batch  450/17632 - Batch bwd acc: 5.00%, Batch fwd acc: 10.00%, Batch loss: 3.1165\n",
      "Epoch   0 Batch  500/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 10.00%, Batch loss: 2.7692\n",
      "Epoch   0 Batch  550/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 15.00%, Batch loss: 2.8567\n",
      "Epoch   0 Batch  600/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 10.00%, Batch loss: 2.8437\n",
      "Epoch   0 Batch  650/17632 - Batch bwd acc: 5.00%, Batch fwd acc: 10.00%, Batch loss: 2.8622\n",
      "Epoch   0 Batch  700/17632 - Batch bwd acc: 5.00%, Batch fwd acc: 25.00%, Batch loss: 2.8239\n",
      "Epoch   0 Batch  750/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 0.00%, Batch loss: 3.1484\n",
      "Epoch   0 Batch  800/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.6266\n",
      "Epoch   0 Batch  850/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 0.00%, Batch loss: 2.8439\n",
      "Epoch   0 Batch  900/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 10.00%, Batch loss: 2.8036\n",
      "Epoch   0 Batch  950/17632 - Batch bwd acc: 5.00%, Batch fwd acc: 15.00%, Batch loss: 2.8171\n",
      "Epoch   0 Batch 1000/17632 - Batch bwd acc: 5.00%, Batch fwd acc: 10.00%, Batch loss: 2.9132\n",
      "Epoch   0 Batch 1050/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 25.00%, Batch loss: 2.2415\n",
      "Epoch   0 Batch 1100/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 2.2759\n",
      "Epoch   0 Batch 1150/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 10.00%, Batch loss: 2.5925\n",
      "Epoch   0 Batch 1200/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 0.00%, Batch loss: 2.6168\n",
      "Epoch   0 Batch 1250/17632 - Batch bwd acc: 5.00%, Batch fwd acc: 10.00%, Batch loss: 3.0630\n",
      "Epoch   0 Batch 1300/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 20.00%, Batch loss: 3.1790\n",
      "Epoch   0 Batch 1350/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 5.00%, Batch loss: 2.9768\n",
      "Epoch   0 Batch 1400/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 25.00%, Batch loss: 2.6867\n",
      "Epoch   0 Batch 1450/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 20.00%, Batch loss: 2.5375\n",
      "Epoch   0 Batch 1500/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 5.00%, Batch loss: 3.0551\n",
      "Epoch   0 Batch 1550/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 5.00%, Batch loss: 3.0713\n",
      "Epoch   0 Batch 1600/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 15.00%, Batch loss: 2.5468\n",
      "Epoch   0 Batch 1650/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 5.00%, Batch loss: 2.7915\n",
      "Epoch   0 Batch 1700/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 30.00%, Batch loss: 2.7161\n",
      "Epoch   0 Batch 1750/17632 - Batch bwd acc: 5.00%, Batch fwd acc: 10.00%, Batch loss: 2.7090\n",
      "Epoch   0 Batch 1800/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 20.00%, Batch loss: 2.2988\n",
      "Epoch   0 Batch 1850/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 10.00%, Batch loss: 2.5978\n",
      "Epoch   0 Batch 1900/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 15.00%, Batch loss: 3.0455\n",
      "Epoch   0 Batch 1950/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 20.00%, Batch loss: 2.5647\n",
      "Epoch   0 Batch 2000/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 2.3228\n",
      "Epoch   0 Batch 2050/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 25.00%, Batch loss: 2.5054\n",
      "Epoch   0 Batch 2100/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 10.00%, Batch loss: 2.6139\n",
      "Epoch   0 Batch 2150/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 0.00%, Batch loss: 3.2098\n",
      "Epoch   0 Batch 2200/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.0471\n",
      "Epoch   0 Batch 2250/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 10.00%, Batch loss: 2.6316\n",
      "Epoch   0 Batch 2300/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 25.00%, Batch loss: 2.7911\n",
      "Epoch   0 Batch 2350/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 0.00%, Batch loss: 2.7586\n",
      "Epoch   0 Batch 2400/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 20.00%, Batch loss: 2.4561\n",
      "Epoch   0 Batch 2450/17632 - Batch bwd acc: 5.00%, Batch fwd acc: 10.00%, Batch loss: 2.9136\n",
      "Epoch   0 Batch 2500/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 10.00%, Batch loss: 2.5312\n",
      "Epoch   0 Batch 2550/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 15.00%, Batch loss: 2.6785\n",
      "Epoch   0 Batch 2600/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 15.00%, Batch loss: 2.4554\n",
      "Epoch   0 Batch 2650/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 15.00%, Batch loss: 2.6497\n",
      "Epoch   0 Batch 2700/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 25.00%, Batch loss: 2.9694\n",
      "Epoch   0 Batch 2750/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 20.00%, Batch loss: 2.1704\n",
      "Epoch   0 Batch 2800/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 20.00%, Batch loss: 2.3057\n",
      "Epoch   0 Batch 2850/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 2.2609\n",
      "Epoch   0 Batch 2900/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 15.00%, Batch loss: 2.4316\n",
      "Epoch   0 Batch 2950/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 15.00%, Batch loss: 2.8728\n",
      "Epoch   0 Batch 3000/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 15.00%, Batch loss: 2.8323\n",
      "Epoch   0 Batch 3050/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 35.00%, Batch loss: 2.4609\n",
      "Epoch   0 Batch 3100/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 2.1609\n",
      "Epoch   0 Batch 3150/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 20.00%, Batch loss: 2.5301\n",
      "Epoch   0 Batch 3200/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 15.00%, Batch loss: 2.3790\n",
      "Epoch   0 Batch 3250/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 20.00%, Batch loss: 2.4329\n",
      "Epoch   0 Batch 3300/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 5.00%, Batch loss: 2.2585\n",
      "Epoch   0 Batch 3350/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 25.00%, Batch loss: 2.3670\n",
      "Epoch   0 Batch 3400/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 10.00%, Batch loss: 2.8514\n",
      "Epoch   0 Batch 3450/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 35.00%, Batch loss: 1.9964\n",
      "Epoch   0 Batch 3500/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 30.00%, Batch loss: 2.4298\n",
      "Epoch   0 Batch 3550/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 2.3237\n",
      "Epoch   0 Batch 3600/17632 - Batch bwd acc: 5.00%, Batch fwd acc: 20.00%, Batch loss: 2.9233\n",
      "Epoch   0 Batch 3650/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 20.00%, Batch loss: 2.2906\n",
      "Epoch   0 Batch 3700/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 10.00%, Batch loss: 3.1956\n",
      "Epoch   0 Batch 3750/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.1956\n",
      "Epoch   0 Batch 3800/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 20.00%, Batch loss: 2.2743\n",
      "Epoch   0 Batch 3850/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 10.00%, Batch loss: 2.8606\n",
      "Epoch   0 Batch 3900/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 5.00%, Batch loss: 2.4605\n",
      "Epoch   0 Batch 3950/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 20.00%, Batch loss: 2.2481\n",
      "Epoch   0 Batch 4000/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 20.00%, Batch loss: 2.3772\n",
      "Epoch   0 Batch 4050/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 10.00%, Batch loss: 2.5383\n",
      "Epoch   0 Batch 4100/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 25.00%, Batch loss: 2.0573\n",
      "Epoch   0 Batch 4150/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 15.00%, Batch loss: 2.3853\n",
      "Epoch   0 Batch 4200/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 20.00%, Batch loss: 2.4737\n",
      "Epoch   0 Batch 4250/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 15.00%, Batch loss: 2.5173\n",
      "Epoch   0 Batch 4300/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 30.00%, Batch loss: 1.9361\n",
      "Epoch   0 Batch 4350/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 20.00%, Batch loss: 2.3590\n",
      "Epoch   0 Batch 4400/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 5.00%, Batch loss: 3.1607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch 4450/17632 - Batch bwd acc: 5.00%, Batch fwd acc: 35.00%, Batch loss: 2.1714\n",
      "Epoch   0 Batch 4500/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 40.00%, Batch loss: 2.0904\n",
      "Epoch   0 Batch 4550/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 30.00%, Batch loss: 2.6118\n",
      "Epoch   0 Batch 4600/17632 - Batch bwd acc: 5.00%, Batch fwd acc: 15.00%, Batch loss: 2.5732\n",
      "Epoch   0 Batch 4650/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 25.00%, Batch loss: 2.4339\n",
      "Epoch   0 Batch 4700/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 10.00%, Batch loss: 2.5818\n",
      "Epoch   0 Batch 4750/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.5125\n",
      "Epoch   0 Batch 4800/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 50.00%, Batch loss: 2.1110\n",
      "Epoch   0 Batch 4850/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 5.00%, Batch loss: 2.8811\n",
      "Epoch   0 Batch 4900/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 35.00%, Batch loss: 2.0006\n",
      "Epoch   0 Batch 4950/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.1614\n",
      "Epoch   0 Batch 5000/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 5.00%, Batch loss: 2.6650\n",
      "Epoch   0 Batch 5050/17632 - Batch bwd acc: 0.00%, Batch fwd acc: 20.00%, Batch loss: 2.6692\n",
      "Epoch   0 Batch 5100/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 25.00%, Batch loss: 2.6902\n",
      "Epoch   0 Batch 5150/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 30.00%, Batch loss: 2.3260\n",
      "Epoch   0 Batch 5200/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 10.00%, Batch loss: 2.8798\n",
      "Epoch   0 Batch 5250/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 35.00%, Batch loss: 2.0861\n",
      "Epoch   0 Batch 5300/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 25.00%, Batch loss: 2.5496\n",
      "Epoch   0 Batch 5350/17632 - Batch bwd acc: 5.00%, Batch fwd acc: 25.00%, Batch loss: 2.3752\n",
      "Epoch   0 Batch 5400/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 5.00%, Batch loss: 2.3770\n",
      "Epoch   0 Batch 5450/17632 - Batch bwd acc: 45.00%, Batch fwd acc: 20.00%, Batch loss: 1.8994\n",
      "Epoch   0 Batch 5500/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 25.00%, Batch loss: 2.5996\n",
      "Epoch   0 Batch 5550/17632 - Batch bwd acc: 5.00%, Batch fwd acc: 25.00%, Batch loss: 2.2626\n",
      "Epoch   0 Batch 5600/17632 - Batch bwd acc: 55.00%, Batch fwd acc: 25.00%, Batch loss: 1.9750\n",
      "Epoch   0 Batch 5650/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 40.00%, Batch loss: 1.6110\n",
      "Epoch   0 Batch 5700/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 15.00%, Batch loss: 2.3230\n",
      "Epoch   0 Batch 5750/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 15.00%, Batch loss: 2.5417\n",
      "Epoch   0 Batch 5800/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 20.00%, Batch loss: 2.4737\n",
      "Epoch   0 Batch 5850/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 15.00%, Batch loss: 2.1001\n",
      "Epoch   0 Batch 5900/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.5087\n",
      "Epoch   0 Batch 5950/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 25.00%, Batch loss: 2.3431\n",
      "Epoch   0 Batch 6000/17632 - Batch bwd acc: 0.00%, Batch fwd acc: 10.00%, Batch loss: 2.6933\n",
      "Epoch   0 Batch 6050/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 20.00%, Batch loss: 2.3332\n",
      "Epoch   0 Batch 6100/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 20.00%, Batch loss: 2.5650\n",
      "Epoch   0 Batch 6150/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 25.00%, Batch loss: 1.9747\n",
      "Epoch   0 Batch 6200/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 20.00%, Batch loss: 2.3388\n",
      "Epoch   0 Batch 6250/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 30.00%, Batch loss: 2.1457\n",
      "Epoch   0 Batch 6300/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 10.00%, Batch loss: 2.4289\n",
      "Epoch   0 Batch 6350/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 20.00%, Batch loss: 2.1842\n",
      "Epoch   0 Batch 6400/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 20.00%, Batch loss: 2.0007\n",
      "Epoch   0 Batch 6450/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 15.00%, Batch loss: 2.0796\n",
      "Epoch   0 Batch 6500/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 40.00%, Batch loss: 1.8068\n",
      "Epoch   0 Batch 6550/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 15.00%, Batch loss: 2.3919\n",
      "Epoch   0 Batch 6600/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 20.00%, Batch loss: 2.5740\n",
      "Epoch   0 Batch 6650/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 15.00%, Batch loss: 2.0946\n",
      "Epoch   0 Batch 6700/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 25.00%, Batch loss: 2.6862\n",
      "Epoch   0 Batch 6750/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 25.00%, Batch loss: 2.2631\n",
      "Epoch   0 Batch 6800/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 25.00%, Batch loss: 2.4769\n",
      "Epoch   0 Batch 6850/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 20.00%, Batch loss: 2.0067\n",
      "Epoch   0 Batch 6900/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 25.00%, Batch loss: 2.2123\n",
      "Epoch   0 Batch 6950/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 20.00%, Batch loss: 2.6279\n",
      "Epoch   0 Batch 7000/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 30.00%, Batch loss: 2.1934\n",
      "Epoch   0 Batch 7050/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 35.00%, Batch loss: 2.4637\n",
      "Epoch   0 Batch 7100/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 35.00%, Batch loss: 2.4596\n",
      "Epoch   0 Batch 7150/17632 - Batch bwd acc: 5.00%, Batch fwd acc: 10.00%, Batch loss: 2.6305\n",
      "Epoch   0 Batch 7200/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 10.00%, Batch loss: 2.6357\n",
      "Epoch   0 Batch 7250/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 20.00%, Batch loss: 2.0058\n",
      "Epoch   0 Batch 7300/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.3884\n",
      "Epoch   0 Batch 7350/17632 - Batch bwd acc: 5.00%, Batch fwd acc: 5.00%, Batch loss: 2.9464\n",
      "Epoch   0 Batch 7400/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 10.00%, Batch loss: 2.3319\n",
      "Epoch   0 Batch 7450/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.3005\n",
      "Epoch   0 Batch 7500/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 10.00%, Batch loss: 2.4181\n",
      "Epoch   0 Batch 7550/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 15.00%, Batch loss: 2.7762\n",
      "Epoch   0 Batch 7600/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 20.00%, Batch loss: 2.2609\n",
      "Epoch   0 Batch 7650/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 2.3277\n",
      "Epoch   0 Batch 7700/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 15.00%, Batch loss: 2.5559\n",
      "Epoch   0 Batch 7750/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 30.00%, Batch loss: 2.3801\n",
      "Epoch   0 Batch 7800/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 10.00%, Batch loss: 2.7358\n",
      "Epoch   0 Batch 7850/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 25.00%, Batch loss: 2.3156\n",
      "Epoch   0 Batch 7900/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.1783\n",
      "Epoch   0 Batch 7950/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 20.00%, Batch loss: 2.4728\n",
      "Epoch   0 Batch 8000/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.6373\n",
      "Epoch   0 Batch 8050/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 20.00%, Batch loss: 2.5640\n",
      "Epoch   0 Batch 8100/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 2.3323\n",
      "Epoch   0 Batch 8150/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 20.00%, Batch loss: 2.4786\n",
      "Epoch   0 Batch 8200/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.2147\n",
      "Epoch   0 Batch 8250/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 30.00%, Batch loss: 1.8042\n",
      "Epoch   0 Batch 8300/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 40.00%, Batch loss: 1.9626\n",
      "Epoch   0 Batch 8350/17632 - Batch bwd acc: 5.00%, Batch fwd acc: 25.00%, Batch loss: 2.6932\n",
      "Epoch   0 Batch 8400/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 30.00%, Batch loss: 2.1894\n",
      "Epoch   0 Batch 8450/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 10.00%, Batch loss: 2.2265\n",
      "Epoch   0 Batch 8500/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 25.00%, Batch loss: 2.3580\n",
      "Epoch   0 Batch 8550/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 30.00%, Batch loss: 1.7485\n",
      "Epoch   0 Batch 8600/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 10.00%, Batch loss: 1.8894\n",
      "Epoch   0 Batch 8650/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 25.00%, Batch loss: 2.3465\n",
      "Epoch   0 Batch 8700/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 20.00%, Batch loss: 2.4266\n",
      "Epoch   0 Batch 8750/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 20.00%, Batch loss: 2.3906\n",
      "Epoch   0 Batch 8800/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 1.9256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch 8850/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 25.00%, Batch loss: 2.1983\n",
      "Epoch   0 Batch 8900/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 40.00%, Batch loss: 1.9185\n",
      "Epoch   0 Batch 8950/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 20.00%, Batch loss: 2.4469\n",
      "Epoch   0 Batch 9000/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 30.00%, Batch loss: 2.2283\n",
      "Epoch   0 Batch 9050/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 30.00%, Batch loss: 2.0310\n",
      "Epoch   0 Batch 9100/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 15.00%, Batch loss: 2.7339\n",
      "Epoch   0 Batch 9150/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 20.00%, Batch loss: 2.4191\n",
      "Epoch   0 Batch 9200/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 20.00%, Batch loss: 2.1557\n",
      "Epoch   0 Batch 9250/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 20.00%, Batch loss: 2.7450\n",
      "Epoch   0 Batch 9300/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 15.00%, Batch loss: 2.6083\n",
      "Epoch   0 Batch 9350/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 20.00%, Batch loss: 2.3912\n",
      "Epoch   0 Batch 9400/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.4221\n",
      "Epoch   0 Batch 9450/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 35.00%, Batch loss: 2.0608\n",
      "Epoch   0 Batch 9500/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 25.00%, Batch loss: 2.4423\n",
      "Epoch   0 Batch 9550/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 25.00%, Batch loss: 2.1796\n",
      "Epoch   0 Batch 9600/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 2.0266\n",
      "Epoch   0 Batch 9650/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 40.00%, Batch loss: 2.1218\n",
      "Epoch   0 Batch 9700/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 10.00%, Batch loss: 2.6964\n",
      "Epoch   0 Batch 9750/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 2.1703\n",
      "Epoch   0 Batch 9800/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.3065\n",
      "Epoch   0 Batch 9850/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 15.00%, Batch loss: 2.7879\n",
      "Epoch   0 Batch 9900/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 20.00%, Batch loss: 2.5515\n",
      "Epoch   0 Batch 9950/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 25.00%, Batch loss: 2.4935\n",
      "Epoch   0 Batch 10000/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 15.00%, Batch loss: 2.6311\n",
      "Epoch   0 Batch 10050/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 25.00%, Batch loss: 2.3443\n",
      "Epoch   0 Batch 10100/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 1.9916\n",
      "Epoch   0 Batch 10150/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 2.1282\n",
      "Epoch   0 Batch 10200/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 30.00%, Batch loss: 2.3971\n",
      "Epoch   0 Batch 10250/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.1226\n",
      "Epoch   0 Batch 10300/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 1.9608\n",
      "Epoch   0 Batch 10350/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 25.00%, Batch loss: 2.8810\n",
      "Epoch   0 Batch 10400/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 45.00%, Batch loss: 1.8940\n",
      "Epoch   0 Batch 10450/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 20.00%, Batch loss: 2.4608\n",
      "Epoch   0 Batch 10500/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.0134\n",
      "Epoch   0 Batch 10550/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 30.00%, Batch loss: 1.8278\n",
      "Epoch   0 Batch 10600/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 20.00%, Batch loss: 2.5479\n",
      "Epoch   0 Batch 10650/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 10.00%, Batch loss: 2.4739\n",
      "Epoch   0 Batch 10700/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.6152\n",
      "Epoch   0 Batch 10750/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 15.00%, Batch loss: 2.9202\n",
      "Epoch   0 Batch 10800/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 20.00%, Batch loss: 3.2671\n",
      "Epoch   0 Batch 10850/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 45.00%, Batch loss: 1.6741\n",
      "Epoch   0 Batch 10900/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 25.00%, Batch loss: 2.6519\n",
      "Epoch   0 Batch 10950/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 20.00%, Batch loss: 2.2412\n",
      "Epoch   0 Batch 11000/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 20.00%, Batch loss: 2.4197\n",
      "Epoch   0 Batch 11050/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 40.00%, Batch loss: 2.0864\n",
      "Epoch   0 Batch 11100/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 40.00%, Batch loss: 2.0472\n",
      "Epoch   0 Batch 11150/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 15.00%, Batch loss: 2.1899\n",
      "Epoch   0 Batch 11200/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 15.00%, Batch loss: 2.4672\n",
      "Epoch   0 Batch 11250/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 45.00%, Batch loss: 2.0975\n",
      "Epoch   0 Batch 11300/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 40.00%, Batch loss: 2.0048\n",
      "Epoch   0 Batch 11350/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.1028\n",
      "Epoch   0 Batch 11400/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 20.00%, Batch loss: 1.9812\n",
      "Epoch   0 Batch 11450/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 25.00%, Batch loss: 2.1733\n",
      "Epoch   0 Batch 11500/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 15.00%, Batch loss: 2.9191\n",
      "Epoch   0 Batch 11550/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.3187\n",
      "Epoch   0 Batch 11600/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 20.00%, Batch loss: 2.4115\n",
      "Epoch   0 Batch 11650/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.2044\n",
      "Epoch   0 Batch 11700/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 2.2021\n",
      "Epoch   0 Batch 11750/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.1686\n",
      "Epoch   0 Batch 11800/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 15.00%, Batch loss: 2.4162\n",
      "Epoch   0 Batch 11850/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 20.00%, Batch loss: 2.4034\n",
      "Epoch   0 Batch 11900/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 35.00%, Batch loss: 1.8715\n",
      "Epoch   0 Batch 11950/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 2.2667\n",
      "Epoch   0 Batch 12000/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 25.00%, Batch loss: 2.5051\n",
      "Epoch   0 Batch 12050/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.3728\n",
      "Epoch   0 Batch 12100/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 20.00%, Batch loss: 2.2728\n",
      "Epoch   0 Batch 12150/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 25.00%, Batch loss: 2.3263\n",
      "Epoch   0 Batch 12200/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 2.5610\n",
      "Epoch   0 Batch 12250/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 20.00%, Batch loss: 2.5386\n",
      "Epoch   0 Batch 12300/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 20.00%, Batch loss: 2.0838\n",
      "Epoch   0 Batch 12350/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 25.00%, Batch loss: 1.9572\n",
      "Epoch   0 Batch 12400/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 35.00%, Batch loss: 2.1960\n",
      "Epoch   0 Batch 12450/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 20.00%, Batch loss: 2.4340\n",
      "Epoch   0 Batch 12500/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 25.00%, Batch loss: 2.1157\n",
      "Epoch   0 Batch 12550/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 40.00%, Batch loss: 1.9927\n",
      "Epoch   0 Batch 12600/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 10.00%, Batch loss: 2.3673\n",
      "Epoch   0 Batch 12650/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 35.00%, Batch loss: 2.0816\n",
      "Epoch   0 Batch 12700/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 20.00%, Batch loss: 2.1874\n",
      "Epoch   0 Batch 12750/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 2.1089\n",
      "Epoch   0 Batch 12800/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.0279\n",
      "Epoch   0 Batch 12850/17632 - Batch bwd acc: 5.00%, Batch fwd acc: 20.00%, Batch loss: 2.5912\n",
      "Epoch   0 Batch 12900/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 45.00%, Batch loss: 2.1490\n",
      "Epoch   0 Batch 12950/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 20.00%, Batch loss: 2.3240\n",
      "Epoch   0 Batch 13000/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 25.00%, Batch loss: 2.2439\n",
      "Epoch   0 Batch 13050/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 30.00%, Batch loss: 2.1597\n",
      "Epoch   0 Batch 13100/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 15.00%, Batch loss: 2.3585\n",
      "Epoch   0 Batch 13150/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 45.00%, Batch loss: 2.0838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch 13200/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 40.00%, Batch loss: 2.0744\n",
      "Epoch   0 Batch 13250/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.3078\n",
      "Epoch   0 Batch 13300/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 20.00%, Batch loss: 2.6661\n",
      "Epoch   0 Batch 13350/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 35.00%, Batch loss: 2.2141\n",
      "Epoch   0 Batch 13400/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 20.00%, Batch loss: 2.3307\n",
      "Epoch   0 Batch 13450/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 15.00%, Batch loss: 2.3901\n",
      "Epoch   0 Batch 13500/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 25.00%, Batch loss: 1.8258\n",
      "Epoch   0 Batch 13550/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 15.00%, Batch loss: 2.4948\n",
      "Epoch   0 Batch 13600/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 40.00%, Batch loss: 2.0384\n",
      "Epoch   0 Batch 13650/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 15.00%, Batch loss: 1.9949\n",
      "Epoch   0 Batch 13700/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 40.00%, Batch loss: 2.1677\n",
      "Epoch   0 Batch 13750/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 20.00%, Batch loss: 2.0079\n",
      "Epoch   0 Batch 13800/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 15.00%, Batch loss: 2.4255\n",
      "Epoch   0 Batch 13850/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 20.00%, Batch loss: 2.2814\n",
      "Epoch   0 Batch 13900/17632 - Batch bwd acc: 5.00%, Batch fwd acc: 40.00%, Batch loss: 2.1970\n",
      "Epoch   0 Batch 13950/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 20.00%, Batch loss: 2.3000\n",
      "Epoch   0 Batch 14000/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.1969\n",
      "Epoch   0 Batch 14050/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 25.00%, Batch loss: 2.2715\n",
      "Epoch   0 Batch 14100/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 15.00%, Batch loss: 2.5784\n",
      "Epoch   0 Batch 14150/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 30.00%, Batch loss: 2.7573\n",
      "Epoch   0 Batch 14200/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.4119\n",
      "Epoch   0 Batch 14250/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 15.00%, Batch loss: 2.0383\n",
      "Epoch   0 Batch 14300/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 10.00%, Batch loss: 2.5473\n",
      "Epoch   0 Batch 14350/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 15.00%, Batch loss: 2.1396\n",
      "Epoch   0 Batch 14400/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 45.00%, Batch loss: 1.8382\n",
      "Epoch   0 Batch 14450/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 25.00%, Batch loss: 2.0726\n",
      "Epoch   0 Batch 14500/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 40.00%, Batch loss: 1.8595\n",
      "Epoch   0 Batch 14550/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 25.00%, Batch loss: 2.4609\n",
      "Epoch   0 Batch 14600/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 20.00%, Batch loss: 2.5382\n",
      "Epoch   0 Batch 14650/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 20.00%, Batch loss: 2.6990\n",
      "Epoch   0 Batch 14700/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 25.00%, Batch loss: 2.5101\n",
      "Epoch   0 Batch 14750/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 30.00%, Batch loss: 2.3218\n",
      "Epoch   0 Batch 14800/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 15.00%, Batch loss: 2.4775\n",
      "Epoch   0 Batch 14850/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 15.00%, Batch loss: 2.3039\n",
      "Epoch   0 Batch 14900/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 35.00%, Batch loss: 2.4385\n",
      "Epoch   0 Batch 14950/17632 - Batch bwd acc: 45.00%, Batch fwd acc: 20.00%, Batch loss: 2.3247\n",
      "Epoch   0 Batch 15000/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 40.00%, Batch loss: 2.1769\n",
      "Epoch   0 Batch 15050/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 45.00%, Batch loss: 1.7947\n",
      "Epoch   0 Batch 15100/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 5.00%, Batch loss: 1.9337\n",
      "Epoch   0 Batch 15150/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 40.00%, Batch loss: 2.0659\n",
      "Epoch   0 Batch 15200/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 20.00%, Batch loss: 2.3424\n",
      "Epoch   0 Batch 15250/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 35.00%, Batch loss: 2.6678\n",
      "Epoch   0 Batch 15300/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 2.2563\n",
      "Epoch   0 Batch 15350/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 30.00%, Batch loss: 1.9705\n",
      "Epoch   0 Batch 15400/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 35.00%, Batch loss: 2.2335\n",
      "Epoch   0 Batch 15450/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.2160\n",
      "Epoch   0 Batch 15500/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 15.00%, Batch loss: 2.1645\n",
      "Epoch   0 Batch 15550/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 15.00%, Batch loss: 2.3658\n",
      "Epoch   0 Batch 15600/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 15.00%, Batch loss: 2.6026\n",
      "Epoch   0 Batch 15650/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 25.00%, Batch loss: 2.6519\n",
      "Epoch   0 Batch 15700/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 15.00%, Batch loss: 2.5089\n",
      "Epoch   0 Batch 15750/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 55.00%, Batch loss: 1.6771\n",
      "Epoch   0 Batch 15800/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 20.00%, Batch loss: 3.1194\n",
      "Epoch   0 Batch 15850/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.0946\n",
      "Epoch   0 Batch 15900/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 10.00%, Batch loss: 2.3744\n",
      "Epoch   0 Batch 15950/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 2.2562\n",
      "Epoch   0 Batch 16000/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 20.00%, Batch loss: 2.2239\n",
      "Epoch   0 Batch 16050/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 20.00%, Batch loss: 2.3086\n",
      "Epoch   0 Batch 16100/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 35.00%, Batch loss: 1.9575\n",
      "Epoch   0 Batch 16150/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 35.00%, Batch loss: 2.1546\n",
      "Epoch   0 Batch 16200/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 35.00%, Batch loss: 1.9268\n",
      "Epoch   0 Batch 16250/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 25.00%, Batch loss: 2.0245\n",
      "Epoch   0 Batch 16300/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 15.00%, Batch loss: 2.3133\n",
      "Epoch   0 Batch 16350/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 50.00%, Batch loss: 1.5953\n",
      "Epoch   0 Batch 16400/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 35.00%, Batch loss: 2.0548\n",
      "Epoch   0 Batch 16450/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 35.00%, Batch loss: 2.2114\n",
      "Epoch   0 Batch 16500/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.5538\n",
      "Epoch   0 Batch 16550/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 35.00%, Batch loss: 2.0226\n",
      "Epoch   0 Batch 16600/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 35.00%, Batch loss: 1.9272\n",
      "Epoch   0 Batch 16650/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 2.3291\n",
      "Epoch   0 Batch 16700/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.6662\n",
      "Epoch   0 Batch 16750/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 15.00%, Batch loss: 2.4416\n",
      "Epoch   0 Batch 16800/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.1330\n",
      "Epoch   0 Batch 16850/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 45.00%, Batch loss: 1.9344\n",
      "Epoch   0 Batch 16900/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 30.00%, Batch loss: 2.3468\n",
      "Epoch   0 Batch 16950/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 30.00%, Batch loss: 2.2661\n",
      "Epoch   0 Batch 17000/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 30.00%, Batch loss: 2.3068\n",
      "Epoch   0 Batch 17050/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 15.00%, Batch loss: 2.1838\n",
      "Epoch   0 Batch 17100/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 30.00%, Batch loss: 1.9157\n",
      "Epoch   0 Batch 17150/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.4320\n",
      "Epoch   0 Batch 17200/17632 - Batch bwd acc: 45.00%, Batch fwd acc: 25.00%, Batch loss: 1.9983\n",
      "Epoch   0 Batch 17250/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 10.00%, Batch loss: 2.5665\n",
      "Epoch   0 Batch 17300/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.0950\n",
      "Epoch   0 Batch 17350/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 35.00%, Batch loss: 2.6701\n",
      "Epoch   0 Batch 17400/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 25.00%, Batch loss: 2.1098\n",
      "Epoch   0 Batch 17450/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 25.00%, Batch loss: 2.4867\n",
      "Epoch   0 Batch 17500/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 2.0732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch 17550/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 15.00%, Batch loss: 2.8025\n",
      "Epoch   0 Batch 17600/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 15.00%, Batch loss: 2.4541\n",
      "Epoch   1 Batch   50/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 30.00%, Batch loss: 2.0088\n",
      "Epoch   1 Batch  100/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 40.00%, Batch loss: 2.1555\n",
      "Epoch   1 Batch  150/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 35.00%, Batch loss: 2.1128\n",
      "Epoch   1 Batch  200/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 30.00%, Batch loss: 1.6632\n",
      "Epoch   1 Batch  250/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 30.00%, Batch loss: 1.8755\n",
      "Epoch   1 Batch  300/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 40.00%, Batch loss: 2.1795\n",
      "Epoch   1 Batch  350/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 20.00%, Batch loss: 2.3676\n",
      "Epoch   1 Batch  400/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 40.00%, Batch loss: 1.9350\n",
      "Epoch   1 Batch  450/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 2.3313\n",
      "Epoch   1 Batch  500/17632 - Batch bwd acc: 55.00%, Batch fwd acc: 30.00%, Batch loss: 1.6516\n",
      "Epoch   1 Batch  550/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 45.00%, Batch loss: 1.9403\n",
      "Epoch   1 Batch  600/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 40.00%, Batch loss: 1.9329\n",
      "Epoch   1 Batch  650/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 20.00%, Batch loss: 2.3197\n",
      "Epoch   1 Batch  700/17632 - Batch bwd acc: 45.00%, Batch fwd acc: 35.00%, Batch loss: 1.6179\n",
      "Epoch   1 Batch  750/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 20.00%, Batch loss: 2.1502\n",
      "Epoch   1 Batch  800/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.0230\n",
      "Epoch   1 Batch  850/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 40.00%, Batch loss: 1.8867\n",
      "Epoch   1 Batch  900/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 20.00%, Batch loss: 2.3154\n",
      "Epoch   1 Batch  950/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 30.00%, Batch loss: 2.4056\n",
      "Epoch   1 Batch 1000/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 10.00%, Batch loss: 2.3400\n",
      "Epoch   1 Batch 1050/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 35.00%, Batch loss: 1.9329\n",
      "Epoch   1 Batch 1100/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 1.7354\n",
      "Epoch   1 Batch 1150/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.0199\n",
      "Epoch   1 Batch 1200/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 15.00%, Batch loss: 2.0634\n",
      "Epoch   1 Batch 1250/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 20.00%, Batch loss: 2.8193\n",
      "Epoch   1 Batch 1300/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 30.00%, Batch loss: 2.6315\n",
      "Epoch   1 Batch 1350/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 25.00%, Batch loss: 2.5692\n",
      "Epoch   1 Batch 1400/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 25.00%, Batch loss: 2.0527\n",
      "Epoch   1 Batch 1450/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 20.00%, Batch loss: 1.8439\n",
      "Epoch   1 Batch 1500/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 20.00%, Batch loss: 2.6182\n",
      "Epoch   1 Batch 1550/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 45.00%, Batch loss: 2.3461\n",
      "Epoch   1 Batch 1600/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 35.00%, Batch loss: 2.1540\n",
      "Epoch   1 Batch 1650/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 35.00%, Batch loss: 1.8331\n",
      "Epoch   1 Batch 1700/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 30.00%, Batch loss: 2.3097\n",
      "Epoch   1 Batch 1750/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.1124\n",
      "Epoch   1 Batch 1800/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 35.00%, Batch loss: 2.1126\n",
      "Epoch   1 Batch 1850/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 30.00%, Batch loss: 1.9597\n",
      "Epoch   1 Batch 1900/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 40.00%, Batch loss: 2.4072\n",
      "Epoch   1 Batch 1950/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 25.00%, Batch loss: 2.2056\n",
      "Epoch   1 Batch 2000/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 55.00%, Batch loss: 1.6824\n",
      "Epoch   1 Batch 2050/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 20.00%, Batch loss: 2.2600\n",
      "Epoch   1 Batch 2100/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.0078\n",
      "Epoch   1 Batch 2150/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.3366\n",
      "Epoch   1 Batch 2200/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 20.00%, Batch loss: 1.9776\n",
      "Epoch   1 Batch 2250/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 30.00%, Batch loss: 2.0269\n",
      "Epoch   1 Batch 2300/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.1962\n",
      "Epoch   1 Batch 2350/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 30.00%, Batch loss: 2.0705\n",
      "Epoch   1 Batch 2400/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 40.00%, Batch loss: 2.0639\n",
      "Epoch   1 Batch 2450/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 20.00%, Batch loss: 2.0245\n",
      "Epoch   1 Batch 2500/17632 - Batch bwd acc: 45.00%, Batch fwd acc: 30.00%, Batch loss: 1.8735\n",
      "Epoch   1 Batch 2550/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 15.00%, Batch loss: 2.4319\n",
      "Epoch   1 Batch 2600/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 25.00%, Batch loss: 1.8885\n",
      "Epoch   1 Batch 2650/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 45.00%, Batch loss: 1.9584\n",
      "Epoch   1 Batch 2700/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 40.00%, Batch loss: 1.9988\n",
      "Epoch   1 Batch 2750/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 35.00%, Batch loss: 1.6565\n",
      "Epoch   1 Batch 2800/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 1.9151\n",
      "Epoch   1 Batch 2850/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 35.00%, Batch loss: 1.6295\n",
      "Epoch   1 Batch 2900/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 30.00%, Batch loss: 2.2527\n",
      "Epoch   1 Batch 2950/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 25.00%, Batch loss: 2.6113\n",
      "Epoch   1 Batch 3000/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 2.1667\n",
      "Epoch   1 Batch 3050/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 25.00%, Batch loss: 1.9613\n",
      "Epoch   1 Batch 3100/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 2.0411\n",
      "Epoch   1 Batch 3150/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 20.00%, Batch loss: 2.3044\n",
      "Epoch   1 Batch 3200/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 30.00%, Batch loss: 1.9451\n",
      "Epoch   1 Batch 3250/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 25.00%, Batch loss: 2.4921\n",
      "Epoch   1 Batch 3300/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 45.00%, Batch loss: 1.5464\n",
      "Epoch   1 Batch 3350/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 15.00%, Batch loss: 2.0982\n",
      "Epoch   1 Batch 3400/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 15.00%, Batch loss: 2.5097\n",
      "Epoch   1 Batch 3450/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 45.00%, Batch loss: 1.6146\n",
      "Epoch   1 Batch 3500/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 25.00%, Batch loss: 2.1386\n",
      "Epoch   1 Batch 3550/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 15.00%, Batch loss: 2.1813\n",
      "Epoch   1 Batch 3600/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 40.00%, Batch loss: 2.4875\n",
      "Epoch   1 Batch 3650/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 25.00%, Batch loss: 1.9069\n",
      "Epoch   1 Batch 3700/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 35.00%, Batch loss: 2.4066\n",
      "Epoch   1 Batch 3750/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 2.1010\n",
      "Epoch   1 Batch 3800/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 35.00%, Batch loss: 2.1025\n",
      "Epoch   1 Batch 3850/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 30.00%, Batch loss: 2.3210\n",
      "Epoch   1 Batch 3900/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 15.00%, Batch loss: 2.0450\n",
      "Epoch   1 Batch 3950/17632 - Batch bwd acc: 50.00%, Batch fwd acc: 25.00%, Batch loss: 1.7733\n",
      "Epoch   1 Batch 4000/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 35.00%, Batch loss: 1.8539\n",
      "Epoch   1 Batch 4050/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 20.00%, Batch loss: 1.9006\n",
      "Epoch   1 Batch 4100/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 35.00%, Batch loss: 1.6505\n",
      "Epoch   1 Batch 4150/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 35.00%, Batch loss: 2.0670\n",
      "Epoch   1 Batch 4200/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 35.00%, Batch loss: 2.0399\n",
      "Epoch   1 Batch 4250/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 15.00%, Batch loss: 2.2451\n",
      "Epoch   1 Batch 4300/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 50.00%, Batch loss: 1.4893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 Batch 4350/17632 - Batch bwd acc: 50.00%, Batch fwd acc: 35.00%, Batch loss: 1.8245\n",
      "Epoch   1 Batch 4400/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 30.00%, Batch loss: 2.4205\n",
      "Epoch   1 Batch 4450/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 35.00%, Batch loss: 1.6846\n",
      "Epoch   1 Batch 4500/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 35.00%, Batch loss: 1.8099\n",
      "Epoch   1 Batch 4550/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 25.00%, Batch loss: 2.2994\n",
      "Epoch   1 Batch 4600/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 40.00%, Batch loss: 2.0850\n",
      "Epoch   1 Batch 4650/17632 - Batch bwd acc: 55.00%, Batch fwd acc: 35.00%, Batch loss: 1.4842\n",
      "Epoch   1 Batch 4700/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 25.00%, Batch loss: 2.0430\n",
      "Epoch   1 Batch 4750/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 35.00%, Batch loss: 1.9777\n",
      "Epoch   1 Batch 4800/17632 - Batch bwd acc: 45.00%, Batch fwd acc: 50.00%, Batch loss: 1.5561\n",
      "Epoch   1 Batch 4850/17632 - Batch bwd acc: 45.00%, Batch fwd acc: 40.00%, Batch loss: 2.0291\n",
      "Epoch   1 Batch 4900/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 1.7985\n",
      "Epoch   1 Batch 4950/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 25.00%, Batch loss: 1.7186\n",
      "Epoch   1 Batch 5000/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 45.00%, Batch loss: 1.8972\n",
      "Epoch   1 Batch 5050/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.2452\n",
      "Epoch   1 Batch 5100/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 30.00%, Batch loss: 2.0526\n",
      "Epoch   1 Batch 5150/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 35.00%, Batch loss: 2.1306\n",
      "Epoch   1 Batch 5200/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 20.00%, Batch loss: 2.3687\n",
      "Epoch   1 Batch 5250/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 55.00%, Batch loss: 1.6962\n",
      "Epoch   1 Batch 5300/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 40.00%, Batch loss: 1.8914\n",
      "Epoch   1 Batch 5350/17632 - Batch bwd acc: 50.00%, Batch fwd acc: 25.00%, Batch loss: 1.6481\n",
      "Epoch   1 Batch 5400/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 25.00%, Batch loss: 2.2336\n",
      "Epoch   1 Batch 5450/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 35.00%, Batch loss: 1.5366\n",
      "Epoch   1 Batch 5500/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 30.00%, Batch loss: 2.0510\n",
      "Epoch   1 Batch 5550/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 35.00%, Batch loss: 1.7864\n",
      "Epoch   1 Batch 5600/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 30.00%, Batch loss: 1.9868\n",
      "Epoch   1 Batch 5650/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 45.00%, Batch loss: 1.4348\n",
      "Epoch   1 Batch 5700/17632 - Batch bwd acc: 45.00%, Batch fwd acc: 35.00%, Batch loss: 1.6274\n",
      "Epoch   1 Batch 5750/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 30.00%, Batch loss: 1.9957\n",
      "Epoch   1 Batch 5800/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 2.1478\n",
      "Epoch   1 Batch 5850/17632 - Batch bwd acc: 45.00%, Batch fwd acc: 30.00%, Batch loss: 1.5340\n",
      "Epoch   1 Batch 5900/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 35.00%, Batch loss: 1.7075\n",
      "Epoch   1 Batch 5950/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 40.00%, Batch loss: 2.0884\n",
      "Epoch   1 Batch 6000/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 2.0112\n",
      "Epoch   1 Batch 6050/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 1.8560\n",
      "Epoch   1 Batch 6100/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 30.00%, Batch loss: 2.0819\n",
      "Epoch   1 Batch 6150/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 1.8575\n",
      "Epoch   1 Batch 6200/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.2545\n",
      "Epoch   1 Batch 6250/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 30.00%, Batch loss: 1.9656\n",
      "Epoch   1 Batch 6300/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 30.00%, Batch loss: 2.0006\n",
      "Epoch   1 Batch 6350/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 35.00%, Batch loss: 1.7114\n",
      "Epoch   1 Batch 6400/17632 - Batch bwd acc: 45.00%, Batch fwd acc: 40.00%, Batch loss: 1.4384\n",
      "Epoch   1 Batch 6450/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 30.00%, Batch loss: 1.9616\n",
      "Epoch   1 Batch 6500/17632 - Batch bwd acc: 50.00%, Batch fwd acc: 45.00%, Batch loss: 1.5477\n",
      "Epoch   1 Batch 6550/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 20.00%, Batch loss: 2.6265\n",
      "Epoch   1 Batch 6600/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 1.9394\n",
      "Epoch   1 Batch 6650/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 40.00%, Batch loss: 1.9269\n",
      "Epoch   1 Batch 6700/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 40.00%, Batch loss: 1.9961\n",
      "Epoch   1 Batch 6750/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 45.00%, Batch loss: 1.9208\n",
      "Epoch   1 Batch 6800/17632 - Batch bwd acc: 50.00%, Batch fwd acc: 20.00%, Batch loss: 1.7292\n",
      "Epoch   1 Batch 6850/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 40.00%, Batch loss: 2.0507\n",
      "Epoch   1 Batch 6900/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 45.00%, Batch loss: 1.7052\n",
      "Epoch   1 Batch 6950/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 35.00%, Batch loss: 2.0601\n",
      "Epoch   1 Batch 7000/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 20.00%, Batch loss: 2.1739\n",
      "Epoch   1 Batch 7050/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 25.00%, Batch loss: 1.9840\n",
      "Epoch   1 Batch 7100/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 25.00%, Batch loss: 2.2211\n",
      "Epoch   1 Batch 7150/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 25.00%, Batch loss: 1.8836\n",
      "Epoch   1 Batch 7200/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 1.9569\n",
      "Epoch   1 Batch 7250/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 25.00%, Batch loss: 1.6559\n",
      "Epoch   1 Batch 7300/17632 - Batch bwd acc: 45.00%, Batch fwd acc: 25.00%, Batch loss: 1.8567\n",
      "Epoch   1 Batch 7350/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 20.00%, Batch loss: 2.4908\n",
      "Epoch   1 Batch 7400/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 30.00%, Batch loss: 1.9405\n",
      "Epoch   1 Batch 7450/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 30.00%, Batch loss: 1.9970\n",
      "Epoch   1 Batch 7500/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 30.00%, Batch loss: 1.8352\n",
      "Epoch   1 Batch 7550/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 20.00%, Batch loss: 2.3181\n",
      "Epoch   1 Batch 7600/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 10.00%, Batch loss: 2.2701\n",
      "Epoch   1 Batch 7650/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 2.0929\n",
      "Epoch   1 Batch 7700/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 20.00%, Batch loss: 2.0719\n",
      "Epoch   1 Batch 7750/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 15.00%, Batch loss: 2.1396\n",
      "Epoch   1 Batch 7800/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.2908\n",
      "Epoch   1 Batch 7850/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 30.00%, Batch loss: 1.7781\n",
      "Epoch   1 Batch 7900/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.0888\n",
      "Epoch   1 Batch 7950/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 30.00%, Batch loss: 2.0965\n",
      "Epoch   1 Batch 8000/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 20.00%, Batch loss: 2.1824\n",
      "Epoch   1 Batch 8050/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.0752\n",
      "Epoch   1 Batch 8100/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 40.00%, Batch loss: 2.1607\n",
      "Epoch   1 Batch 8150/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 2.0881\n",
      "Epoch   1 Batch 8200/17632 - Batch bwd acc: 45.00%, Batch fwd acc: 40.00%, Batch loss: 1.8597\n",
      "Epoch   1 Batch 8250/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 35.00%, Batch loss: 1.4706\n",
      "Epoch   1 Batch 8300/17632 - Batch bwd acc: 50.00%, Batch fwd acc: 40.00%, Batch loss: 1.5800\n",
      "Epoch   1 Batch 8350/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 2.2065\n",
      "Epoch   1 Batch 8400/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 40.00%, Batch loss: 1.7479\n",
      "Epoch   1 Batch 8450/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 35.00%, Batch loss: 1.8253\n",
      "Epoch   1 Batch 8500/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 40.00%, Batch loss: 1.7745\n",
      "Epoch   1 Batch 8550/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 40.00%, Batch loss: 1.5658\n",
      "Epoch   1 Batch 8600/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 30.00%, Batch loss: 1.6428\n",
      "Epoch   1 Batch 8650/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 15.00%, Batch loss: 2.3480\n",
      "Epoch   1 Batch 8700/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 50.00%, Batch loss: 1.9831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 Batch 8750/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.0016\n",
      "Epoch   1 Batch 8800/17632 - Batch bwd acc: 50.00%, Batch fwd acc: 30.00%, Batch loss: 1.6979\n",
      "Epoch   1 Batch 8850/17632 - Batch bwd acc: 50.00%, Batch fwd acc: 45.00%, Batch loss: 1.4789\n",
      "Epoch   1 Batch 8900/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 25.00%, Batch loss: 1.5834\n",
      "Epoch   1 Batch 8950/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.0831\n",
      "Epoch   1 Batch 9000/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 35.00%, Batch loss: 1.9202\n",
      "Epoch   1 Batch 9050/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 40.00%, Batch loss: 1.6214\n",
      "Epoch   1 Batch 9100/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 40.00%, Batch loss: 2.3068\n",
      "Epoch   1 Batch 9150/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 35.00%, Batch loss: 1.7656\n",
      "Epoch   1 Batch 9200/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 45.00%, Batch loss: 1.8865\n",
      "Epoch   1 Batch 9250/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 30.00%, Batch loss: 2.4927\n",
      "Epoch   1 Batch 9300/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 2.1328\n",
      "Epoch   1 Batch 9350/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 45.00%, Batch loss: 1.7647\n",
      "Epoch   1 Batch 9400/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 15.00%, Batch loss: 2.1415\n",
      "Epoch   1 Batch 9450/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 35.00%, Batch loss: 1.6905\n",
      "Epoch   1 Batch 9500/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 15.00%, Batch loss: 2.1436\n",
      "Epoch   1 Batch 9550/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 30.00%, Batch loss: 1.7876\n",
      "Epoch   1 Batch 9600/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 35.00%, Batch loss: 2.0170\n",
      "Epoch   1 Batch 9650/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 45.00%, Batch loss: 1.8487\n",
      "Epoch   1 Batch 9700/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.0678\n",
      "Epoch   1 Batch 9750/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 40.00%, Batch loss: 2.0157\n",
      "Epoch   1 Batch 9800/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 40.00%, Batch loss: 2.0100\n",
      "Epoch   1 Batch 9850/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 35.00%, Batch loss: 2.0791\n",
      "Epoch   1 Batch 9900/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 40.00%, Batch loss: 1.9297\n",
      "Epoch   1 Batch 9950/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 40.00%, Batch loss: 1.8859\n",
      "Epoch   1 Batch 10000/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.2124\n",
      "Epoch   1 Batch 10050/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 2.0019\n",
      "Epoch   1 Batch 10100/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 1.8486\n",
      "Epoch   1 Batch 10150/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 15.00%, Batch loss: 2.2001\n",
      "Epoch   1 Batch 10200/17632 - Batch bwd acc: 5.00%, Batch fwd acc: 40.00%, Batch loss: 2.3518\n",
      "Epoch   1 Batch 10250/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 35.00%, Batch loss: 1.6935\n",
      "Epoch   1 Batch 10300/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 35.00%, Batch loss: 1.8145\n",
      "Epoch   1 Batch 10350/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 20.00%, Batch loss: 4.1661\n",
      "Epoch   1 Batch 10400/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 40.00%, Batch loss: 1.4973\n",
      "Epoch   1 Batch 10450/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 30.00%, Batch loss: 1.9289\n",
      "Epoch   1 Batch 10500/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 40.00%, Batch loss: 1.8546\n",
      "Epoch   1 Batch 10550/17632 - Batch bwd acc: 45.00%, Batch fwd acc: 35.00%, Batch loss: 1.3924\n",
      "Epoch   1 Batch 10600/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 50.00%, Batch loss: 1.9304\n",
      "Epoch   1 Batch 10650/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 35.00%, Batch loss: 2.4031\n",
      "Epoch   1 Batch 10700/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 30.00%, Batch loss: 2.2475\n",
      "Epoch   1 Batch 10750/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 2.3047\n",
      "Epoch   1 Batch 10800/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 15.00%, Batch loss: 2.6339\n",
      "Epoch   1 Batch 10850/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 35.00%, Batch loss: 1.7127\n",
      "Epoch   1 Batch 10900/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 2.3426\n",
      "Epoch   1 Batch 10950/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 35.00%, Batch loss: 1.9629\n",
      "Epoch   1 Batch 11000/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 40.00%, Batch loss: 1.8908\n",
      "Epoch   1 Batch 11050/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 40.00%, Batch loss: 1.7924\n",
      "Epoch   1 Batch 11100/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 60.00%, Batch loss: 1.5072\n",
      "Epoch   1 Batch 11150/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 45.00%, Batch loss: 1.8020\n",
      "Epoch   1 Batch 11200/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 25.00%, Batch loss: 2.2222\n",
      "Epoch   1 Batch 11250/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 45.00%, Batch loss: 1.8327\n",
      "Epoch   1 Batch 11300/17632 - Batch bwd acc: 55.00%, Batch fwd acc: 20.00%, Batch loss: 1.6604\n",
      "Epoch   1 Batch 11350/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 30.00%, Batch loss: 1.7674\n",
      "Epoch   1 Batch 11400/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 45.00%, Batch loss: 1.6345\n",
      "Epoch   1 Batch 11450/17632 - Batch bwd acc: 50.00%, Batch fwd acc: 35.00%, Batch loss: 1.5631\n",
      "Epoch   1 Batch 11500/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 25.00%, Batch loss: 2.1869\n",
      "Epoch   1 Batch 11550/17632 - Batch bwd acc: 45.00%, Batch fwd acc: 30.00%, Batch loss: 1.8977\n",
      "Epoch   1 Batch 11600/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 30.00%, Batch loss: 2.2504\n",
      "Epoch   1 Batch 11650/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 40.00%, Batch loss: 1.9396\n",
      "Epoch   1 Batch 11700/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 2.0612\n",
      "Epoch   1 Batch 11750/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.0285\n",
      "Epoch   1 Batch 11800/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 40.00%, Batch loss: 2.0296\n",
      "Epoch   1 Batch 11850/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 30.00%, Batch loss: 2.2762\n",
      "Epoch   1 Batch 11900/17632 - Batch bwd acc: 45.00%, Batch fwd acc: 45.00%, Batch loss: 1.7388\n",
      "Epoch   1 Batch 11950/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 40.00%, Batch loss: 1.8446\n",
      "Epoch   1 Batch 12000/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 20.00%, Batch loss: 2.2130\n",
      "Epoch   1 Batch 12050/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 35.00%, Batch loss: 1.9388\n",
      "Epoch   1 Batch 12100/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 40.00%, Batch loss: 1.7117\n",
      "Epoch   1 Batch 12150/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 35.00%, Batch loss: 2.2623\n",
      "Epoch   1 Batch 12200/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 25.00%, Batch loss: 2.1305\n",
      "Epoch   1 Batch 12250/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 35.00%, Batch loss: 2.0086\n",
      "Epoch   1 Batch 12300/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.0020\n",
      "Epoch   1 Batch 12350/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 35.00%, Batch loss: 1.9678\n",
      "Epoch   1 Batch 12400/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 35.00%, Batch loss: 1.7063\n",
      "Epoch   1 Batch 12450/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 25.00%, Batch loss: 2.1511\n",
      "Epoch   1 Batch 12500/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 25.00%, Batch loss: 1.9031\n",
      "Epoch   1 Batch 12550/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 50.00%, Batch loss: 1.8148\n",
      "Epoch   1 Batch 12600/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 30.00%, Batch loss: 2.0796\n",
      "Epoch   1 Batch 12650/17632 - Batch bwd acc: 45.00%, Batch fwd acc: 20.00%, Batch loss: 1.9252\n",
      "Epoch   1 Batch 12700/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 20.00%, Batch loss: 1.8241\n",
      "Epoch   1 Batch 12750/17632 - Batch bwd acc: 45.00%, Batch fwd acc: 50.00%, Batch loss: 1.7461\n",
      "Epoch   1 Batch 12800/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 2.0639\n",
      "Epoch   1 Batch 12850/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 25.00%, Batch loss: 2.2671\n",
      "Epoch   1 Batch 12900/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 2.0026\n",
      "Epoch   1 Batch 12950/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 2.2152\n",
      "Epoch   1 Batch 13000/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 30.00%, Batch loss: 1.8655\n",
      "Epoch   1 Batch 13050/17632 - Batch bwd acc: 50.00%, Batch fwd acc: 35.00%, Batch loss: 1.7951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 Batch 13100/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 10.00%, Batch loss: 2.4312\n",
      "Epoch   1 Batch 13150/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 50.00%, Batch loss: 1.9970\n",
      "Epoch   1 Batch 13200/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 45.00%, Batch loss: 1.8610\n",
      "Epoch   1 Batch 13250/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 20.00%, Batch loss: 2.0424\n",
      "Epoch   1 Batch 13300/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 25.00%, Batch loss: 2.2095\n",
      "Epoch   1 Batch 13350/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 45.00%, Batch loss: 1.6987\n",
      "Epoch   1 Batch 13400/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 25.00%, Batch loss: 1.9021\n",
      "Epoch   1 Batch 13450/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 35.00%, Batch loss: 2.2131\n",
      "Epoch   1 Batch 13500/17632 - Batch bwd acc: 45.00%, Batch fwd acc: 40.00%, Batch loss: 1.4698\n",
      "Epoch   1 Batch 13550/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 5.00%, Batch loss: 2.1874\n",
      "Epoch   1 Batch 13600/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 40.00%, Batch loss: 1.7838\n",
      "Epoch   1 Batch 13650/17632 - Batch bwd acc: 50.00%, Batch fwd acc: 25.00%, Batch loss: 1.6389\n",
      "Epoch   1 Batch 13700/17632 - Batch bwd acc: 50.00%, Batch fwd acc: 45.00%, Batch loss: 1.4342\n",
      "Epoch   1 Batch 13750/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 1.8305\n",
      "Epoch   1 Batch 13800/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 1.9897\n",
      "Epoch   1 Batch 13850/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 35.00%, Batch loss: 1.9788\n",
      "Epoch   1 Batch 13900/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 45.00%, Batch loss: 1.6998\n",
      "Epoch   1 Batch 13950/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 35.00%, Batch loss: 1.7887\n",
      "Epoch   1 Batch 14000/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 25.00%, Batch loss: 1.9969\n",
      "Epoch   1 Batch 14050/17632 - Batch bwd acc: 45.00%, Batch fwd acc: 20.00%, Batch loss: 1.9355\n",
      "Epoch   1 Batch 14100/17632 - Batch bwd acc: 55.00%, Batch fwd acc: 40.00%, Batch loss: 1.9708\n",
      "Epoch   1 Batch 14150/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 2.3422\n",
      "Epoch   1 Batch 14200/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 1.8989\n",
      "Epoch   1 Batch 14250/17632 - Batch bwd acc: 50.00%, Batch fwd acc: 25.00%, Batch loss: 1.9370\n",
      "Epoch   1 Batch 14300/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 25.00%, Batch loss: 2.4787\n",
      "Epoch   1 Batch 14350/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 30.00%, Batch loss: 2.1091\n",
      "Epoch   1 Batch 14400/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 50.00%, Batch loss: 1.6561\n",
      "Epoch   1 Batch 14450/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 30.00%, Batch loss: 1.8363\n",
      "Epoch   1 Batch 14500/17632 - Batch bwd acc: 45.00%, Batch fwd acc: 40.00%, Batch loss: 1.5699\n",
      "Epoch   1 Batch 14550/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 20.00%, Batch loss: 2.2221\n",
      "Epoch   1 Batch 14600/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 2.1027\n",
      "Epoch   1 Batch 14650/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 20.00%, Batch loss: 2.1256\n",
      "Epoch   1 Batch 14700/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 10.00%, Batch loss: 2.2620\n",
      "Epoch   1 Batch 14750/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 35.00%, Batch loss: 1.8923\n",
      "Epoch   1 Batch 14800/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 20.00%, Batch loss: 2.1587\n",
      "Epoch   1 Batch 14850/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 25.00%, Batch loss: 2.2504\n",
      "Epoch   1 Batch 14900/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 40.00%, Batch loss: 2.0571\n",
      "Epoch   1 Batch 14950/17632 - Batch bwd acc: 45.00%, Batch fwd acc: 40.00%, Batch loss: 2.2258\n",
      "Epoch   1 Batch 15000/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 1.8804\n",
      "Epoch   1 Batch 15050/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 35.00%, Batch loss: 1.7126\n",
      "Epoch   1 Batch 15100/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 40.00%, Batch loss: 1.6731\n",
      "Epoch   1 Batch 15150/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 1.9155\n",
      "Epoch   1 Batch 15200/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 1.9729\n",
      "Epoch   1 Batch 15250/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 25.00%, Batch loss: 2.5097\n",
      "Epoch   1 Batch 15300/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 1.9098\n",
      "Epoch   1 Batch 15350/17632 - Batch bwd acc: 50.00%, Batch fwd acc: 30.00%, Batch loss: 1.4437\n",
      "Epoch   1 Batch 15400/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 45.00%, Batch loss: 1.7758\n",
      "Epoch   1 Batch 15450/17632 - Batch bwd acc: 10.00%, Batch fwd acc: 50.00%, Batch loss: 1.7641\n",
      "Epoch   1 Batch 15500/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 30.00%, Batch loss: 1.9334\n",
      "Epoch   1 Batch 15550/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 30.00%, Batch loss: 2.0814\n",
      "Epoch   1 Batch 15600/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 20.00%, Batch loss: 2.4511\n",
      "Epoch   1 Batch 15650/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 25.00%, Batch loss: 2.6152\n",
      "Epoch   1 Batch 15700/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 35.00%, Batch loss: 2.4770\n",
      "Epoch   1 Batch 15750/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 45.00%, Batch loss: 1.6275\n",
      "Epoch   1 Batch 15800/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 20.00%, Batch loss: 2.4631\n",
      "Epoch   1 Batch 15850/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 2.1603\n",
      "Epoch   1 Batch 15900/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 25.00%, Batch loss: 2.0793\n",
      "Epoch   1 Batch 15950/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 40.00%, Batch loss: 1.8022\n",
      "Epoch   1 Batch 16000/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 40.00%, Batch loss: 1.7961\n",
      "Epoch   1 Batch 16050/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 2.0067\n",
      "Epoch   1 Batch 16100/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 35.00%, Batch loss: 1.7960\n",
      "Epoch   1 Batch 16150/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 35.00%, Batch loss: 2.1308\n",
      "Epoch   1 Batch 16200/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 25.00%, Batch loss: 1.6879\n",
      "Epoch   1 Batch 16250/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 35.00%, Batch loss: 1.7860\n",
      "Epoch   1 Batch 16300/17632 - Batch bwd acc: 50.00%, Batch fwd acc: 20.00%, Batch loss: 1.9236\n",
      "Epoch   1 Batch 16350/17632 - Batch bwd acc: 50.00%, Batch fwd acc: 45.00%, Batch loss: 1.5157\n",
      "Epoch   1 Batch 16400/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 40.00%, Batch loss: 1.8514\n",
      "Epoch   1 Batch 16450/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 35.00%, Batch loss: 2.0227\n",
      "Epoch   1 Batch 16500/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 30.00%, Batch loss: 2.1556\n",
      "Epoch   1 Batch 16550/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 55.00%, Batch loss: 1.5797\n",
      "Epoch   1 Batch 16600/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 50.00%, Batch loss: 1.6528\n",
      "Epoch   1 Batch 16650/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 35.00%, Batch loss: 2.2612\n",
      "Epoch   1 Batch 16700/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 15.00%, Batch loss: 2.5526\n",
      "Epoch   1 Batch 16750/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 20.00%, Batch loss: 2.2900\n",
      "Epoch   1 Batch 16800/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 45.00%, Batch loss: 1.8738\n",
      "Epoch   1 Batch 16850/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 40.00%, Batch loss: 1.6201\n",
      "Epoch   1 Batch 16900/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 55.00%, Batch loss: 1.7522\n",
      "Epoch   1 Batch 16950/17632 - Batch bwd acc: 15.00%, Batch fwd acc: 45.00%, Batch loss: 2.1668\n",
      "Epoch   1 Batch 17000/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 45.00%, Batch loss: 1.9556\n",
      "Epoch   1 Batch 17050/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 45.00%, Batch loss: 1.6626\n",
      "Epoch   1 Batch 17100/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 30.00%, Batch loss: 1.8133\n",
      "Epoch   1 Batch 17150/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 30.00%, Batch loss: 1.8352\n",
      "Epoch   1 Batch 17200/17632 - Batch bwd acc: 45.00%, Batch fwd acc: 35.00%, Batch loss: 1.9823\n",
      "Epoch   1 Batch 17250/17632 - Batch bwd acc: 40.00%, Batch fwd acc: 20.00%, Batch loss: 1.8809\n",
      "Epoch   1 Batch 17300/17632 - Batch bwd acc: 25.00%, Batch fwd acc: 40.00%, Batch loss: 1.8098\n",
      "Epoch   1 Batch 17350/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 20.00%, Batch loss: 2.6254\n",
      "Epoch   1 Batch 17400/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 30.00%, Batch loss: 2.0134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 Batch 17450/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 25.00%, Batch loss: 2.0870\n",
      "Epoch   1 Batch 17500/17632 - Batch bwd acc: 35.00%, Batch fwd acc: 25.00%, Batch loss: 1.7858\n",
      "Epoch   1 Batch 17550/17632 - Batch bwd acc: 20.00%, Batch fwd acc: 40.00%, Batch loss: 2.1845\n",
      "Epoch   1 Batch 17600/17632 - Batch bwd acc: 30.00%, Batch fwd acc: 35.00%, Batch loss: 2.1756\n"
     ]
    }
   ],
   "source": [
    "### This is the training section, comment out if you want to use the latest trained model in SAVE_DIR\n",
    "print('Starting training...')\n",
    "start_time = time.strftime('%y-%m-%d-%H-%M-%S')\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    saver = tf.train.Saver(max_to_keep=MAX_TO_KEEP)\n",
    "    train_writer = tf.summary.FileWriter('{}run-{}/{}'.format(SUMMARIES_DIR, start_time, 'train'), sess.graph)\n",
    "    valid_writer = tf.summary.FileWriter('{}run-{}/{}'.format(SUMMARIES_DIR, start_time, 'valid'), sess.graph)\n",
    "    step = 0\n",
    "    for e in range(EPOCHS):\n",
    "        valid_batch =  get_batches(validation_inputs, validation_masks, batch_size=3, n_sent_in_story=3,\n",
    "                                   is_quiz=True, quiz_answers=answers, shuffle=False)\n",
    "        for batch_i, (batch_inputs, batch_masks, batch_targets, batch_weights) in \\\n",
    "        enumerate(get_batches(enc_sentences, enc_masks, batch_size=BATCH_SIZE, n_sent_in_story=5, shuffle=False)):\n",
    "            \n",
    "            feed_dict = {encoder_inputs: batch_inputs,\n",
    "                         encoder_input_masks: batch_masks,\n",
    "                         encoder_targets: batch_targets,\n",
    "                         label_weights: batch_weights,\n",
    "                         dropout_rate: DROPOUT_RATE}\n",
    "              \n",
    "            _, batch_loss, bwd_accuracy, fwd_accuracy, summary = sess.run([train_op,\n",
    "                                                                           loss,\n",
    "                                                                           bwd_acc,\n",
    "                                                                           fwd_acc,\n",
    "                                                                           merged],\n",
    "                                                                           feed_dict=feed_dict)\n",
    "            train_writer.add_summary(summary, step)\n",
    "            \n",
    "            if batch_i % DISPLAY_STEP == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Batch bwd acc: {:>3.2%}, Batch fwd acc: {:>3.2%}, Batch loss: {:>6.4f}'\n",
    "                      .format(e, batch_i, len(enc_sentences) // BATCH_SIZE, bwd_accuracy, fwd_accuracy, batch_loss))\n",
    "                \n",
    "            if batch_i % VALIDATE_STEP == 0 and batch_i > 0:\n",
    "                valid_input, valid_mask, valid_target, valid_weight = next(valid_batch)\n",
    "                feed_dict = {encoder_inputs: valid_input,\n",
    "                             encoder_input_masks: valid_mask,\n",
    "                             encoder_targets: valid_target,\n",
    "                             label_weights: valid_weight,\n",
    "                             dropout_rate: 0}\n",
    "                \n",
    "                valid_loss, stream_bwd_accuracy, stream_fwd_accuracy, summary = sess.run([loss,\n",
    "                                                                                             stream_bwd_acc,\n",
    "                                                                                             stream_fwd_acc,\n",
    "                                                                                             merged],\n",
    "                                                                                             feed_dict=feed_dict)\n",
    "                \n",
    "                valid_writer.add_summary(summary, step)\n",
    "                \n",
    "            step += 1\n",
    "        saver.save(sess, '{}/run-{}_ep_{}_step_{}_enc_{}_bsize_{}.ckpt'.format(\n",
    "            SAVE_DIR, start_time, e, step, ENCODER_TYPE, BATCH_SIZE))\n",
    "    train_writer.close()\n",
    "    valid_writer.close()\n",
    "print('Training finished.')\n",
    "### End of training section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get _cloze_ predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.latest_checkpoint(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/run-18-05-22-22-19-37_ep_1_step_35264_enc_GRU_bsize_25.ckpt\n",
      "Checkpoint ./checkpoints/run-18-05-22-22-19-37_ep_1_step_35264_enc_GRU_bsize_25.ckpt\n",
      "Validation score: 0.6146445750935329\n"
     ]
    }
   ],
   "source": [
    "print('Checking validation score...')\n",
    "cloze_preds = []\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, checkpoint)\n",
    "    for valid_i, (valid_input, valid_mask, valid_target, valid_weight) \\\n",
    "    in enumerate(get_batches(validation_inputs, validation_masks, batch_size=3, n_sent_in_story=3,\n",
    "                             is_quiz=True, quiz_answers=answers, shuffle=False)):\n",
    "        scr = sess.run(scores,\n",
    "                      {encoder_inputs: valid_input,\n",
    "                       encoder_input_masks: valid_mask,\n",
    "                       encoder_targets: valid_target,\n",
    "                       label_weights: valid_weight,\n",
    "                       dropout_rate: 0})\n",
    "        cloze_pred = np.argmax(scr[0, 1:]) + 1\n",
    "        cloze_preds.append(cloze_pred)\n",
    "    cloze_preds = np.array(cloze_preds).reshape((-1, 1))\n",
    "cloze_score = np.mean((answers == cloze_preds))\n",
    "print('Checkpoint {}'.format(checkpoint))\n",
    "print('Validation score: {}'.format(cloze_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best obtained score so far:\n",
    "```0.6376269374665954```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating NLU 2018 test predictions...')\n",
    "cloze_preds = []\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, checkpoint)\n",
    "    for test_i, (test_input, test_mask, test_target, test_weight) \\\n",
    "    in enumerate(get_batches(NLU_test_inputs, NLU_test_masks, batch_size=3, n_sent_in_story=3,\n",
    "                             is_quiz=True, quiz_answers=np.ones((len(NLU_test_inputs), 1)), shuffle=False)):\n",
    "        scr = sess.run(scores,\n",
    "                      {encoder_inputs: test_input,\n",
    "                       encoder_input_masks: test_mask,\n",
    "                       encoder_targets: test_target,\n",
    "                       label_weights: test_weight,\n",
    "                       dropout_rate: 0})\n",
    "        cloze_pred = np.argmax(scr[0, 1:]) + 1\n",
    "        cloze_preds.append(cloze_pred)\n",
    "    cloze_preds = np.array(cloze_preds).reshape((-1, 1))\n",
    "output_file = 'NLU_test_preds.csv'\n",
    "np.savetxt(output_file, cloze_preds, fmt='%1u', delimiter=',')\n",
    "print('NLU 2018 test predictions written to {} - Bye!'.format(output_file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
